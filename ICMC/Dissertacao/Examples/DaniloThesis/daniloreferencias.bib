% This file was created with JabRef 2.5.
% Encoding: Cp1252

@ARTICLE{Adami2001,
  author = {Adami, N. and Bugatti, A. and Leonardi, R. and Migliorati, P. and
	Rossi, L. A.},
  title = {The ToCAI Description Scheme for Indexing and Retrieval of Multimedia
	Documents},
  journal = {Multimedia Tools and Applications},
  year = {2001},
  volume = {14},
  pages = {153--173},
  number = {2},
  month = {June},
  abstract = {A framework, called Table of Content-Analytical Index (ToCAI), for
	the content description of multimedia material is presented. The
	idea for such a description scheme (DS) comes out from the structures
	used for indexing technical books (containing a Table of Content,
	typically placed at the beginning of the book, where the list of
	topics is organized hierarchically into chapters, sections, and an
	Analytical Index, typically placed at the end of the book, where
	keywords are listed alphabetically). The ToCAI description scheme
	provides similarly a hierarchical description of the time sequential
	structure of a multimedia document (ToC), suitable for browsing,
	and an â€œAnalytical Indexï¿½? (AI) of audio-visual key items for
	the document, suitable for effective retrieval. Besides two other
	sub-description schemes are proposed to specify the program category
	and the description of other metadata associated to the multimedia
	document in the general DS. The detailed structure of the DS is presented
	by means of a UML diagram. Moreover, some suitable automatic extraction
	methods for the identification of the values associated to the descriptors
	that compose the ToCAI are presented and discussed. Finally, a browsing
	application example is also proposed.},
  citeulike-article-id = {3252942},
  doi = {http://dx.doi.org/10.1023/A:1011347200133},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\The ToCAI Description Scheme for Indexing and Retrieval of Multimedia Documents.pdf:PDF},
  owner = {danilo},
  posted-at = {2008-09-13 20:11:32},
  priority = {2},
  timestamp = {2008.09.13},
  url = {http://dx.doi.org/10.1023/A:1011347200133}
}

@ARTICLE{Adomavicius2005,
  author = {Gediminas Adomavicius and Alexander Tuzhilin},
  title = {Toward the Next Generation of Recommender Systems: A Survey of the
	State-of-the-Art and Possible Extensions},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  year = {2005},
  volume = {17},
  pages = {734-749},
  address = {Los Alamitos, CA, USA},
  doi = {http://doi.ieeecomputersociety.org/10.1109/TKDE.2005.99},
  issn = {1041-4347},
  publisher = {IEEE Computer Society}
}

@INPROCEEDINGS{Al-Hames2006,
  author = {Al-Hames, M. and Zettl, S. and Wallhoff, F. and Reiter, S. and Schuller,
	B. and Rigoll, G.},
  title = {A Two-Layer Graphical Model for Combined Video Shot and Scene Boundary
	Detection},
  booktitle = {Multimedia and Expo, 2006 IEEE International Conference on},
  year = {2006},
  pages = {261 -264},
  doi = {10.1109/ICME.2006.262432},
  keywords = {scene boundary detection;semantic feature;two-layer graphical model;video
	shot detection;feature extraction;video signal processing;}
}

@ARTICLE{Alatan2001,
  author = {Alatan, Aydin A. and Akansu, Ali N. and Wolf, Wayne },
  title = {Multi-Modal Dialog Scene Detection Using Hidden Markov Models for
	Content-Based Multimedia Indexing},
  journal = {Multimedia Tools and Applications},
  year = {2001},
  volume = {14},
  pages = {137--151},
  number = {2},
  month = {June},
  abstract = {A class of audio-visual data (fiction entertainment: movies, TV series)
	is segmented into scenes, which contain dialogs, using a novel hidden
	Markov model-based (HMM) method. Each shot is classified using both
	audio track (via classification of speech, silence and music) and
	visual content (face and location information). The result of this
	shot-based classification is an audio-visual token to be used by
	the HMM state diagram to achieve scene analysis. After simulations
	with circular and left-to-right HMM topologies, it is observed that
	both are performing very good with multi-modal inputs. Moreover,
	for circular topology, the comparisons between different training
	and observation sets show that audio and face information together
	gives the most consistent results among different observation sets.}
}

@BOOK{Altheide1985,
  title = {Media Power},
  publisher = {Beverly Hills},
  year = {1985},
  editor = {Sage},
  author = {Altheide, David},
  owner = {danilocoimbra},
  timestamp = {2010.12.05}
}

@ARTICLE{Aner-Wolf2004,
  author = {Aya Aner-Wolf and John R. Kender},
  title = {Video summaries and cross-referencing through mosaic-based representation},
  journal = {Comput. Vis. Image Underst.},
  year = {2004},
  volume = {95},
  pages = {201--237},
  number = {2},
  abstract = {We present an approach for compact video summaries that allows fast
	and direct access to video data. The video is segmented into shots
	and, in appropriate video genres, into scenes, using previously proposed
	methods. A new concept that supports an hierarchical representation
	of video is presented, and is based on physical setting and camera
	locations. We use mosaics to represent shots and then scenes. We
	use a novel method for mosaic comparison which is robust against
	changes in viewpoint and illumination. In contrast to approaches
	to video indexing which rely on a frame-based representation, our
	efficient mosaic-based representation allows fast clustering of scenes
	into physical settings, a new conceptual form grounded in the recognition
	of real-world backgrounds. We employ a technique for choosing representative
	mosaics for each physical setting, for a more compact representation
	and faster comparison between settings. This compact representation
	and comparison method runs in real time and allows fast and accurate
	summaries and comparison of scenes across different videos, and serves
	as a basis for indexing video libraries. We demonstrate our work
	using situation comedies (sitcoms), where each half-hour episode
	is well structured by rules governing background use. Consequently,
	browsing, indexing, and comparison across videos by physical setting
	is very fast. Further, we show that physical settings lead to a higher-level
	contextual identification of the main plots in each video. We demonstrate
	these contributions with a browsing tool whose top-level single page
	displays the settings of several episodes. In sports videos where
	settings are not as well defined, our approach allows classifying
	shots for characteristic event detection.},
  address = {New York, NY, USA},
  doi = {http://dx.doi.org/10.1016/j.cviu.2004.03.005},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Video summaries and cross-referencing through mosaic-based representation .pdf:PDF},
  owner = {danilo},
  publisher = {Elsevier Science Inc.},
  timestamp = {2008.12.03}
}

@ARTICLE{Athanasiadis2010,
  author = {Erast Athanasiadis and Sarandis Mitropoulos},
  title = {A distributed platform for personalized advertising in digital interactive
	TV environments},
  journal = {Journal of Systems and Software},
  year = {2010},
  volume = {83},
  pages = {1453 - 1469},
  number = {8},
  abstract = {Advertising plays an important role in modern free markets. Furthermore,
	advertising is moving towards the establishment of one-to-one marketing
	relationships. Thus, personalized advertisement is currently considered
	as a hot topic in product promotion as it can be proved beneficial
	for all the key players, such as the advertisers, the advertised
	companies, as well as the consumers. Interactive TV and WWW can provide
	the means for personalized advertising. But of course, special systems
	and platforms for personalization must be first developed. This paper
	proposes a prototype system which efficiently achieves the personalization
	of the advertisements in the environment of digital interactive TV.
	Thus, the environment for the exploitation of the proposed system
	are examined, the details in design and implementation are given,
	while extensive operation testing and evaluation are provided proving
	its high applicability in real business environments.}
}

@INPROCEEDINGS{Auephanwiriyakul1998,
  author = {Auephanwiriyakul, S. and Joshi, A. and Krishnapuram, R.},
  title = {Fuzzy shot clustering to support networked video databases},
  booktitle = {Proc. IEEE World Congress on Computational Intelligence. The 1998
	IEEE International Conference on Fuzzy Systems},
  year = {1998},
  volume = {2},
  pages = {1338--1343 vol.2},
  abstract = {Video querying involves a lot of user interaction and feedback based
	query refinement, which can generate large traffic volumes on the
	network if full video segments are sent. For efficient video browsing,
	search and retrieval, one need to find good compact representations
	for long video sequences. Representative frames (Rframes) provide
	such a representation. Extant algorithms use scene change detection
	to segment video into shots and pick Rframes. However, scene change
	detection techniques fail badly in presence of gradual scene changes
	which are quite prevalent in most videos. We present another way
	of finding Rframe using fuzzy clustering without dealing with any
	scene change detection algorithms. Fuzzy clusters provide a more
	natural approach to this problem since membership of a frame in some
	particular scene is not binary. This allows one to handle gradual
	scene changes. We report on our approach, the initial experimental
	results and future plans},
  doi = {10.1109/FUZZY.1998.686313},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Fuzzy shot clustering to support networked video databases.pdf:PDF},
  keywords = {database theory, fuzzy set theory, interactive television, pattern
	recognition, query processing, visual databases, fuzzy clustering,
	gradual scene change, networked video databases, pattern recognition,
	query process, representative frames, video shot clustering},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@INCOLLECTION{Baek2005,
  author = {Baek, Joon-Sik and Lee, Soon-Tak and Baek, Joong-Hwan},
  title = {Scene Boundary Detection by Audiovisual Contents Analysis},
  year = {2005},
  pages = {530--539},
  abstract = {Scene boundary detection is an essential research in content-based
	video summary, retrieval, and browsing. In this paper, we present
	an efficient and robust scene extraction algorithm. The proposed
	algorithm consists of three stages. The first stage is shot boundary
	detection, and the second stage is the musical scene boundary detection
	through detection of musical shot. In the last stage, scene detection
	among non-musical shots is accomplished. In order to detect musical
	shots, audio categorization is accomplished on audio clips that are
	divided into visual shot unit. Then low level audio features are
	calculated for categorization of audio clips. Finally, the parts
	of video which are containing music component are discriminated on
	the assumption that the shots in a scene contain same background
	music. In scene change detection among non-musical shots, distance
	matrix among shots is calculated based on visual information and
	time distances between each shot. To provide a reasonable limitation
	of time distance, variable length time-window method is proposed.
	The scene boundaries are detected by using shot clustering and scene
	formation.},
  citeulike-article-id = {3252844},
  doi = {http://dx.doi.org/10.1007/11589990\_55},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Scene Boundary Detection by Audiovisual Contents Analysis.pdf:PDF},
  journal = {AI 2005: Advances in Artificial Intelligence},
  owner = {danilo},
  posted-at = {2008-09-13 20:05:22},
  priority = {2},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.09.13},
  url = {http://dx.doi.org/10.1007/11589990\_55}
}

@BOOK{Modern1999,
  title = {Modern Information Retrieval},
  publisher = {Addison-Wesley Longman Publishing Co., Inc.},
  year = {1999},
  author = {Baeza-Yates, Ricardo A. and Ribeiro-Neto, Berthier},
  address = {Boston, MA, USA}
}

@INPROCEEDINGS{Barla2003,
  author = {Barla, A. and Odone, F. and Verri, A.},
  title = {Histogram intersection kernel for image classification},
  booktitle = {Image Processing, 2003. ICIP 2003. Proceedings. 2003 International
	Conference on},
  year = {2003},
  volume = {3},
  pages = {513--516}
}

@INPROCEEDINGS{barrios2005,
  author = {V. M. G. Barrios and F. Mödritscher and C. Gütl},
  title = {{Personalization versus Adaptation? A User-centred Model Approach
	and its Application}},
  booktitle = {Proceedings of I-KNOW'05},
  year = {2005},
  pages = {120--127},
  location = {Graz, Australia}
}

@ARTICLE{Benini2006,
  author = {S. Benini and A. Bianchetti and R. Leonardi and P. Migliorati},
  title = {Hierarchical Summarization of Videos by Tree-Structured Vector Quantization},
  journal = {icme},
  year = {2006},
  volume = {0},
  pages = {969-972},
  abstract = {Accurate grouping of video shots could lead to semantic indexing of
	video segments for content analysis and retrieval. This paper introduces
	a novel cluster analysis which, depending both on the video genre
	and the specific user needs, produces a hierarchical representation
	of the video only on a reduced number of significant summaries. An
	outlook on a possible implementation strategy is then suggested.
	Specifically, vector-quantization codebooks are used to represent
	the visual content and to cluster the shots with a similar chromatic
	consistency. The evaluation of the codebook distortion introduced
	in each cluster is used to stop the procedure on few levels, exploiting
	the dependency relationships between clusters. Finally, the user
	can navigate through summaries at each hierarchical level and then
	decide which level to adopt for eventual post-processing. The effectiveness
	of the proposed method is validated through a series of experiments
	on real visual-data excerpted from different kinds of programmes.},
  address = {Los Alamitos, CA, USA},
  doi = {http://doi.ieeecomputersociety.org/10.1109/ICME.2006.262694},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Hierarchical Summarization of Videos by Tree-Structured Vector Quantization.pdf:PDF},
  isbn = {1-4244-0366-7},
  owner = {danilo},
  publisher = {IEEE Computer Society},
  timestamp = {2008.12.05}
}

@ARTICLE{bertini2006,
  author = {M. Bertini and R. Cucchuara and A. D. Bimbo and A. Prati},
  title = {{Semantic Adaptation of Sport Videos With User-Centred Performance
	Analysis}},
  journal = {IEEE Transactions on Multimedia},
  year = {2006},
  volume = {8},
  pages = {433--443},
  number = {3}
}

@INPROCEEDINGS{bilenko2004,
  author = {M. Bilenko and S. Basu and R. J. Mooney},
  title = {Integrating constraints and metric learning in semi-supervised clustering},
  booktitle = {Proceedings of the 21st. International Conference on Machine Learning
	(ICML)},
  year = {2004},
  pages = {81--88}
}

@TECHREPORT{Biolchini2005,
  author = {Biolchini, J. and Mian, P. and Natali, A. and Travassos, G.},
  title = {Systematic review in software engineering: Relevance and utility},
  institution = {COPPE/UFRJ},
  year = {2005},
  owner = {danilobc},
  timestamp = {2008.09.28}
}

@ARTICLE{Bo2005,
  author = {Bo, Cai and Lu, Zhang and Dong-Ru, Zhou},
  title = {A study of video scenes clustering based on shot key frames},
  journal = {Wuhan University Journal of Natural Sciences},
  year = {2005},
  volume = {10},
  pages = {966--970},
  number = {6},
  month = {November},
  abstract = {In digital video analysis, browse, retrieval and query, shot is incapable
	of meeting needs. Scene is a cluster of a series of shots, which
	partially meets above demands. In this paper, an algorithm of video
	scenes clustering based on shot key frame sets is proposed. We useX
	2 histogram match and twin histogram comparison for shot detection.
	A method is presented for key frame set extraction based on distance
	of non adjacent frames, further more, the minimum distance of key
	frame sets as distance of shots is computed, eventually scenes are
	clustered according to the distance of shots. Experiments of this
	algorithm show satisfactory performance in correctness and computing
	speed.},
  citeulike-article-id = {3252953},
  doi = {http://dx.doi.org/10.1007/BF02832449},
  owner = {danilo},
  posted-at = {2008-09-13 20:24:53},
  priority = {2},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.09.13},
  url = {http://dx.doi.org/10.1007/BF02832449}
}

@BOOK{Boggs2000,
  title = {The Art of Watching Films},
  publisher = {Mayfield Publishing Company},
  year = {2000},
  editor = {Moutain View},
  author = {J. M. Boggs and D. W. Petrie},
  edition = {5th},
  owner = {danilocoimbra},
  timestamp = {2010.12.16}
}

@INPROCEEDINGS{Bouthemy1999,
  author = {Patrick Bouthemy and Christophe Garcia and R\'{e}mi Ronfard and George
	Tziritas and Emmanuel Veneau and Didier Zugaj},
  title = {Scene Segmentation and Image Feature Extraction for Video Indexing
	and Retrieval},
  booktitle = {VISUAL '99: Proceedings of the Third International Conference on
	Visual Information and Information Systems},
  year = {1999},
  pages = {245--252},
  address = {London, UK},
  publisher = {Springer-Verlag},
  abstract = {We present a video analysis and indexing engine, that can perform
	fully automatic scene segmentation and feature extraction, in the
	context of a television archive, based on a library of image analysis
	functions and templates.},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Scene Segmentation and Image Feature Extraction for Video Indexing and Retrieval .pdf:PDF},
  isbn = {3-540-66079-8},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@ARTICLE{Brezeale2008,
  author = {Brezeale, D. and Cook, D.J.},
  title = {Automatic Video Classification: A Survey of the Literature},
  journal = {Systems, Man, and Cybernetics, Part C: Applications and Reviews,
	IEEE Transactions on},
  year = {2008},
  volume = {38},
  pages = {416 -430},
  number = {3},
  month = may,
  keywords = {automatic video classification;video retrieval;classification;video
	retrieval;}
}

@PHDTHESIS{Bulcao2006,
  author = {Bulcão Neto, R. F.},
  title = {Um processo de software e um modelo ontológico para apoio ao desenvolvimento
	de aplicações sensíveis ao contexto},
  school = {ICMC-USP},
  year = {2006},
  address = {São Carlos},
  owner = {danilo},
  timestamp = {2009.02.23}
}

@ARTICLE{Calic2002,
  author = {Calic,, Janko and Izquierdo,, Ebroul},
  title = {Temporal segmentation of MPEG video streams},
  journal = {EURASIP J. Appl. Signal Process.},
  year = {2002},
  volume = {2002},
  pages = {561--565},
  number = {1},
  address = {New York, NY, United States},
  publisher = {Hindawi Publishing Corp.}
}

@ARTICLE{Cao2007,
  author = {Cao, Jian-Rong},
  title = {Algorithm of Scene Segmentation Based on SVM for Scenery Documentary},
  year = {2007},
  pages = {95--98},
  address = {Washington, DC, USA},
  booktitle = {ICNC '07: Proceedings of the Third International Conference on Natural
	Computation (ICNC 2007)},
  doi = {http://dx.doi.org/10.1109/ICNC.2007.166},
  publisher = {IEEE Computer Society}
}

@INCOLLECTION{Cao2004,
  author = {Cao, Yu and Tavanapong, Wallapak and Li, Dalei and Oh, JungHwan and
	Groen, Piet C. de and Wong, Johnny},
  title = {A Visual Model Approach for Parsing Colonoscopy Videos},
  booktitle = {Image and Video Retrieval},
  publisher = {Springer Berlin/Heidelberg},
  year = {2004},
  editor = {Enser, Peter and Kompatsiaris, Yiannis and O'Connor, Noel E. and
	Smeaton, Alan F. and Smeulders, Arnold W. M.},
  volume = {3115},
  series = {Lecture Notes in Computer Science},
  pages = {1969-1969},
  affiliation = {Department of Computer Science, Iowa State University, Ames, IA 50011-1040
	USA}
}

@ARTICLE{Cavallaro2003,
  author = {Cavallaro, A. and Steiger, O. and Ebrahimi, T.},
  title = {Semantic segmentation and description for video transcoding},
  journal = {ICME '03: Proceedings of the 2003 International Conference on Multimedia
	and Expo (ICME '03)},
  year = {2003},
  volume = {3},
  pages = {597-600},
  keywords = { image representation, image segmentation, video coding automatic
	content-based video transcoding algorithm, code non-relevant object,
	frame-based encoder, humans visual information, multiple video objects,
	objects-based encoder, semantic segmentation, video representation}
}

@ARTICLE{Chaisorn2003,
  author = {Chaisorn, Lekha and Chua, Tat-Seng and Lee, Chin-Hui },
  title = {A Multi-Modal Approach to Story Segmentation for News Video},
  journal = {World Wide Web},
  year = {2003},
  volume = {6},
  pages = {187--208},
  number = {2},
  month = {June},
  abstract = {This research proposes a two-level, multi-modal framework to perform
	the segmentation and classification of news video into single-story
	semantic units. The video is analyzed at the shot and story unit
	(or scene) levels using a variety of features and techniques. At
	the shot level, we employ Decision Trees technique to classify the
	shots into one of 13 predefined categories or mid-level features.
	At the scene/story level, we perform the HMM (Hidden Markov Models)
	analysis to locate story boundaries. Our initial results indicate
	that we could achieve a high accuracy of over 95\% for shot classification,
	and over 89\% in F1 measure on scene/story boundary detection. Detailed
	analysis reveals that HMM is effective in identifying dominant features,
	which helps in locating story boundaries. Our eventual goal is to
	support the retrieval of news video at story unit level, together
	with associated texts retrieved from related news sites on the web.}
}

@ARTICLE{Chang2001,
  author = {Shih-Fu Chang and Sikora, T. and Purl, A.},
  title = {Overview of the MPEG-7 standard},
  journal = {Circuits and Systems for Video Technology, IEEE Transactions on},
  year = {2001},
  volume = {11},
  pages = {688-695},
  number = {6},
  month = {Jun},
  doi = {10.1109/76.927421},
  issn = {1051-8215},
  keywords = {IEC standards, ISO standards, audio-visual systems, meta data, multimedia
	systems, telecommunication standardsISO/IEC standard, MPEG-7 standard,
	Multimedia Content Description Interface, audio-visual information,
	content broadcasting, content description, description definition
	language, description schemes, descriptors, granularity levels, language,
	management, media portals, metadata standards, multimedia, navigation,
	organization, standardized tools, structural descriptions, user interaction,
	video segment}
}

@INPROCEEDINGS{Chang2000,
  author = {Chang, Shih-Fu and Sundaram, H.},
  title = {Structural and semantic analysis of video},
  booktitle = {Proc. IEEE International Conference on Multimedia and Expo ICME 2000},
  year = {2000},
  volume = {2},
  pages = {687--690 vol.2},
  abstract = {We discuss our recent research and open issues in structural and semantic
	analysis of digital video. Specifically, we focus on segmentation,
	summarization and classification of digital video. In each area,
	we also emphasize the importance of understanding domain-specific
	characteristics. In scene segmentation, we introduce the idea of
	a computable scene as a chunk of audio-visual data that exhibits
	long-term consistency with regard to several audio-visual properties.
	In summarization, we discuss shot and program level summaries. We
	describe classification schemes based on Bayesian networks, which
	model interaction of multiple classes at different levels using multimedia.
	We also discuss classification techniques that exploit domain-specific
	spatial structural constraints as well as temporal transitional models},
  doi = {10.1109/ICME.2000.871455},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Structural and semantic analysis of video .pdf:PDF},
  keywords = {belief networks, computational linguistics, image classification,
	image segmentation, video signal processing, Bayesian networks, audio-visual
	properties, classification, computable scene, digital video, domain-specific
	characteristics, multimedia, scene segmentation, segmentation, semantic
	analysis, structural analysis, summarization},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@ARTICLE{Chang2005,
  author = {Chang, S.-F. and Vetro, A.},
  title = {Video Adaptation: Concepts, Technologies, and Open Issues},
  journal = {Proceedings of the IEEE},
  year = {2005},
  volume = {93},
  pages = {148-158},
  number = {1},
  abstract = {Video adaptation is an emerging field that offers a rich body of techniques
	for answering challenging questions in pervasive media applications.
	It transforms the input video(s) to an output in video or augmented
	multimedia form by utilizing manipulations at multiple levels (signal,
	structural, or semantic) in order to meet diverse resource constraints
	and user preferences while optimizing the overall utility of the
	video. There has been a vast amount of activity in research and standard
	development in this area. This paper first presents a general framework
	that defines the fundamental entities, important concepts (i.e.,
	adaptation, resource, and utility), and formulation of video adaptation
	as constrained optimization problems. A taxonomy is used to classify
	different types of adaptation techniques. The state of the art in
	several active research areas is reviewed with open challenging issues
	identified. Finally, support of video adaptation from related international
	standards is discussed.},
  doi = {10.1109/JPROC.2004.839600},
  keywords = {multimedia communication, optimisation, video codingaugmented multimedia
	form, constrained optimization problems, diverse resource constraints,
	pervasive media applications, taxonomy, video adaptation}
}

@INPROCEEDINGS{Chen2010,
  author = {Hui Chen and Cuihua Li},
  title = {A practical method for video scene segmentation},
  booktitle = {Computer Science and Information Technology (ICCSIT), 2010 3rd IEEE
	International Conference on},
  year = {2010},
  volume = {9},
  pages = {153 --156},
  abstract = {Video segmentation is a crucial pass to content-based video summarization
	and retrieval. In this paper, we present a practical method to efficiently
	group video content into semantic segments. First we detect shots
	with double-threshold method to find raw shots quickly, followed
	by redundant frames removal though spatial color distribution to
	get the key frames. Finally, we cluster the key frames using the
	inter-shot correlation via domain color histogram and motion intensity
	to get the final scenes.},
  keywords = {content based video retrieval;content based video summarization;domain
	color histogram;inter shot correlation;key frames cluster;motion
	intensity;spatial color distribution;video scene segmentation;content-based
	retrieval;image colour analysis;image motion analysis;image segmentation;image
	sequences;pattern clustering;video retrieval;}
}

@CONFERENCE{Chen2003,
  author = {Lei Chen and Shariq J. Rizvi and M. Tamer Ozsu},
  title = {Incorporating Audio Cues into Dialog and Action Scene Extraction},
  year = {2003},
  editor = {Minerva M. Yeung and Rainer W. Lienhart and Chung-Sheng Li},
  volume = {5021},
  number = {1},
  pages = {252-263},
  publisher = {SPIE},
  doi = {10.1117/12.476317},
  journal = {Storage and Retrieval for Media Databases 2003},
  location = {Santa Clara, CA, USA},
  url = {http://link.aip.org/link/?PSI/5021/252/1}
}

@ARTICLE{Chen2008a,
  author = {Liang-Hua Chen and Yu-Chun Lai and Hong-Yuan Mark Liao},
  title = {Movie scene segmentation using background information},
  journal = {Pattern Recogn.},
  year = {2008},
  volume = {41},
  pages = {1056--1065},
  number = {3},
  abstract = {Scene extraction is the first step toward semantic understanding of
	a video. It also provides improved browsing and retrieval facilities
	to users of video database. This paper presents an effective approach
	to movie scene extraction based on the analysis of background images.
	Our approach exploits the fact that shots belonging to one particular
	scene often have similar backgrounds. Although part of the video
	frame is covered by foreground objects, the background scene can
	still be reconstructed by a mosaic technique. The proposed scene
	extraction algorithm consists of two main components: determination
	of the shot similarity measure and a shot grouping process. In our
	approach, several low-level visual features are integrated to compute
	the similarity measure between two shots. On the other hand, the
	rules of film-making are used to guide the shot grouping process.
	Experimental results show that our approach is promising and outperforms
	some existing techniques.},
  address = {New York, NY, USA},
  doi = {http://dx.doi.org/10.1016/j.patcog.2007.07.024},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Movie scene segmentation using background information.pdf:PDF},
  owner = {danilo},
  publisher = {Elsevier Science Inc.},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.11.06}
}

@CONFERENCE{Chen2010a,
  author = {Ming-yu Chen and Huan Li and Alexander Hauptmann},
  title = {Combining motion understanding and keyframe image analysis for broadcast
	video information extraction},
  year = {2010},
  editor = {Teresa H. O'Donnell and Misty Blowers and Kevin L. Priddy},
  volume = {7704},
  number = {1},
  pages = {77040H},
  publisher = {SPIE},
  doi = {10.1117/12.853465},
  eid = {77040H},
  journal = {Evolutionary and Bio-Inspired Computation: Theory and Applications
	IV},
  location = {Orlando, Florida, USA},
  numpages = {9},
  url = {http://link.aip.org/link/?PSI/7704/77040H/1}
}

@INPROCEEDINGS{Chen2002,
  author = {Chen, Shu-Ching and Shyu, Mei-Ling and Liao, Wenhui and Zhang, Chengcui},
  title = {Scene change detection by audio and video clues},
  booktitle = {Proc. IEEE International Conference on Multimedia and Expo ICME '02},
  year = {2002},
  volume = {2},
  pages = {365--368 vol.2},
  abstract = {Automatic video scene change detection is a challenging task. Using
	audio or visual information alone often cannot provide a satisfactory
	solution. However, how to combine audio and visual information efficiently
	still remains a difficult issue since there are various cases in
	their relationship due to the versatility of videos. We present an
	effective scene change detection method that adopts the joint evaluation
	of the audio and visual features. First, video information is used
	to find the shot boundaries. Second, the audio features for each
	video shot can be extracted. Lastly, an audio-video combination schema
	is proposed to detect the video scene boundaries.},
  doi = {10.1109/ICME.2002.1035606},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Scene change detection by audio and video clues.pdf:PDF},
  keywords = {audio signal processing, feature extraction, image segmentation, image
	sequences, video signal processing, audio clues, audio feature extraction,
	audio features evaluation, audio visual information, audio-video
	combination schema, automatic video scene change detection, shot
	boundaries, video clues, video frame segmentation, video scene boundaries
	detection, visual features evaluation},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@ARTICLE{Choi2010,
  author = {Choi, Y. and Lee, J.},
  title = {Reliability and Validity of Scene Unit Coding in the Visual Content
	Analysis},
  journal = {Annual Meeting of the International Communication Association},
  year = {2010},
  pages = {40},
  owner = {danilocoimbra},
  timestamp = {2010.12.05}
}

@ARTICLE{Choi2006,
  author = {Choi, Y. and Lee, J.},
  title = {The role of scene in framing a story: An analysis of scenes position,
	length and dominance in a story},
  journal = {Journal of Broadcasting and Electronic Media},
  year = {2006},
  volume = {50},
  pages = {703--722}
}

@INPROCEEDINGS{Chua2004,
  author = {Chua, Tat-Seng and Chang, Shih-Fu and Chaisorn, Lekha and Hsu, Winston},
  title = {Story boundary detection in large broadcast news video archives:
	techniques, experience and trends},
  booktitle = {Proceedings of the 12th annual ACM international conference on Multimedia},
  year = {2004},
  series = {MULTIMEDIA '04},
  pages = {656--659},
  address = {New York, NY, USA},
  publisher = {ACM},
  isbn = {1-58113-893-8},
  keywords = {machine learning techniques, news video, story segmentation},
  location = {New York, NY, USA},
  numpages = {4}
}

@INPROCEEDINGS{Danilo2009,
  author = {D.B. Coimbra and R. Goularte},
  title = {Identificação de Cenas em Vídeos Digitais Utilizando Características
	Audiovisuais},
  booktitle = {Proceedings of the XV Brazilian Symposium on Multimedia and the Web
	(WebMedia'09)},
  year = {2009},
  volume = {2},
  pages = {43-46},
  timestamp = {2010.04.20}
}

@INPROCEEDINGS{Colace2005,
  author = {Colace, F. and Foggia, P. and Percannella, G.},
  title = {A Probabilistic Framework for TV-News Stories Detection and Classification},
  booktitle = {Multimedia and Expo, 2005. ICME 2005. IEEE International Conference
	on},
  year = {2005},
  pages = {1350 -1353},
  month = july,
  doi = {10.1109/ICME.2005.1521680},
  keywords = {Bayesian network;Italian news;TV-news story detection;anchor detection;audio
	tracking;classification;hidden Markov model;information extraction;multilevel
	probabilistic framework;phase segmentation;speaker identification;speech
	transcription;superimposed text recognition;video database;video
	tracking;belief networks;character recognition;feature extraction;hidden
	Markov models;pattern classification;speech recognition;video signal
	processing;}
}

@INCOLLECTION{Cordelia2002,
  author = {Cordelia, L. and De Santo, M. and Percannella, G. and Sansone, C.
	and Vento, M.},
  title = {A Multi-expert System for Movie Segmentation},
  year = {2002},
  pages = {657--661},
  abstract = {In this paper we present a system for movie segmentation based on
	the automatic detection of dialogue scenes. The proposed system processes
	the video stream directly in the MPEG domain: it starts with the
	segmentation of the video footage in shots. Then, a characterization
	of each shot between dialogue and not-dialogue according to a Multi-Expert
	System (MES) is performed. Finally, the individuated sequences of
	shots are aggregated in dialogue scenes by means of a suitable algorithm.
	The MES integrates three experts, which classifies a given shot on
	the basis of very complementary descriptions; in particular an audio
	classifier, a face detector and a camera motion estimator have been
	built up and employed. The performance of the system have been tested
	on a huge MPEG movie database made up of more than 15000 shots and
	200 scenes, giving rise to encouraging results.},
  citeulike-article-id = {3388296},
  doi = {http://dx.doi.org/10.1007/3-540-45428-4\_30},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\A Multi-expert System for Movie Segmentation .pdf:PDF},
  journal = {Multiple Classifier Systems},
  owner = {danilo},
  posted-at = {2008-10-08 20:48:49},
  priority = {2},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09},
  url = {http://dx.doi.org/10.1007/3-540-45428-4\_30}
}

@ARTICLE{Correia2004,
  author = {Correia, P.L. and Pereira, F.},
  title = {Classification of video segmentation application scenarios},
  journal = {Circuits and Systems for Video Technology, IEEE Transactions on},
  year = {2004},
  volume = {14},
  pages = { 735-741},
  number = {5},
  doi = {10.1109/TCSVT.2004.826778},
  keywords = { image segmentation, interactive systems, real-time systems, video
	signal processing automatic tools configuration parameters, real-time-nonuser-interactive
	scenarios, user interaction scenarios, video segmentation classification}
}

@INPROCEEDINGS{Danna2007,
  author = {D'Anna, L. and Percannella, G. and Sansone, C. and Vento, M.},
  title = {A Multi-Stage Approach for News Video Segmentation Based on Automatic
	Anchorperson Number Detection},
  booktitle = {Mobile Ubiquitous Computing, Systems, Services and Technologies,
	2007. UBICOMM '07. International Conference on},
  year = {2007},
  pages = {229 -234},
  month = nov.,
  doi = {10.1109/UBICOMM.2007.35},
  keywords = {audio-video templates;automatic anchorperson number detection;automatic
	selector;multistage approach;news-based digital libraries;video segmentation;image
	segmentation;object detection;video signal processing;}
}

@INCOLLECTION{Damnjanovic2007,
  author = {Damnjanovic, Uros and Piatrik, Tomas and Djordjevic, Divna and Izquierdo,
	Ebroul},
  title = {Video Summarisation for Surveillance and News Domain},
  year = {2007},
  pages = {99--112},
  abstract = {Video summarization approaches have various fields of application,
	specifically related to organizing, browsing and accessing large
	video databases. In this paper we propose and evaluate two novel
	approaches for video summarization, one based on spectral methods
	and the other on ant-tree clustering. The overall summary creation
	process is broke down in two steps: detection of similar scenes and
	extraction of the most representative ones. While clustering approaches
	are used for scene segmentation, the post-processing logic merges
	video scenes into a subset of user relevant scenes. In the case of
	the spectral approach, representative scenes are extracted following
	the logic that important parts of the video are related with high
	motion activity of segments within scenes. In the alternative approach
	we estimate a subset of relevant video scene using ant-tree optimization
	approaches and in a supervised scenario certain scenes of no interest
	to the user are recognized and excluded from the summary. An experimental
	evaluation validating the feasibility and the robustness of these
	approaches is presented.},
  citeulike-article-id = {3253050},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Video Summarisation for Surveillance and News Domain.pdf:PDF},
  journal = {Semantic Multimedia},
  owner = {danilo},
  posted-at = {2008-09-13 21:22:45},
  priority = {2},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.09.13}
}

@INPROCEEDINGS{Desanto2006,
  author = {De Santo, M. and Foggia, P. and Sansone, C. and Percannella, G. and
	Vento, M.},
  title = {An Unsupervised Algorithm for Anchor Shot Detection},
  booktitle = {Pattern Recognition, 2006. ICPR 2006. 18th International Conference
	on},
  year = {2006},
  volume = {2},
  pages = {1238 -1241},
  doi = {10.1109/ICPR.2006.266},
  issn = {1051-4651},
  keywords = {anchor shot detection;clustering method;news video segmentation;news-based
	digital library;pruning technique;unsupervised algorithm;image segmentation;object
	detection;pattern clustering;video signal processing;}
}

@INCOLLECTION{Desanto2006a,
  author = {De Santo, M. and Percannella, G. and Sansone, C. and Vento, M.},
  title = {Unsupervised News Video Segmentation by Combined Audio-Video Analysis},
  booktitle = {Multimedia Content Representation, Classification and Security},
  publisher = {Springer Berlin / Heidelberg},
  year = {2006},
  editor = {Gunsel, Bilge and Jain, Anil and Tekalp, A. and Sankur, Bülent},
  volume = {4105},
  series = {Lecture Notes in Computer Science},
  pages = {273-281},
  affiliation = {Dip. di Ingegneria dell’Informazione ed Ingegneria Elettrica, Università
	degli Studi di Salerno, Via Ponte Don Melillo, I, I-84084, Fisciano
	(SA) Italy Italy}
}

@BOOK{Deb2005,
  title = {Video Data Management and Information Retrieval},
  publisher = {IRM Press},
  year = {2005},
  author = {Deb,, Sagarmay},
  isbn = {1591405475}
}

@INPROCEEDINGS{Deb2004,
  author = {Deb, Sagarmay and Zhang, Yanchun},
  title = {An Overview of Content-based Image Retrieval Techniques},
  booktitle = {AINA '04: Proceedings of the 18th International Conference on Advanced
	Information Networking and Applications},
  year = {2004},
  pages = {59},
  address = {Washington, DC, USA},
  publisher = {IEEE Computer Society}
}

@ARTICLE{Dimitrova2002,
  author = {Dimitrova, N. and Hong-Jiang Zhang and Shahraray, B. and Sezan, I.
	and Huang, T. and Zakhor, A.},
  title = {Applications of video-content analysis and retrieval},
  journal = {Multimedia, IEEE},
  year = {2002},
  volume = {9},
  pages = { 42-55},
  number = {3},
  doi = {10.1109/MMUL.2002.1022858},
  keywords = { content-based retrieval, multimedia databases, video databases computer
	networks, multimedia data management, survey, video database, video-content
	analysis, video-content retrieval}
}

@INCOLLECTION{Ding2005,
  author = {Ding, Dayong and Chen, Le and Zhang, Bo},
  title = {Temporal Shot Clustering Analysis for Video Concept Detection},
  year = {2005},
  pages = {558--560},
  abstract = {The phenomenon that conceptually related shots appear together in
	videos is called temporal shot clustering. This phenomenon is a useful
	cue for video concept detection, which is one of basic steps in content-based
	video indexing and retrieval. We propose a method, called temporal
	shot clustering analysis, to improve results of video concept detection
	by exploiting the temporal shot clustering phenomenon. Two other
	methods are compared with temporal shot clustering analysis on the
	TRECVID 2003 dataset. Experiments showed that temporal shot clustering
	is of much benefit for video concept detection, and that temporal
	shot clustering method outperforms the other methods.},
  citeulike-article-id = {3252968},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Temporal Shot Clustering Analysis for Video Concept Detection.pdf:PDF},
  journal = {Advances in Information Retrieval},
  owner = {danilo},
  posted-at = {2008-09-13 20:36:56},
  priority = {2},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.09.13},
  url = {http://www.springerlink.com/content/q5t9rgumafykrxlg}
}

@INPROCEEDINGS{Ding2003,
  author = {Dawei Ding and Qing Li and Bo Feng and Liu Wenyin},
  title = {A semantic model for flash retrieval using co-occurrence analysis},
  booktitle = {MULTIMEDIA '03: Proceedings of the eleventh ACM international conference
	on Multimedia},
  year = {2003},
  pages = {239--242},
  address = {New York, NY, USA},
  publisher = {ACM},
  abstract = {The development of mid-level shot description helps to bridge the
	gap between low-level feature and high-level semantics in video indexing
	and analysis. In this paper, we present a unified framework for semantic
	shot representation in field-ball sports genres, in which a video
	shot is characterized via three essential properties, namely, camera
	shot size, subject in a scene and video production technology. The
	three properties clearly represent the primary factors of a shot,
	and provide a unified viewpoint of semantic shot definition. Based
	on this framework, we design an effective architecture for semantic
	shot management comprising three main components as: 1) flexible
	shot clustering and retrieval by adjusting the weights of three properties
	according to different requirements; 2) semantics based video temporal
	segmentation for further event recognition; and 3) comprehensive
	sports video semantics analysis. Extensive experiments on soccer,
	basketball and tennis demonstrate the effectiveness and validity
	of this framework.},
  doi = {http://doi.acm.org/10.1145/957013.957061},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\A semantic model for flash retrieval using co-occurrence analysis.pdf:PDF},
  isbn = {1-58113-722-2},
  location = {Berkeley, CA, USA},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@ARTICLE{Dong2006,
  author = {Dong, A. and Honglin Li},
  title = {Semantic Segmentation of Documentary Video using Music Breaks},
  journal = {Multimedia and Expo, 2006 IEEE International Conference on},
  year = {2006},
  pages = {1825-1828},
  doi = {10.1109/ICME.2006.262908},
  keywords = {document image processing, image segmentation, music, speech processing,
	video streamingdocumentary video stream, music break, semantic video
	segmentation, speech text}
}

@ARTICLE{Doulamis2000,
  author = {Doulamis, N.D. and Doulamis, A.D. and Avrithis, Y.S. and Ntalianis,
	K.S. and Kollias, S.D.},
  title = {Efficient summarization of stereoscopic video sequences},
  journal = IEEE_J_CASVT,
  year = {2000},
  volume = {10},
  pages = {501--517},
  number = {4},
  abstract = {An efficient technique for summarization of stereoscopic video sequences
	is presented, which extracts a small but meaningful set of video
	frames using a content-based sampling algorithm. The proposed video-content
	representation provides the capability of browsing digital stereoscopic
	video sequences and performing more efficient content-based queries
	and indexing. Each stereoscopic video sequence is first partitioned
	into shots by applying a shot-cut detection algorithm so that frames
	(or stereo pairs) of similar visual characteristics are gathered
	together. Each shot is then analyzed using stereo-imaging techniques,
	and the disparity field, occluded areas, and depth map are estimated.
	A multiresolution implementation of the recursive shortest spanning
	tree (RSST) algorithm is applied for color and depth segmentation,
	while fusion of color and depth segments is employed for reliable
	video object extraction. In particular, color segments are projected
	onto depth segments so that video objects on the same depth plane
	are retained, while at the same time accurate object boundaries are
	extracted. Feature vectors are then constructed using multidimensional
	fuzzy classification of segment features including size, location,
	color, and depth. Shot selection is accomplished by clustering similar
	shots based on the generalized Lloyd-Max algorithm, while for a given
	shot, key frames are extracted using an optimization method for locating
	frames of minimally correlated feature vectors. For efficient implementation
	of the latter method, a genetic algorithm is used. Experimental results
	are presented, which indicate the reliable performance of the proposed
	scheme on real-life stereoscopic video sequences},
  doi = {10.1109/76.844996},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Efficient summarization of stereoscopic video sequences.pdf:PDF},
  issn = {1051-8215},
  keywords = {content-based retrieval, edge detection, feature extraction, fuzzy
	logic, genetic algorithms, image classification, image colour analysis,
	image representation, image resolution, image sampling, image segmentation,
	image sequences, indexing, pattern clustering, stereo image processing,
	trees (mathematics), video signal processing, RSST algorithm, browsing,
	clustering, color segmentation, color segments, content-based queries,
	content-based sampling algorithm, depth map, depth segmentation,
	depth segments, digital stereoscopic video sequences, disparity field,
	feature vectors, fusion, generalized Lloyd-Max algorithm, genetic
	algorithm, indexing, location, multidimensional fuzzy classification,
	multiresolution implementation, object boundaries, occluded areas,
	optimization method, real-life stereoscopic video sequences, recursive
	shortest spanning tree algorithm, reliable video object extraction,
	shot selection, shot-cut detection algorithm, size, stereo pairs,
	stereo-imaging, stereoscopic video sequences, summarization, video
	frames, video-content representation},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@INPROCEEDINGS{Duan2006,
  author = {Ling-Yu Duan and Jinqiao Wang and Yantao Zheng and Jesse S. Jin and
	Hanqing Lu and Changsheng Xu},
  title = {Segmentation, categorization, and identification of commercial clips
	from TV streams using multimodal analysis},
  booktitle = {MULTIMEDIA '06: Proceedings of the 14th annual ACM international
	conference on Multimedia},
  year = {2006},
  pages = {201--210},
  address = {New York, NY, USA},
  publisher = {ACM},
  abstract = {TV advertising is ubiquitous, perseverant, and economically vital.
	Millions of people's living and working habits are affected by TV
	commercials. In this paper, we present a multimodal ("visual + audio
	+ text") commercial video digest scheme to segment individual commercials
	and carry out semantic content analysis within a detected commercial
	segment from TV streams.Two challenging issues are addressed. Firstly,
	we propose a multimodal approach to robustly detect the boundaries
	of individual commercials. Secondly, we attempt to classify a commercial
	with respect to advertised products/services. For the first, the
	boundary detection of individual commercials is reduced to the problem
	of binary classification of shot boundaries via the mid-level features
	derived from two concepts: Image Frames Marked with Product Information
	(FMPI) and Audio Scene Change Indicator (ASCI). Moreover, the accurate
	individual boundary enables us to perform commercial identification
	by clip matching via a spatial-temporal signature. For the second,
	commercial classification is formulated as the task of text categorization
	by expanding sparse texts from ASR/OCR with external knowledge. Our
	boundary detection has achieved a good result of F1 = 93.7% on the
	dataset comprising 499 individual commercials from TRECVID'05 video
	corpus. Commercial classification has obtained a promising accuracy
	of 80.9% on 141 distinct ones. Based on these achievements, various
	applications such as an intelligent digital TV set-top box can be
	accomplished to enhance the TV viewer's capabilities in monitoring
	and managing commercials from TV streams.},
  doi = {http://doi.acm.org/10.1145/1180639.1180697},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Segmentation, categorization, and identification of commercial clips from TV streams using multimodal analysis.pdf:PDF},
  isbn = {1-59593-447-2},
  location = {Santa Barbara, CA, USA}
}

@INPROCEEDINGS{Dunlop2010,
  author = {Dunlop, H.},
  title = {Scene classification of images and video via semantic segmentation},
  booktitle = {Computer Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE
	Computer Society Conference on},
  year = {2010},
  pages = {72 -79},
  doi = {10.1109/CVPRW.2010.5543746},
  keywords = {hand labeled images;scene images classification;semantic segmentation;spatial
	pyramid;video classification;videos database;image classification;image
	segmentation;video signal processing;}
}

@ARTICLE{Ebrahimi2000,
  author = {Ebrahimi, T. and Horne, C.},
  title = {MPEG-4 Natural Video Coding - An Overview},
  journal = {Signal Processing: Image Communication},
  year = {2000},
  volume = {15},
  pages = {365-385},
  doi = {doi:10.1016/S0923-5965(99)00054-5}
}

@INPROCEEDINGS{Eickeler1999,
  author = {Eickeler, S. and Muller, S.},
  title = {Content-based video indexing of TV broadcast news using hidden Markov
	models},
  booktitle = {Acoustics, Speech, and Signal Processing, 1999. ICASSP '99. Proceedings.,
	1999 IEEE International Conference on},
  year = {1999},
  volume = {6},
  pages = {2997 -3000 vol.6},
  month = mar,
  abstract = {This paper presents a new approach to content-based video indexing
	using hidden Markov models (HMMs). In this approach one feature vector
	is calculated for each image of the video sequence. These feature
	vectors are modeled and classified using HMMs. This approach has
	many advantages compared to other video indexing approaches. The
	system has automatic learning capabilities. It is trained by presenting
	manually indexed video sequences. To improve the system we use a
	video model, that allows the classification of complex video sequences.
	The presented approach works three times faster than real-time. We
	tested our system on TV broadcast news. The rate of 97.3% correctly
	classified frames shows the efficiency of our system},
  doi = {10.1109/ICASSP.1999.757471},
  keywords = { TV broadcast news; automatic learning; classification; content-based
	video indexing; feature vector; hidden Markov models; manually indexed
	video sequences; video sequence; content-based retrieval; database
	indexing; feature extraction; hidden Markov models; image classification;
	image sequences; television broadcasting; video databases;}
}

@MISC{FAIRHURST2001,
  author = {G. FAIRHURST},
  title = {MPEG-2 Overview},
  year = {2001},
  organisation = {University of Aberdeen. Department of Engineering},
  url = {http://www.erg.abdn.ac.uk/research/future-net/digital-video/mpeg2.html},
  urlaccessdate = {Novembro de 2004}
}

@ARTICLE{Fan2008b,
  author = {Fan, Jianping and Gao, Yuli and Luo, Hangzai},
  title = {Integrating Concept Ontology and Multitask Learning to Achieve More
	Effective Classifier Training for Multilevel Image Annotation},
  journal = {Image Processing, IEEE Transactions on},
  year = {2008},
  volume = {17},
  pages = {407-426},
  number = {3},
  doi = {10.1109/TIP.2008.916999},
  keywords = {data visualisation, feature extraction, image classification, image
	representation, learning (artificial intelligence), ontologies (artificial
	intelligence), support vector machinesSVM image classifier training,
	feature extraction, global visual features, hierarchical boosting
	algorithm, image content representation, interactive hypothesis assessment,
	intraconcept visual diversity, large-scale image visualization, local
	visual features, multilevel image annotation, multiple kernel learning
	algorithm, multitask learning algorithm, ontology}
}

@ARTICLE{Fan2008,
  author = {Fan, Jianping and Gao, Yuli and Luo, Hangzai and Jain, R.},
  title = {Mining Multilevel Image Semantics via Hierarchical Classification},
  journal = {IEEE Transactions on Multimedia},
  year = {2008},
  volume = {10},
  pages = {167-187},
  number = {2},
  doi = {10.1109/TMM.2007.911775},
  keywords = {computational linguistics, data mining, data visualisation, feature
	extraction, image classification, learning (artificial intelligence),
	ontologies (artificial intelligence)computational complexity, data
	mining, hierarchical concept learning algorithm, hierarchical image
	classification, high-dimensional feature space, hyperbolic image
	visualization algorithm, mixture-expert product, multilevel image
	semantic, ontology, visual property}
}

@INPROCEEDINGS{Fang2006,
  author = {Yong Fang and Xiaofei Zhai and Jingwang Fan},
  title = {News video story segmentation},
  booktitle = {Multi-Media Modelling Conference Proceedings, 2006 12th International},
  year = {2006},
  pages = {4 pp.},
  month = {0-0},
  doi = {10.1109/MMMC.2006.1651357},
  keywords = {decision tree;hidden Markov model analysis;news video story segmentation;rule-based
	presegmentation module;shot level analysis;similarity measurement
	module;decision trees;hidden Markov models;image segmentation;video
	signal processing;}
}

@INPROCEEDINGS{Forlines2008,
  author = {Clifton Forlines},
  title = {Content aware video presentation on high-resolution displays},
  booktitle = {AVI '08: Proceedings of the working conference on Advanced visual
	interfaces},
  year = {2008},
  pages = {57--64},
  address = {New York, NY, USA},
  publisher = {ACM},
  abstract = {We describe a prototype video presentation system that presents a
	video in a manner consistent with the video's content. Our prototype
	takes advantage of the physically large display and pixel space that
	current high-definition displays and multi-monitor systems offer
	by rendering the frames of the video into various regions of the
	display surface. The structure of the video informs the animation,
	size, and the position of these regions. Additionally, previously
	displayed frames are often allowed to remain on-screen and are filtered
	over time. Our prototype presents a video in a manner that not only
	preserves the continuity of the story, but also supports the structure
	of the video; thus, the content of the video is reflected in its
	presentation, arguably enhancing the viewing experience.},
  doi = {http://doi.acm.org/10.1145/1385569.1385581},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Content aware video presentation on high-resolution displays.pdf:PDF},
  isbn = {0-978-60558-141-5},
  location = {Napoli, Italy},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@INPROCEEDINGS{Fu2006,
  author = {Fu, Chang-Jian and Li, Guo-Hui and Wu, Jun-Tao},
  title = {Video Hierarchical Structure Mining},
  booktitle = {Proc. International Conference on Communications, Circuits and Systems},
  year = {2006},
  volume = {3},
  pages = {2150--2154},
  abstract = {To structuralize video streams plays an important role in the processing
	of video. The basic structure for video is a hierarchical structure
	which consists of four kinds of components, namely frame, shot, scene,
	and video program. A simple framework for video hierarchical structure
	mining is to partition continuous video frames into discrete physical
	shots, extract features from video shots and construct scene structure
	based on shots. In this paper, two crucial algorithms of video hierarchical
	structure mining, multi-features shot clustering (MSC) and scene
	change detection (SCD), are proposed based on color, texture and
	semantic similarity of shot. Our experimental results demonstrate
	the performance of SCD is better than that of MSC},
  doi = {10.1109/ICCCAS.2006.284924},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Video Hierarchical Structure Mining.pdf:PDF},
  keywords = {data mining, feature extraction, image colour analysis, image texture,
	video streaming, MSC, SCD, features extraction, multifeatures shot
	clustering, scene change detection, video hierarchical structure
	mining, video stream},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.15}
}

@ARTICLE{Gao2006,
  author = {Gao, Wen and Huang, Qing-Ming and Jiang, Shu-Qiang and Zhang, Peng
	},
  title = {Sports video summarization and adaptation for application in mobile
	communication},
  journal = {Journal of Zhejiang University - Science A},
  year = {2006},
  volume = {7},
  pages = {819--829},
  number = {5},
  month = {May},
  abstract = {Sports video appeals to large audiences due to its high commercial
	potentials. Automatically extracting useful semantic information
	and generating highlight summary from sports video to facilitate
	usersâ€™ accessing requirements is an important problem, especially
	in the forthcoming broadband mobile communication and the need for
	users to access their multimedia information of interest from anywhere
	at anytime with their most convenient digital equipments. In this
	paper, a system to generate highlight summaries oriented for mobile
	applications is introduced, which includes highlight extraction and
	video adaptation. In this system, several highlight extraction techniques
	are provided for field sports video and racket sports video by using
	multi-modal information. To enhance usersâ€™ viewing experience and
	save bandwidth, 3D animation from highlight segment is also generated.
	As an important procedure to make video analysis results universally
	applicable, video transcoding techniques are applied to adapt the
	video for mobile communication environment and user preference. Experimental
	results are encouraging and show the advantage and feasibility of
	the system for multimedia content personalization, enhancement and
	adaptation to meet different user preference and network/device requirements.},
  citeulike-article-id = {3253060},
  doi = {http://dx.doi.org/10.1631/jzus.2006.A0819},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Sports video summarization and adaptation for application in mobile communication.pdf:PDF},
  owner = {danilo},
  posted-at = {2008-09-13 21:32:23},
  priority = {2},
  timestamp = {2008.09.13},
  url = {http://dx.doi.org/10.1631/jzus.2006.A0819}
}

@ARTICLE{Gatica-Perez2003,
  author = {Gatica-Perez, D. and Loui, A. and Ming-Ting Sun},
  title = {Finding structure in home videos by probabilistic hierarchical clustering},
  journal = {Circuits and Systems for Video Technology, IEEE Transactions on},
  year = {2003},
  volume = {13},
  pages = { 539-548},
  number = {6},
  month = {June},
  abstract = {Accessing, organizing, and manipulating home videos present technical
	challenges due to their unrestricted content and lack of storyline.
	We present a methodology to discover cluster structure in home videos,
	which uses video shots as the unit of organization, and is based
	on two concepts: (1) the development of statistical models of visual
	similarity, duration, and temporal adjacency of consumer video segments
	and (2) the reformulation of hierarchical clustering as a sequential
	binary Bayesian classification process. A Bayesian formulation allows
	for the incorporation of prior knowledge of the structure of home
	video and offers the advantages of a principled methodology. Gaussian
	mixture models are used to represent the class-conditional distributions
	of intra- and inter-segment visual and temporal features. The models
	are then used in the probabilistic clustering algorithm, where the
	merging order is a variation of highest confidence first, and the
	merging criterion is maximum a posteriori. The algorithm does not
	need any ad-hoc parameter determination. We present extensive results
	on a 10-h home-video database with ground truth which thoroughly
	validate the performance of our methodology with respect to cluster
	detection, individual shot-cluster labeling, and the effect of prior
	selection.},
  doi = {10.1109/TCSVT.2003.813428},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Finding structure in home videos by probabilistic hierarchical clustering.pdf:PDF},
  issn = {1558-2205},
  keywords = { Bayes methods, feature extraction, image classification, image segmentation,
	pattern clustering, probability, statistical analysis, video databases,
	video signal processing Bayesian formulation, Gaussian mixture models,
	class-conditional distributions, cluster detection, consumer video
	segments, duration, ground truth, home videos structure, home-video
	database, inter-segment temporal features, inter-segment visual features,
	intra-segment temporal features, intra-segment visual features, maximum
	a posteriori merging criterion, probabilistic clustering algorithm,
	probabilistic hierarchical clustering, sequential binary Bayesian
	classification, shot-cluster labeling, statistical models, temporal
	adjacency, video shots, video-segment feature extraction, video-segment
	feature selection, visual similarity},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@INCOLLECTION{Geng2005,
  author = {Geng, Yuliang and Xu, De and Wu, Aimin},
  title = {Effective Video Scene Detection Approach Based on Cinematic Rules},
  year = {2005},
  pages = {1197--1203},
  abstract = {Scene detection is an essential step to organize the video data properly
	for content-based video analysis and its application. In this paper,
	an effective scene detection approach is proposed, which exploits
	the cinematic rules used by filmmakers as guideline to compute shot
	similarities and identify the video scenes in narrative film. First,
	a clustering method with time constraint is used to group the shots
	into scene slices. Then dialog sceneâ€™s alternative structure and
	audio correlation are used to identify dialog scene. Finally, motion
	and audio correlation guided by cinematic rules are used to further
	detect action scene. Experimental results show that the proposed
	method works well and can deal with complex scene.},
  citeulike-article-id = {3253038},
  doi = {http://dx.doi.org/10.1007/11552451\_165},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Effective Video Scene Detection Approach Based on Cinematic Rules.pdf:PDF},
  journal = {Knowledge-Based Intelligent Information and Engineering Systems},
  owner = {danilo},
  posted-at = {2008-09-13 21:16:01},
  priority = {2},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.09.13},
  url = {http://dx.doi.org/10.1007/11552451\_165}
}

@ARTICLE{Goela2007,
  author = {Goela, N. and Wilson, K. and Feng Niu and Divakaran, A. and Otsuka,
	I.},
  title = {An SVM Framework for Genre-Independent Scene Change Detection},
  journal = {Multimedia and Expo, 2007 IEEE International Conference on},
  year = {2007},
  pages = {532-535},
  month = {July},
  abstract = {We present a novel genre-independent SVM framework for detecting scene
	changes in broadcast video. Our framework works on content from a
	diverse range of genres by allowing sets of features, extracted from
	both audio and video streams, to be combined and compared automatically
	without the use of explicit thresholds. For ground truth, we use
	hand-labeled video scene boundaries from a wide variety of broadcast
	genres to generate positive and negative samples for the SVM. Our
	experiments include high-and low-level audio features such as semantic
	histograms and distances between Gaussian models, as well as video
	features such as shot cut positions. We evaluate the importance of
	these measures in a structured framework, with performance comparisons
	obtained via ROC curves. We achieve over 70% detection rate for 10%
	false positive rate on our corpus of over 7.5 hours of data collected
	from news, talk shows, sitcoms, dramas, music videos, and how-to
	shows.},
  doi = {10.1109/ICME.2007.4284704},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\An SVM Framework for Genre-Independent Scene Change Detection.pdf:PDF},
  keywords = {Gaussian processes, audio signal processing, broadcasting, feature
	extraction, object detection, support vector machines, video signal
	processingGaussian models, SVM framework, audio streams, broadcast
	video, feature extraction, genre-independent scene change detection,
	hand-labeled video scene boundaries, semantic histograms, video streams},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@PHDTHESIS{GOULARTE2003,
  author = {R. Goularte},
  title = {Personalização e adaptação de conteúdo baseadas em contexto para
	TV Interativa},
  school = {ICMC-USP},
  year = {2003},
  address = {São Carlos}
}

@ARTICLE{Graber1990,
  author = {Graber, Doris A.},
  title = {Seeing Is Remembering: How Visuals Contribute to Learning from Television
	News},
  journal = {Journal of Communication},
  year = {1990},
  volume = {40},
  pages = {134--156},
  owner = {danilocoimbra},
  timestamp = {2010.12.05}
}

@ARTICLE{Gruber1993,
  author = {Thomas R. Gruber},
  title = {A translation approach to portable ontology specifications},
  journal = {KNOWLEDGE ACQUISITION},
  year = {1993},
  volume = {5},
  pages = {199--220}
}

@ARTICLE{Gu2007,
  author = {Zhiwei Gu and Tao Mei and Xian-Sheng Hua and Xiuqing Wu and Shipeng
	Li},
  title = {EMS: Energy Minimization Based Video Scene Segmentation},
  journal = {Multimedia and Expo, 2007 IEEE International Conference on},
  year = {2007},
  pages = {520-523},
  month = {July},
  abstract = {This paper proposes a novel energy minimization based approach to
	video scene segmentation. In video content analysis, scene is defined
	as a set of adjacent shots related to a particular setting or a continuous
	action in one place. This indicates that not only the global distribution
	of time and content, but also the local temporal continuity should
	be taken into account for scene segmentation. Motivated from this
	fact, we formulate the segmentation procedure as a unified energy
	minimization framework, in which the global and local constraint
	is represented by content and context energy, respectively. This
	energy minimization problem is optimized by two steps in an iterative
	fashion: first find an initial estimation of scene label for content
	energy by a generative model; and then iterated conditional modes
	(ICM) is used for context energy to find the global optimization.
	Furthermore, a boundary voting procedure is devised to decide the
	optimal scene boundaries. We apply EMS on an extensive set of home
	videos and feature movies, and report superior performance compared
	with several existing key approaches to scene segmentation.},
  doi = {10.1109/ICME.2007.4284701},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\EMS Energy Minimization Based Video Scene Segmentation.pdf:PDF},
  keywords = {image segmentation, iterative methods, minimisation, video signal
	processingboundary voting procedure, content energy, context energy,
	feature movies, generative model, global optimization, home videos,
	iterated conditional mode, scene label estimation, unified energy
	minimization, video content analysis, video scene segmentation},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@INPROCEEDINGS{Hampapur1994,
  author = {Hampapur, A. and Weymouth, T. and Jain, R.},
  title = {Digital video segmentation},
  booktitle = {Proceedings of the second ACM international conference on Multimedia},
  year = {1994},
  series = {MULTIMEDIA '94},
  pages = {357--364},
  address = {New York, NY, USA},
  publisher = {ACM},
  acmid = {192699},
  doi = {http://doi.acm.org/10.1145/192593.192699},
  isbn = {0-89791-686-7},
  location = {San Francisco, California, United States},
  numpages = {8},
  url = {http://doi.acm.org/10.1145/192593.192699}
}

@BOOK{Hanjalic2004,
  title = {Content-Based Analysis of Digital Video},
  publisher = {Kluwer Academic Publishers},
  year = {2004},
  editor = {Springer Science},
  author = {Hanjalic, Alan},
  note = {193 pags},
  owner = {danilo},
  timestamp = {2009.01.31}
}

@CONFERENCE{Hanjalic2001b,
  author = {Alan Hanjalic and Geerd Kakes and Reginald L. Lagendijk and Jan Biemond},
  title = {DANCERS: Delft advanced news retrieval system},
  year = {2001},
  editor = {Minerva M. Yeung and Chung-Sheng Li and Rainer W. Lienhart},
  volume = {4315},
  number = {1},
  pages = {301-310},
  publisher = {SPIE},
  doi = {10.1117/12.410940},
  journal = {Storage and Retrieval for Media Databases 2001},
  location = {San Jose, CA, USA},
  url = {http://link.aip.org/link/?PSI/4315/301/1}
}

@ARTICLE{Hanjalic2001,
  author = {Alan Hanjalic and Geerd Kakes and Reginald L. Lagendijk and Jan Biemond},
  title = {Indexing and retrieval of TV broadcast news using DANCERS},
  journal = {Journal of Electronic Imaging},
  year = {2001},
  volume = {10},
  pages = {871-882},
  number = {4},
  doi = {10.1117/1.1406506},
  keywords = {television broadcasting; image retrieval; database indexing; speech
	processing; video databases; video signal processing},
  publisher = {SPIE}
}

@ARTICLE{Harb2006,
  author = {Harb, Hadi and Chen, Liming},
  title = {Audio-based description and structuring of videos},
  journal = {International Journal on Digital Libraries},
  year = {2006},
  volume = {6},
  pages = {70--81},
  number = {1},
  abstract = {Enabling a rapid on-the-fly view of the content of a movie requires
	segmenting the movie and describing the segments in a user-compatible
	manner. The difficulty resides in extracting relevant semantic information
	from the audiovisual signal, both for the segmentation and the description.
	We introduce in this paper audio scenes and chapters in movies and
	present an algorithm for automatically segmenting a video based on
	the audio stream only. Audio scenes and chapters are defined as the
	equivalent of shots and scenes in the visual domain. A tree-like
	audio-based structure of a video is proposed. A chapter is then classified
	into different chapter categories. The automatic solution to audio
	scene and chapter segmentation and classification is evaluated on
	manually segmented and classified videos},
  doi = {http://dx.doi.org/10.1007/s00799-005-0120-5},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Audio-based description and structuring of videos.pdf:PDF},
  owner = {danilo},
  timestamp = {2008.09.13}
}

@CONFERENCE{Hare2006,
  author = {Jonathon S. Hare and Paul H. Lewis and Peter G. B. Enser and Christine
	J. Sandom},
  title = {Mind the gap: another look at the problem of the semantic gap in
	image retrieval},
  year = {2006},
  editor = {Edward Y. Chang and Alan Hanjalic and Nicu Sebe},
  volume = {6073},
  number = {1},
  pages = {607309},
  publisher = {SPIE},
  doi = {10.1117/12.647755},
  eid = {607309},
  journal = {Multimedia Content Analysis, Management, and Retrieval 2006},
  location = {San Jose, CA, USA},
  numpages = {12}
}

@INPROCEEDINGS{Hauptmann1998,
  author = {Hauptmann, A.G. and Witbrock, M.J.},
  title = {Story segmentation and detection of commercials in broadcast news
	video},
  booktitle = {Research and Technology Advances in Digital Libraries, 1998. ADL
	98. Proceedings. IEEE International Forum on},
  year = {1998},
  pages = {168 -179},
  month = apr,
  doi = {10.1109/ADL.1998.670392},
  issn = {1092-9959},
  keywords = {Informedia Digital Library Project;audio;audio processing;broadcast
	news video;closed-captioning cues;commercials;content retrieval;digital
	video library;full content indexing;full-length news broadcasts;image
	processing;information retrieval;news stories;speech recognition
	transcripts;story detection;story segmentation;text;video;video news
	stories;image recognition;indexing;information retrieval;multimedia
	computing;speech recognition;video signal processing;}
}

@INPROCEEDINGS{Hoi2007,
  author = {Hoi, S.C.H. and Lyu, M.R.},
  title = {A Multimodal and Multilevel Ranking Framework for Content-Based Video
	Retrieval},
  booktitle = {Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE
	International Conference on},
  year = {2007},
  volume = {4},
  pages = {IV-1225 -IV-1228},
  month = april,
  issn = {1520-6149},
  keywords = {TRECVID 2005 dataset;content-based video retrieval;harmonic ranking
	functions;multilevel learning scheme;multilevel ranking framework;multimodal
	ranking framework;semi-supervised ranking method;content-based retrieval;graph
	theory;video retrieval;}
}

@BOOK{Houaiss2001,
  title = {Dicionário Houaiss de Língua Portuguesa},
  publisher = {Villar Ms},
  year = {2001},
  editor = {Editora Objetiva},
  author = {Houaiss, A},
  address = {Rio de Janeiro},
  edition = {1a Edição},
  note = {572p.},
  owner = {danilo},
  timestamp = {2009.02.28}
}

@INPROCEEDINGS{HUA2004,
  author = {Xian-Sheng HUA and Lie LU and Hong-Jiang ZHANG},
  title = {Automatic music video generation based on temporal pattern analysis},
  booktitle = {MULTIMEDIA '04: Proceedings of the 12th annual ACM international
	conference on Multimedia},
  year = {2004},
  pages = {472--475},
  address = {New York, NY, USA},
  publisher = {ACM},
  abstract = {Music video (MV) is a short film meant to present a visual representation
	of a popular music song. In this paper, we present a system that
	automatically generates MV-like videos from personal home videos
	based on observations that generally there are obvious repetitive
	visual and aural patterns in MVs. Based on a set of video and music
	analysis algorithms, the automatic music video (AMV) generation system
	automatically extracts temporal structures of the video and music,
	as well as repetitive patterns in the music. And then, according
	to the structure and patterns, a set of highlight segments from the
	raw home video footage are selected, aiming at matching the visual
	content with the aural structure and pattern. And last, the output
	music video is rendered by connecting the selected highlight video
	segments with appropriate transition effects, accompanied with the
	music. Experiments show that the results are compelling and promising.},
  doi = {http://doi.acm.org/10.1145/1027527.1027641},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Automatic music video generation based on temporal pattern analysis.pdf:PDF},
  isbn = {1-58113-893-8},
  location = {New York, NY, USA},
  owner = {danilo},
  timestamp = {2008.12.05}
}

@INPROCEEDINGS{Hua2009,
  author = {Liu Hua-Yong and He Tingting},
  title = {Content-Based Story Segmentation of News Video by Multimodal Analysis},
  booktitle = {Fuzzy Systems and Knowledge Discovery, 2009. FSKD '09. Sixth International
	Conference on},
  year = {2009},
  volume = {7},
  pages = {423 -426},
  month = aug.,
  keywords = {content-based story segmentation;multimodal analysis;news video;silence
	clips detection;topic-caption frame detection;image segmentation;object
	detection;video signal processing;}
}

@ARTICLE{Huang2005,
  author = {Jincheng Huang and Zhu Liu and Yao Wang},
  title = {Joint scene classification and segmentation based on hidden Markov
	model},
  journal = {Multimedia, IEEE Transactions on},
  year = {2005},
  volume = {7},
  pages = { 538-550},
  number = {3},
  month = {June},
  abstract = {Scene classification and segmentation are fundamental steps for efficient
	accessing, retrieving and browsing large amount of video data. We
	have developed a scene classification scheme using a Hidden Markov
	Model (HMM)-based classifier. By utilizing the temporal behaviors
	of different scene classes, HMM classifier can effectively classify
	presegmented clips into one of the predefined scene classes. In this
	paper, we describe three approaches for joint classification and
	segmentation based on HMM, which search for the most likely class
	transition path by using the dynamic programming technique. All these
	approaches utilize audio and visual information simultaneously. The
	first two approaches search optimal scene class transition based
	on the likelihood values computed for short video segment belonging
	to a particular class but with different search constrains. The third
	approach searches the optimal path in a super HMM by concatenating
	HMM's for different scene classes.},
  doi = {10.1109/TMM.2005.843346},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Joint scene classification and segmentation based on hidden Markov model.pdf:PDF},
  issn = {1520-9210},
  keywords = { dynamic programming, hidden Markov models, image classification,
	image segmentation, search problems, video signal processing HMM-based
	classifier, dynamic programming technique, hidden Markov model, joint
	video scene classification, video analysis, video scene segmentation,
	video understanding},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@INPROCEEDINGS{Huang1999,
  author = {Huang, J. and Liu, Z. and Wang, Y. and Chen, Y. and Wong, E.K.},
  title = {Integration of multimodal features for video scene classification
	based on HMM},
  booktitle = {Multimedia Signal Processing, 1999 IEEE 3rd Workshop on},
  year = {1999},
  pages = {53 -58},
  abstract = {Along with the advances in multimedia and Internet technology, a huge
	amount of data, including digital video and audio, are generated
	daily. Tools for the efficient indexing and retrieval of such data
	are indispensable. With multi-modal information present in the data,
	effective integration is necessary and is still a challenging problem.
	In this paper, we present four different methods for integrating
	audio and visual information for video classification based on a
	hidden Markov model (HMM): direct concatenation, product HMM, two-stage
	HMM, and integration by neural network. Our results have shown significant
	improvements over using a single modality},
  doi = {10.1109/MMSP.1999.793797},
  keywords = {Internet;audio-visual information;data retrieval;direct concatenation;hidden
	Markov model;indexing;multi-modal information;multimedia;multimodal
	feature integration;neural network;video scene classification;Internet;audio-visual
	systems;content-based retrieval;database indexing;hidden Markov models;multimedia
	systems;neural nets;video databases;video signal processing;}
}

@ARTICLE{Huang2010,
  author = {Shao Nian Huang and Zhi Yong Zhang},
  title = {Scene Detection in Videos Using Mutual Information},
  journal = {Applied Mechanics and Materials},
  year = {2010},
  volume = {34-35},
  pages = {920--926},
  owner = {danilocoimbra},
  timestamp = {2010.12.07}
}

@INCOLLECTION{Ide1999,
  author = {Ide, Ichiro and Yamamoto, Koji and Tanaka, Hidehiko},
  title = {Automatic Video Indexing Based on Shot Classification},
  booktitle = {Advanced Multimedia Content Processing},
  publisher = {Springer US},
  year = {1999},
  editor = {Nishio, Shojiro and Kishino, Fumio},
  volume = {1554},
  series = {Lecture Notes in Computer Science},
  pages = {87-102},
  affiliation = {The University of Tokyo Graduate School of Electrical Engineering
	7-3-1 Hongo, Bunkyo-ku Tokyo 113-8656 Japan}
}

@MISC{iso2000,
  author = {ISO},
  title = {for Standardization},
  howpublished = {Short MPEG-2 Description},
  year = {2000},
  owner = {danilo},
  timestamp = {2009.02.28}
}

@INPROCEEDINGS{Javed2001,
  author = {Javed, O. and Rasheed, Z. and Shah, M.},
  title = {A framework for segmentation of talk and game shows},
  booktitle = {Proc. Eighth IEEE International Conference on Computer Vision ICCV
	2001},
  year = {2001},
  volume = {2},
  pages = {532--537 vol.2},
  abstract = {In this paper, we present a method to remove commercials from talk
	and game show videos and to segment these videos into host and guest
	shots. In our approach, we mainly rely on information contained in
	shot transitions, rather than analyzing the scene content of individual
	frames. We utilize the inherent differences in scene structure of
	commercials and talk shows to differentiate between them. Similarly,
	we make use of the well-defined structure of talk shows, which can
	be exploited to classify shots as host or guest shots. The entire
	show is first segmented into camera shots based on color histogram.
	Then, we construct a data-structure (shot connectivity graph) which
	links similar shots over time. Analysis of the shot connectivity
	graph helps us to automatically separate commercials from program
	segments. This is done by first detecting stories, and then assigning
	a weight to each story based on its likelihood of being a commercial.
	Further analysis on stories is done to distinguish shots of the hosts
	from shots of the guests. We have tested our approach on several
	full-length shows (including commercials) and have achieved video
	segmentation with high accuracy. The whole scheme is fast and works
	even on low quality video (160&times;120 pixel images at 5 Hz)},
  doi = {10.1109/ICCV.2001.937671},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\A framework for segmentation of talk and game shows.pdf:PDF},
  keywords = {image segmentation, video coding, camera shots, color histogram, data
	structure, program segments, shot connectivity graph, talk and game
	shows segmentation, video segmentation},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@ARTICLE{Jiang2000,
  author = {Hao Jiang and Tong Lin and Hong-Jiang Zhang},
  title = {Video segmentation with the assistance of audio content analysis
	},
  journal = {Multimedia and Expo, 2000. ICME 2000. 2000 IEEE International Conference
	on},
  year = {2000},
  volume = {3},
  pages = {1507-1510 vol.3},
  doi = {10.1109/ICME.2000.871053},
  keywords = {audio signal processing, content-based retrieval, image segmentation,
	multimedia systems, video databasesaudio content analysis, audio
	segmentation, color information, content-based retrieval, correlation
	analysis, music, scene extraction, shot grouping, shot segmentation
	algorithms, speech, video browsing, video segmentation, video structure
	extraction}
}

@INPROCEEDINGS{Jianping2009,
  author = {Wan Jianping and Peng Tianqiang and Li Bicheng},
  title = {News Video Story Segmentation Based on Naive Bayes Model},
  booktitle = {Natural Computation, 2009. ICNC '09. Fifth International Conference
	on},
  year = {2009},
  volume = {6},
  pages = {77 -81},
  month = aug.,
  keywords = {audio type;caption extraction;content based news video retrieval;maximum
	posterior probability;middle-level features;motion extraction;multimodal
	feature fusion;naive Bayes model;news program;news video story segmentation;shot
	detection;story boundary detection;visual features;Bayes methods;content-based
	retrieval;feature extraction;maximum likelihood estimation;probability;video
	retrieval;video signal processing;}
}

@INPROCEEDINGS{jin2004,
  author = {W. Jin and R. Shi and T. S. Chua},
  title = {{A semi-naive bayesian method incorporating clustering with pair-wise
	constraints for auto image annotation}},
  booktitle = {Proceedings of the ACM Multimedia},
  year = {2004}
}

@INPROCEEDINGS{Joshi1998,
  author = {Joshi, A. and Auephanwiriyakul, S. and Krishnapuram, R.},
  title = {On fuzzy clustering and content based access to networked video databases},
  booktitle = {Proc. Eighth International Workshop on Research Issues In Data Engineering
	'Continuous-Media Databases and Applications'},
  year = {1998},
  pages = {42--49},
  abstract = {Video databases and video on demand represent an important application
	of the evolving global information infrastructure. However, video
	querying involves a lot of user interaction and feedback based query
	refinement, which can generate large traffic volumes on the network
	if full video segments are sent. To aid in efficient video browsing,
	search and retrieval across the network, we need to find good compact
	representations for long video sequences. Representative frames (Rframes)
	provide such a representation. Extant algorithms use scene change
	detection to segment video into shots and pick Rframes. However,
	scene change detection techniques fail badly in presence of gradual
	scene changes which are quite prevalent in most videos. We present
	another way of finding Rframes using fuzzy clustering without dealing
	with any scene change detection algorithms. Fuzzy clusters provide
	a more natural approach to this problem since membership of a frame
	in some particular scene is not binary. This allows us to handle
	gradual scene changes. We report on our approach, present preliminary
	experimental results, and discuss ongoing work},
  doi = {10.1109/RIDE.1998.658277},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\On fuzzy clustering and content based access to networked video databases.pdf:PDF},
  keywords = {fuzzy logic, image representation, image segmentation, image sequences,
	information networks, interactive television, multimedia computing,
	query processing, visual databases, content based access, feedback
	based query refinement, fuzzy clustering, global information infrastructure,
	network traffic, networked video databases, representative frames,
	scene change detection, user interaction, video browsing, video on
	demand, video querying, video retrieval, video searching, video segmentation,
	video segments, video sequences},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@ARTICLE{Joyce2006,
  author = {Joyce, R.A. and Bede Liu},
  title = {Temporal segmentation of video using frame and histogram space},
  journal = {Multimedia, IEEE Transactions on},
  year = {2006},
  volume = {8},
  pages = { 130-140},
  number = {1},
  doi = {10.1109/TMM.2005.861285},
  keywords = { error statistics, image segmentation, image sequences, video coding,
	video streaming compressed video stream, dissolve detection algorithm,
	error statistics, frame space, gradual transition detection, histogram
	space, parametric detector, temporal video segmentation, threshold-based
	detector, video sequence, wipe detection}
}

@INPROCEEDINGS{Judge2010,
  author = {Judge, Tejinder K. and Neustaedter, Carman},
  title = {Sharing conversation and sharing life: video conferencing in the
	home},
  booktitle = {Proceedings of the 28th international conference on Human factors
	in computing systems},
  year = {2010},
  series = {CHI '10},
  pages = {655--658},
  address = {New York, NY, USA},
  publisher = {ACM},
  acmid = {1753422},
  doi = {http://doi.acm.org/10.1145/1753326.1753422},
  isbn = {978-1-60558-929-9},
  keywords = {domestic, families, media spaces, video conferencing},
  location = {Atlanta, Georgia, USA},
  numpages = {4}
}

@INPROCEEDINGS{Jun2000,
  author = {Sung-Bae Jun and Kyoungro Yoon and Hee-Youn Lee},
  title = {Dissolve transition detection algorithm using spatio-temporal distribution
	of MPEG macro-block types (poster session)},
  booktitle = {MULTIMEDIA '00: Proceedings of the eighth ACM international conference
	on Multimedia},
  year = {2000},
  pages = {391--394},
  address = {New York, NY, USA},
  publisher = {ACM},
  abstract = {Almost every shot change detection algorithm detects abrupt transition
	(hard cut) without difficulty, but gradual transitions such as fades,
	dissolves, wipes are left as hard-to-detect problems. Dissolve effect,
	among the various gradual transition effects, is one of the most
	frequently used shot transition methods with special semantic meaning
	such as scene transition. Information of the shot change type can
	also be the basis for the shot clustering algorithms. In this paper,
	we present a fast and effective dissolve-transition detection algorithm
	based on the spatio-temporal distribution of the macro block pipes
	in MPEG-compressed video. In the proposed algorithm, the ratio of
	forward macro blocks in the B-type frames and the spatial distribution
	of forward/backward macro blocks are utilized for detecting dissolve
	transition. After finding such sequence of frames, we apply 2 heuristic
	rules: (1) The global color distributions of the frames at which
	dissolve starts and terminates are very different, and (2) The duration
	of dissolve transition is typically more than 0.3 second.},
  doi = {http://doi.acm.org/10.1145/354384.354542},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Dissolve transition detection algorithm using spatio-temporal distribution of MPEG macro-block types (poster session).pdf:PDF},
  isbn = {1-58113-198-4},
  location = {Marina del Rey, California, United States},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@ARTICLE{Kang2001,
  author = {Hang-Bong Kang},
  title = {A hierarchical approach to scene segmentation},
  journal = {cbaivl},
  year = {2001},
  volume = {0},
  pages = {65},
  abstract = {A scene is an important semantic unit to a user who is accessing video
	data and is useful in semantic-based video indexing and retrieval.
	However, scene segmentation from video data is not an easy task.
	We propose a novel hierarchical scene segmentation method which consists
	of three steps: initial segmentation, refinement, and adjustment.
	First, we detect initial scene boundaries using a continuous coherence
	computing model. As a coherence computing model, we use a short-term
	memory-based model in which the memory size and attention span size
	are not fixed. In our model, there are times when the removal of
	shots from the memory buffer may not follow a First-In-First-Out
	rule. After detection of initial scene boundaries, we execute the
	refinement process for each scene using k-means clustering algorithm
	to find misses not detected in scene boundary detection. The cluster-validity
	analysis technique is used to find the optimal number of clusters.
	If a new scene boundary is found within a scene by the analysis of
	clustering results, the scene is split into two sub-scenes. After
	that, we execute the adjustment process. We choose two consecutive
	scenes to detect whether the scene boundary is false or not. For
	shots of two consecutive scenes, we also perform a k-means clustering
	algorithm and detect desirable scene boundaries. Finally, accurate
	scene segmentation results are obtained and experimental results
	are presented},
  address = {Los Alamitos, CA, USA},
  doi = {http://doi.ieeecomputersociety.org/10.1109/IVL.2001.990858},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\A hierarchical approach to scene segmentation.pdf:PDF},
  isbn = {0-7695-1354-9},
  owner = {danilo},
  publisher = {IEEE Computer Society},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@INPROCEEDINGS{Kender1998,
  author = {Kender, J.R. and Yeo, Boon-Lock},
  title = {Video scene segmentation via continuous video coherence},
  booktitle = {Proc. IEEE Computer Society Conference on Computer Vision and Pattern
	Recognition},
  year = {1998},
  pages = {367--373},
  abstract = {In extended video sequences, individual frames are grouped into shots
	which are defined as a sequence taken by a single camera, and related
	shots are grouped into scenes which are defined as a single dramatic
	event taken by a small number of related cameras. This hierarchical
	structure is deliberately constructed, dictated by the limitations
	and preferences of the human visual and memory systems. We present
	three novel high-level segmentation results derived from these considerations,
	some of which are analogous to those involved in the perception of
	the structure of music. First and primarily, we derive and demonstrate
	a method for measuring probable scene boundaries, by calculating
	a short term memory-based model of shot-to-shot &ldquo;coherence&rdquo;.
	The detection of local minima in this continuous measure permits
	robust and flexible segmentation of the video into scenes, without
	the necessity for first aggregating shots into clusters. Second,
	and independently of the first, we then derive and demonstrate a
	one-pass on-the-fly shot clustering algorithm. Third, we demonstrate
	partially successful results on the application of these two new
	methods to the next higher, &ldquo;theme&rdquo;, level of video structure},
  doi = {10.1109/CVPR.1998.698632},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Video scene segmentation via continuous video coherence.pdf:PDF},
  issn = {1063-6919},
  keywords = {image segmentation, image sequences, clustering algorithm, continuous
	measure, continuous video coherence, hierarchical structure, memory
	systems, memory-based model, single dramatic event, video scene segmentation,
	video sequences},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@TECHREPORT{Kitchenham2004,
  author = {Kitchenham, B.},
  title = {Procedures for performing systematic reviews},
  institution = {Keele University and NICTA},
  year = {2004},
  abstract = {The objective of this report is to propose a guideline for systematic
	reviews appropriate for software engineering researchers, including
	PhD students. A systematic review is a means of evaluating and interpreting
	all available research relevant to a particular research question,
	topic area, or phenomenon of interest. Systematic reviews aim to
	present a fair evaluation of a research topic by using a trustworthy,
	rigorous, and auditable methodology. The guideline presented in this
	report was derived from three existing guidelines used by medical
	researchers. The guideline has been adapted to reflect the specific
	problems of software engineering research. The guideline covers three
	phases of a systematic review: planning the review, conducting the
	review and reporting the review. It is at a relatively high level.
	It does not consider the impact of question type on the review procedures,
	nor does it specify in detail mechanisms needed to undertake meta-analysis.},
  citeulike-article-id = {1191541},
  journal = {Technical Report TR/SE-0401},
  keywords = {experimentation, review, systematic},
  posted-at = {2007-03-28 15:07:38},
  priority = {4}
}

@ARTICLE{Kodukula2011,
  author = {Subrahmanyam Kodukula and Mariyam Nazvia},
  title = {Article: Evaluation of Critical Success Factors for Telemedicine
	Implementation},
  journal = {International Journal of Computer Applications},
  year = {2011},
  volume = {12},
  pages = {29--36},
  number = {10},
  month = {January},
  note = {Published by Foundation of Computer Science}
}

@ARTICLE{Kondo2007,
  author = {Kondo, I. and Shimada, S. and Morimoto, M.},
  title = {An Error-Tolerant Video Retrieval Method Based on the Shot Composition
	Sequence in a Scene},
  journal = {Multimedia and Expo, 2007 IEEE International Conference on},
  year = {2007},
  pages = {783-786},
  month = {July},
  abstract = {This paper presents an error-tolerant video retrieval method based
	on the shot composition sequence in a scene. Conventional video players
	in the home can not access interesting scenes directly because they
	offer only play, fast-forward, and rewind. What is needed is an easy-to-use
	video scene player that can directly access scenes important to the
	user. This paper presents an error-tolerant video retrieval method
	that assesses the shot composition sequence in each scene. Our method
	has three characteristics: (1) easy query generation since the regular
	interface is used for query generation; (2) similar video scene retrieval
	based on the sequence of symbols created by shot composition clustering
	to effectively identify scenes similar to the query; (3) fast similarity
	measurement based on sequences of symbols. An experiment shows that
	the proposed system can retrieve video scenes similar to the query
	quickly as well as allowing semantic retrieval to be constructed
	in a non-supervised way.},
  doi = {10.1109/ICME.2007.4284767},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\An Error-Tolerant Video Retrieval Method Based on the Shot Composition Sequence in a Scene.pdf:PDF},
  keywords = {image sequences, video retrievaleasy-to-use video scene player, error-tolerant
	video retrieval method, query generation, semantic retrieval, shot
	composition sequence, video scene retrieval},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@ARTICLE{Koprinska2001,
  author = {Koprinska, I. and Carrato, S.},
  title = {Temporal video segmentation: A survey},
  journal = {Signal Processing: Image Communication},
  year = {2001},
  volume = {16},
  pages = {477-500(24)},
  abstract = {<P>Temporal video segmentation is the first step towards automatic
	annotation of digital video for browsing and retrieval. This article
	gives an overview of existing techniques for video segmentation that
	operate on both uncompressed and compressed video stream. The performance,
	relative merits and limitations of each of the approaches are comprehensively
	discussed and contrasted. The gradual development of the techniques
	and how the uncompressed domain methods were tailored and applied
	into compressed domain are considered. In addition to the algorithms
	for shot boundaries detection, the related topic of camera operation
	recognition is also reviewed.</P>},
  doi = {doi:10.1016/S0923-5965(00)00011-4}
}

@INCOLLECTION{Koskela2009,
  author = {Koskela, Markus and Sjöberg, Mats and Laaksonen, Jorma},
  title = {Improving Automatic Video Retrieval with Semantic Concept Detection},
  booktitle = {Image Analysis},
  publisher = {Springer Berlin / Heidelberg},
  year = {2009},
  editor = {Salberg, Arnt-Børre and Hardeberg, Jon and Jenssen, Robert},
  volume = {5575},
  series = {Lecture Notes in Computer Science},
  pages = {480-489},
  affiliation = {Helsinki University of Technology (TKK) Department of Information
	and Computer Science Espoo Finland}
}

@INPROCEEDINGS{Kwon2000,
  author = {Kwon, Yong-Moo and Song, Chang-Jun and Kim, Ig-Jae},
  title = {A new approach for high level video structuring},
  booktitle = {Proc. IEEE International Conference on Multimedia and Expo ICME 2000},
  year = {2000},
  volume = {2},
  pages = {773--776 vol.2},
  abstract = {The paper introduces a high level video structuring scheme using both
	color and motion features. Until now, color information has been
	used for video scene segmentation research. However, action movies
	with rapid motion of objects have characteristics of dynamic changes
	of color information. In this case, the traditional approach is not
	so appropriate. To overcome this drawback, a new criterion for video
	shot similarity using color and motion features is presented and
	discussed. Then, a new adaptive scene segmentation algorithm is proposed,
	of which the main features include adaptive weighting of color and
	motion similarity between two shots. Moreover, the improved overlapping
	links scheme is proposed for reducing the time cost for shot grouping.
	The proposed scheme is evaluated using the movie: Terminator II,
	together with implementing a prototype video browsing system using
	the structured},
  doi = {10.1109/ICME.2000.871475},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\A new approach for high level video structuring.pdf:PDF},
  keywords = {image colour analysis, image motion analysis, image segmentation,
	multimedia systems, video signal processing, Terminator II, action
	movies, adaptive scene segmentation algorithm, adaptive weighting,
	color information, dynamic changes, high level video structuring,
	motion features, motion similarity, overlapping links scheme, rapid
	motion, shot grouping, structured video, video browsing system, video
	scene segmentation research, video shot similarity},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@ARTICLE{Kyperountas2007,
  author = {Kyperountas, M. and Kotropoulos, C. and Pitas, I.},
  title = {Enhanced Eigen-Audioframes for Audiovisual Scene Change Detection},
  journal = {Multimedia, IEEE Transactions on},
  year = {2007},
  volume = {9},
  pages = {785-797},
  number = {4},
  month = {June },
  abstract = {In this paper, a novel audio-visual scene change detection algorithm
	is presented and evaluated experimentally. An enhanced set of eigen-audioframes
	is created that is related to an audio signal subspace, where audio
	background changes are easily discovered. An analysis is presented
	that justifies why this subspace favors scene change detection. Additionally,
	a novel process is developed in order to detect audio scene change
	candidates in this subspace. Visual information is used to align
	audio scene change indications with neighboring video shot changes
	and, accordingly, to reduce the false alarm rate of the audio-only
	scene change detection. Moreover, video fade effects are identified
	and used independently in order to track scene changes. The false
	alarm rate is reduced further by extracting acoustic features in
	order to verify that the scene change indications are valid. The
	detection methodology was tested on newscast videos provided by the
	TRECVID2003 video test set. The experimental results demonstrate
	that the proposed method achieves an F-measure exceeding 0.85. Accordingly,
	it effectively tackles the scene change detection problem},
  doi = {10.1109/TMM.2007.893337},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Enhanced Eigen-Audioframes for Audiovisual Scene Change Detection.pdf:PDF},
  issn = {1520-9210},
  keywords = {audio signal processing, feature extraction, image segmentation, object
	detection, video signal processingacoustic feature extraction, audio
	signal subspace, audio-visual scene change detection, eigen-audioframe,
	video segmentation},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@ARTICLE{Lan2004,
  author = {Dong-Jun Lan and Yu-Fei Ma and Hong-Jiang Zhang},
  title = {Multi-level Anchorperson Detection Using Multimodal Association},
  journal = {Pattern Recognition, International Conference on},
  year = {2004},
  volume = {3},
  pages = {890-893},
  address = {Los Alamitos, CA, USA},
  doi = {http://doi.ieeecomputersociety.org/10.1109/ICPR.2004.1334671},
  issn = {1051-4651},
  publisher = {IEEE Computer Society}
}

@INCOLLECTION{Lee2005b,
  author = {Lee, Gwang-Gook and Kim, Eui-Jin and Kang, Jung and Kim, Jae-Gon
	and Kim, Whoi-Yul},
  title = {A Method of Generating Table of Contents for Educational Videos},
  year = {2005},
  pages = {129--140},
  abstract = {The increasing amount of multimedia data enforces the development
	of automatic video analysis techniques. In this paper, a method of
	ToC generation is proposed for educational video contents. The proposed
	method consists of two parts: scene segmentation followed by scene
	annotation. First, video sequence is divided into scenes by the proposed
	scene segmentation algorithm which considers the characteristic of
	educational video. Then each shot in the scene is annotated in terms
	of scene type, existence of enclosed caption and main speaker of
	the shot. The experimental result showed that the proposed method
	can generate ToC for educational video with high accuracy.},
  citeulike-article-id = {3252977},
  doi = {http://dx.doi.org/10.1007/11582267\_12},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\A Method of Generating Table of Contents for Educational Videos.pdf:PDF},
  journal = {Advances in Mulitmedia Information Processing - PCM 2005},
  owner = {danilo},
  posted-at = {2008-09-13 20:42:38},
  priority = {2},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.09.13},
  url = {http://dx.doi.org/10.1007/11582267\_12}
}

@ARTICLE{Lee2011,
  author = {Lee, Hansung and Yu, Jaehak and Im, Younghee and Gil, Joon-Min and
	Park, Daihee},
  title = {A unified scheme of shot boundary detection and anchor shot detection
	in news video story parsing},
  journal = {Multimedia Tools and Applications},
  year = {2011},
  volume = {51},
  pages = {1127-1145},
  affiliation = {Electronics and Telecommunications Research Institute, 138, Gajeongno,
	Yueseong-gu, Daejeon, Republic of Korea},
  issn = {1380-7501},
  issue = {3},
  keyword = {Computer Science},
  publisher = {Springer Netherlands}
}

@ARTICLE{Lee2003,
  author = {Lee, Jae-Ho and Lee, Gwang-Gook and Kim, Whoi-Yul},
  title = {Automatic video summarizing tool using MPEG-7 descriptors for personal
	video recorder},
  journal = {IEEE Transactions on Consumer Electronics},
  year = {2003},
  volume = {49},
  pages = {742--749},
  number = {3},
  abstract = {We introduce an automatic video summarizing tool (AVST) for a personal
	video recorder. The tool utilizes MPEG-7 visual descriptors to generate
	a video index for a summary. The resulting index generates not only
	a preview of a movie but also allows non-linear access with thumbnails.
	In addition, the index supports the searching of shots similar to
	a desired one within saved video sequences. Moreover, simple shot-based
	video editing can readily be achieved using the generated index.},
  doi = {10.1109/TCE.2003.1233813},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Automatic video summarizing tool using MPEG-7 descriptors for personal video recorder.pdf:PDF},
  keywords = {image retrieval, image sequences, video recording, video signal processing,
	MPEG-7 visual descriptors, automatic video summarizing tool, movie
	preview, nonlinear access, personal video recorder, saved video sequences,
	shot-based video editing, video index, video retrieval, video summarization},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@ARTICLE{Lee2003b,
  author = {Lee, Raymond S. T.},
  title = {iJADE surveillant--an intelligent multi-resolution composite neuro-oscillatory
	agent-based surveillance system},
  journal = {Pattern Recognition},
  year = {2003},
  volume = {36},
  abstract = {Due to the rapid development of technology, especially in the field
	of Internet systems, there is an increasing demand both for intelligent,
	mobile and autonomous systems and for the usage and conveyance of
	multi-media information through cyberspace. In this paper, we propose
	an innovative, intelligent multi-agent based model, namely intelligent
	Java Agent Development Environment (iJADE), to provide an intelligent
	agent-based platform in the e-commerce environment. In addition to
	the facilities found in contemporary agent development platforms,
	which focus on the autonomy and mobility of multi-agents, iJADE provides
	an intelligent layer (known as the "Conscious Layer") to support
	the implementation of various AI functionalities in order to produce
	"smart" agents.
	
	From an implementation point of view, we introduce an intelligent,
	multi-media processing system known as "iJADE Surveillant"--an intelligent
	multi-resolution composite neuro-oscillatory agent-based surveillance
	system--which is based on the integration of the following modules.
	(a) An automatic coarse-to-fine figure-ground scene segmentation
	module using the Composite Neuro-Oscillatory Wavelet-based model.
	(b) An automatic human face detection and extraction module using
	an Active Contour Model with facial "landmarks" vectors. (c) Invariant
	human face identification based on the Elastic Graph Dynamic Link
	Model. To conform to the current (and future) multi-media system
	standards, all of iJADE Surveillant is implemented using the MPEG-7
	system framework--with comprehensive Description Schemes, feature
	descriptors and a model framework.
	
	From an experimental point of view, a scene gallery of over 6000 color
	scene images is used to test the automatic scene segmentation. One
	hundred distinct human subjects (with over 1020 tested scenes) are
	used to test the intelligent human face identification. An overall
	correct (invariant) facial recognition rate of over 90% is attained.
	We hope that the implementation of the iJADE Surveillant can provide
	an invariant and higher-order intelligent object (pattern) encoding,
	searching and identification solution for future MPEG-7 applications.},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\iJADE surveillantï¿½an intelligent multi-resolution composite neuro-oscillatory agent-based surveillance system.pdf:PDF},
  keywords = {Surveillance system; Composite Neuro-Oscillatory Wavelet-based (CNOW)
	model; Elastic Graph Dynamic Link Model (EGDLM), MPEG-7, Neural networks},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.08.31}
}

@INCOLLECTION{Lee2005a,
  author = {Lee, Wanjoo and Kim, Hyoki and Kang, Hyunchul and Lee, Jinsung and
	Kim, Yongkyu and Jeon, Seokhee},
  title = {Video Cataloging System for Real-Time Scene Change Detection of News
	Video},
  year = {2005},
  pages = {705--715},
  abstract = {It is necessary for various multimedia database applications to develop
	efficient and fast storage, indexing, browsing, and retrieval of
	video. We make a cataloging system, which has important information
	of video data. In this paper, we proposed a method to process huge
	amount of broadcasting data. It can detect scene-change quickly and
	effectively for MPEG stream. To extract the DC-image, minimal decoding
	is employed form MPEG-1 compressed domain. Scene-change is detected
	by the modified histogram comparative method that combines luminance
	and color component. A neural network is used to judge whether it
	is scene-change with an anchorperson and to raise precision of an
	anchorperson scene extraction.},
  citeulike-article-id = {3253048},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Video Cataloging System for Real-Time Scene Change Detection of News Video.pdf:PDF},
  journal = {Combinatorial Image Analysis},
  owner = {danilo},
  posted-at = {2008-09-13 21:21:31},
  priority = {2},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.09.13},
  url = {http://www.springerlink.com/content/v4fcu1q4546fwlcc}
}

@INPROCEEDINGS{Li2001,
  author = {Li, Ying and Ming, Wei and Kuo, C.-C.J.},
  title = {Semantic video content abstraction based on multiple cues},
  booktitle = {Proc. IEEE International Conference on Multimedia and Expo ICME 2001},
  year = {2001},
  pages = {623--626},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Semantic video content abstraction based on multiple cues.pdf:PDF},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@ARTICLE{Li2004,
  author = {Li, Ying and Narayanan, S. and Kuo, C.C.J.},
  title = {Content-based movie analysis and indexing based on audiovisual cues},
  journal = IEEE_J_CASVT,
  year = {2004},
  volume = {14},
  pages = {1073--1085},
  number = {8},
  abstract = {A content-based movie parsing and indexing approach is presented;
	it analyzes both audio and visual sources and accounts for their
	interrelations to extract high-level semantic cues. Specifically,
	the goal of this work is to extract meaningful movie events and assign
	them semantic labels for the purpose of content indexing. Three types
	of key events, namely, 2-speaker dialogs, multiple-speaker dialogs,
	and hybrid events, are considered. Moreover, speakers present in
	the detected movie dialogs are further identified based on the audio
	source parsing. The obtained audio and visual cues are then integrated
	to index the movie content. Our experiments have shown that an effective
	integration of the audio and visual sources can lead to a higher
	level of video content understanding, abstraction and indexing.},
  doi = {10.1109/TCSVT.2004.831968},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Content-based movie analysis and indexing based on audiovisual cues.pdf:PDF},
  keywords = {audio signal processing, image classification, image segmentation,
	speaker recognition, speech processing, video signal processing,
	audio source parsing, audiovisual cues, content indexing, content-based
	movie analysis, content-based movie indexing, content-based movie
	parsing, hybrid events, multiple-speaker dialog, semantic cues, speaker
	identification, video content abstraction, video content indexing,
	video segmentation, Audiovisual integration, content-based video
	indexing, movie event detection, silence detection, speaker identification,
	video content analysis, video segmentation},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@ARTICLE{Li2004a,
  author = {Li, Ying and Smith, John and Zhang, Tong and Chang, Shih-Fu},
  title = {Multimedia database management systems},
  journal = {Journal of Visual Communication and Image Representation},
  year = {2004},
  volume = {15},
  pages = {261 - 264},
  number = {3},
  note = {Multimedia Database Management Systems},
  doi = {DOI: 10.1016/j.jvcir.2004.08.004}
}

@TECHREPORT{Lihp2001,
  author = {Li, Ying and Zhang, Tong and Tretter, Daniel},
  title = {An overview of video abstraction techniques},
  institution = {HP Image Systems Laboratory},
  year = {2001},
  address = {Palo Alto, CA} # pages # {23}
}

@ARTICLE{Li2005,
  author = {Zhenyan Li and Yap-Peng Tan},
  title = {Event detection using multimodal feature analysis},
  journal = {Circuits and Systems, 2005. ISCAS 2005. IEEE International Symposium
	on},
  year = {2005},
  pages = { 3845-3848 Vol. 4},
  month = {May},
  abstract = {This paper presents an event detection framework using multimodal
	feature analysis. In this framework, multimodal features are extracted
	from video data and then analyzed to generate various mid-level concepts,
	such as video shot, face appearance and so on. Two schemes, the logistic
	regression and Bayesian belief network, are then employed to fuse
	the information obtained from multimodal feature analysis and detect
	the video events of interest. We aim to use this framework as a general
	template for event detection in different video domains. Experimental
	results on various test videos in different video domains suggest
	that the proposed event detection framework is promising.},
  doi = {10.1109/ISCAS.2005.1465469},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Event detection using multimodal feature analysis.pdf:PDF},
  keywords = { belief networks, content-based retrieval, feature extraction, multimedia
	databases, regression analysis, video databases Bayesian belief network,
	event detection framework, face appearance, feature extraction, logistic
	regression, mid-level concepts, multimodal feature analysis, video
	data, video event detection, video shot}
}

@INCOLLECTION{Liao2008,
  author = {Liao, Jia and Wang, Guoren and Zhang, Bo and Zhou, Xiaofang and Yu,
	Ge},
  title = {An Adaptive and Efficient Unsupervised Shot Clustering Algorithm
	for Sports Video},
  year = {2008},
  pages = {127--139},
  abstract = {Due to its tremendous commercial potential, sports video has become
	a popular research topic nowadays. As the bridge of low-level features
	and high-level semantic contents, automatic shot clustering is an
	important issue in the field of sports video content analysis. In
	previous work, many clustering approaches need some professional
	knowledge of videos, some experimental parameters, or some thresholds
	to obtain good clustering results. In this article, we present a
	new efficient shot clustering algorithm for sports video which is
	generic and does not need any prior domain knowledge. The novel algorithm,
	which is called Valid Dimension Clustering(VDC), performs in an unsupervised
	manner. For the high-dimensional feature vectors of video shots,
	a new dimensionality reduction approach is proposed first, which
	takes advantage of the available dimension histogram to get ï¿½?valid
	dimensionsï¿½? as a good approximation of the intrinsic characteristics
	of data. Then the clustering algorithm performs on valid dimensions
	one by one to furthest utilize the intrinsic characteristics of each
	valid dimension. The iterations of merging and splitting of similar
	shots on each valid dimension are repeated until the novel stop criterion
	which is designed inheriting the theory of Fisher Discriminant Analysis
	is satisfied. At last, we apply our algorithm on real video data
	in our extensive experiments, the results show that VDC has excellent
	performance and outperforms other clustering algorithms.},
  citeulike-article-id = {3252841},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\An Adaptive and Efficient Unsupervised Shot Clustering Algorithm for Sports Video.pdf:PDF},
  journal = {Advances in Databases: Concepts, Systems and Applications},
  owner = {danilo},
  posted-at = {2008-09-13 20:03:13},
  priority = {2},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.09.13}
}

@INPROCEEDINGS{Lienbart1999,
  author = {Lienbart, R. and Pfeiffer, S. and Effelsberg, W.},
  title = {Scene determination based on video and audio features},
  booktitle = {Proc. IEEE International Conference on Multimedia Computing and Systems},
  year = {1999},
  volume = {1},
  pages = {685--690 vol.1},
  abstract = {Determining automatically what constitutes a scene in a video is a
	challenging task, particularly since there is no precise definition
	of the term &ldquo;scene&rdquo;. It is left to the individual to
	set attributes shared by consecutive shots which group them into
	scenes. Certain basic attributes such as dialogs, like settings and
	continuing sounds are consistent indicators. We have therefore developed
	a scheme for identifying scenes by clustering shots according to
	detected dialogs, like settings and similar audio. Results from experiments
	show automatic identification of these types of scenes to be reliable},
  doi = {10.1109/MMCS.1999.779282},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Scene Determination Based on Video and Audio Features.pdf:PDF},
  keywords = {content-based retrieval, multimedia databases, video databases, audio
	features, automatic scene identification, consecutive shots, content
	based analysis, continuing sounds, dialogs, experiments, multimedia,
	scene determination, shot clustering, video features},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@ARTICLE{Lienhart2000,
  author = {Lienhart, Rainer and Effelsberg, Wolfgang},
  title = {Automatic text segmentation and text recognition for video indexing},
  journal = {Multimedia Systems},
  year = {2000},
  volume = {8},
  pages = {69-81},
  abstract = {Efficient indexing and retrieval of digital video is an important
	function of video databases. One powerful index for retrieval is
	the text appearing in them. It enables content-based browsing. We
	present our new methods for automatic segmentation of text in digital
	videos. The algorithms we propose make use of typical characteristics
	of text in videos in order to enable and enhance segmentation performance.
	The unique features of our approach are the tracking of characters
	and words over their complete duration of occurrence in a video and
	the integration of the multiple bitmaps of a character over time
	into a single bitmap. The output of the text segmentation step is
	then directly passed to a standard OCR software package in order
	to translate the segmented text into ASCII. Also, a straightforward
	indexing and retrieval scheme is introduced. It is used in the experiments
	to demonstrate that the proposed text segmentation algorithms together
	with existing text recognition algorithms are suitable for indexing
	and retrieval of relevant video sequences in and from a video database.
	Our experimental results are very encouraging and suggest that these
	algorithms can be used in video retrieval applications as well as
	to recognize higher level semantics in videos.},
  affiliation = {Intel Corporation, Microprocessor Research Labs, Santa Clara, CA 95052-8119,
	USA; e-mail: Rainer.Lienhart@intel.com US US},
  issn = {0942-4962},
  issue = {1},
  keyword = {Computer Science},
  publisher = {Springer Berlin / Heidelberg}
}

@ARTICLE{Lin2000,
  author = {Tong Lin and Hong-Jiang Zhang},
  title = {Automatic Video Scene Extraction by Shot Grouping},
  journal = {icpr},
  year = {2000},
  volume = {04},
  pages = {4039},
  address = {Los Alamitos, CA, USA},
  doi = {http://doi.ieeecomputersociety.org/10.1109/ICPR.2000.902860},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Automatic video scene extraction by shot grouping.pdf:PDF},
  isbn = {0-7695-0750-6},
  owner = {danilo},
  publisher = {IEEE Computer Society},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@INCOLLECTION{Liu2007,
  author = {Liu, Huayong and He, Tingting and Zhang, Hui},
  title = {NBR: A Content-Based News Video Browsing and Retrieval System},
  booktitle = {Technologies for E-Learning and Digital Entertainment},
  year = {2007},
  pages = {793--800},
  abstract = {An advanced content-based news video browsing and retrieval system,
	NBR, is proposed in this work. The system is built on high-accuracy
	news story segmentation and topic caption text extraction. Its main
	features include category-based news story browsing, key-frame-based
	video abstract and keyword-based news story retrieval. In the paper,
	news story segmentation and topic caption text extraction, as well
	as content-based video browsing and retrieval, are addressed in detail.
	The system is helpful and effective for the overall understanding
	of the news video content.},
  citeulike-article-id = {3253046},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\NBR - A Content-Based News Video Browsing and Retrieval System.pdf:PDF},
  owner = {danilo},
  timestamp = {2008.09.13}
}

@INPROCEEDINGS{Liu2009,
  author = {Wenping Liu and Gang Yang and Xinyuan huang},
  title = {Semantic features based news stories segmentation for news retrieval},
  booktitle = {Wavelet Analysis and Pattern Recognition, 2009. ICWAPR 2009. International
	Conference on},
  year = {2009},
  pages = {258 -265},
  month = july,
  keywords = {audio features;caption-based method;content-based video retrieval;news
	retrieval;news stories segmentation;news video detection;semantic
	features;video clips;video features;content-based retrieval;feature
	extraction;information resources;information retrieval;multimedia
	computing;video retrieval;}
}

@ARTICLE{liu2007b,
  author = {Y. Liu and D. Zhang and G. Lu and W. Y. Ma},
  title = {A survey of content-based image retrieval with high-level semantics},
  journal = {Pattern Recognition},
  year = {2007},
  volume = {40},
  pages = {262--282}
}

@INPROCEEDINGS{Liu1998b,
  author = {Liu, Z. and Huang, J. and Wang, Y.},
  title = {Classification TV programs based on audio information using hidden
	Markov model},
  booktitle = {Multimedia Signal Processing, 1998 IEEE Second Workshop on},
  year = {1998},
  pages = {27 -32},
  month = dec,
  abstract = {This paper describes a technique for classifying TV broadcast video
	using a hidden Markov model (HMM). Here we consider the problem of
	discriminating five types of TV programs, namely commercials, basketball
	games, football games, news reports, and weather forecasts. Eight
	frame-based audio features are used to characterize the low-level
	audio properties, and fourteen clip-based audio features are extracted
	based on these frame-based features to characterize the high-level
	audio properties. For each type of these five TV programs, we build
	an ergodic HMM using the clip-based features as observation vectors.
	The maximum likelihood method is then used for classifying testing
	data using the trained models},
  doi = {10.1109/MMSP.1998.738908},
  keywords = {HMM;TV broadcast video;TV programs classification;audio information;basketball
	games;clip-based audio features;commercials;feature extraction;football
	games;frame-based audio features;hidden Markov model;high-level audio
	properties;low-level audio properties;maximum likelihood method;news
	reports;observation vectors;testing data;trained models;video content
	classifier;video sequences;weather forecasts;audio signal processing;entertainment;feature
	extraction;hidden Markov models;image classification;image sequences;sport;television
	broadcasting;video signal processing;}
}

@ARTICLE{Liu1998a,
  author = {Liu, Zhu and Wang, Yao and Chen, Tsuhan},
  title = {Audio Feature Extraction and Analysis for Scene Segmentation and
	Classification},
  journal = {The Journal of VLSI Signal Processing},
  year = {1998},
  volume = {20},
  pages = {61--79},
  number = {1},
  month = {October},
  abstract = {Understanding of the scene content of a video sequence is very important
	for content-based indexing and retrieval of multimedia databases.
	Research in this area in the past several years has focused on the
	use of speech recognition and image analysis techniques. As a complimentary
	effort to the prior work, we have focused on using the associated
	audio information (mainly the nonspeech portion) for video scene
	analysis. As an example, we consider the problem of discriminating
	five types of TV programs, namely commercials, basketball games,
	football games, news reports, and weather forecasts. A set of low-level
	audio features are proposed for characterizing semantic contents
	of short audio clips. The linear separability of different classes
	under the proposed feature space is examined using a clustering analysis.
	The effective features are identified by evaluating the intracluster
	and intercluster scattering matrices of the feature space. Using
	these features, a neural net classifier was successful in separating
	the above five types of TV programs. By evaluating the changes between
	the feature vectors of adjacent clips, we also can identify scene
	breaks in an audio sequence quite accurately. These results demonstrate
	the capability of the proposed audio features for characterizing
	the semantic content of an audio sequence.},
  citeulike-article-id = {3252833},
  doi = {http://dx.doi.org/10.1023/A:1008066223044},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Audio Feature Extraction and Analysis for Scene Segmentation and Classification.pdf:PDF},
  owner = {danilo},
  posted-at = {2008-09-13 19:46:38},
  priority = {2},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.09.13},
  url = {http://dx.doi.org/10.1023/A:1008066223044}
}

@BOOK{Lowe1999,
  title = {Hypermedia \& the Web},
  publisher = {John Wiley \& Sons Ltd},
  year = {1999},
  author = {Lowe, D. and Hall, W.},
  owner = {danilo},
  timestamp = {2009.02.17}
}

@INPROCEEDINGS{Lu2006b,
  author = {Lu, Lie and Cai, Rui and Hanjalic, A.},
  title = {Audio Elements Based Auditory Scene Segmentation},
  booktitle = {Proc. IEEE International Conference on Acoustics, Speech and Signal
	Processing ICASSP 2006},
  year = {2006},
  volume = {5},
  pages = {17--20},
  abstract = {Auditory scene segmentation is an important step in the process of
	high-level semantic inference from audio data streams, and in particular,
	a prerequisite for auditory scene categorization. In this paper,
	we analyze the limits of previous works on auditory scene segmentation,
	and then propose a novel method that, conceptually, is inspired by
	the ideas used in text and video scene segmentation, and is based
	on an analysis of audio elements and key audio elements, which can
	be seen as equivalents to the words and keywords in a text document,
	respectively. Experiments performed on 1.5 hours of audio data indicate
	that the proposed approach is promising},
  doi = {10.1109/ICASSP.2006.1661201},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Audio Elements Based Auditory Scene Segmentation.pdf:PDF},
  keywords = {audio signal processing, inference mechanisms, audio data streams,
	audio elements, auditory scene categorization, auditory scene segmentation,
	high-level semantic inference, key audio elements},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.15}
}

@ARTICLE{LUM2002,
  author = {W. Y. LUM and F. C. M. LAU},
  title = {{A Context-Aware Decision Engine for Context Adaptation}},
  journal = {IEEE Pervasive Computing},
  year = {2002},
  volume = {1},
  pages = {41--49},
  number = {3}
}

@ARTICLE{Ma2001,
  author = {Yu-fei MA and Xue-sheng BAI and Guang-you XU and Yuan-chun SHI},
  title = {Research on Anchorperson Detection Method in News Video},
  journal = {Journal of Software},
  year = {2001},
  volume = {12(3)},
  pages = {377--382}
}

@ARTICLE{Magalhaes2004,
  author = {Magalhães, J. and Pereira, F.},
  title = {Using MPEG Standards for multimedia customization},
  journal = {Signal Processing: Image Communication},
  year = {2004},
  volume = {19},
  pages = {437--456},
  owner = {danilo},
  timestamp = {2008.12.09}
}

@ARTICLE{Mallat1989,
  author = {Mallat, S.G.},
  title = {A theory for multiresolution signal decomposition: the wavelet representation},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1989},
  volume = {11},
  pages = {674 -693},
  number = {7},
  month = jul,
  abstract = {Multiresolution representations are effective for analyzing the information
	content of images. The properties of the operator which approximates
	a signal at a given resolution were studied. It is shown that the
	difference of information between the approximation of a signal at
	the resolutions 2j+1 and 2j (where j is an integer) can be extracted
	by decomposing this signal on a wavelet orthonormal basis of L2(Rn),
	the vector space of measurable, square-integrable n-dimensional functions.
	In L2(R), a wavelet orthonormal basis is a family of functions which
	is built by dilating and translating a unique function psi;(x). This
	decomposition defines an orthogonal multiresolution representation
	called a wavelet representation. It is computed with a pyramidal
	algorithm based on convolutions with quadrature mirror filters. Wavelet
	representation lies between the spatial and Fourier domains. For
	images, the wavelet representation differentiates several spatial
	orientations. The application of this representation to data compression
	in image coding, texture discrimination and fractal analysis is discussed},
  doi = {10.1109/34.192463},
  issn = {0162-8828},
  keywords = {convolutions;data compression;encoding;fractal analysis;image coding;multiresolution
	signal decomposition;pattern recognition;picture processing;pyramidal
	algorithm;quadrature mirror filters;texture discrimination;wavelet
	representation;data compression;encoding;pattern recognition;picture
	processing;}
}

@ARTICLE{Manzato2010,
  author = {Manzato, Marcelo and Coimbra, Danilo and Goularte, Rudinei},
  title = {An enhanced content selection mechanism for personalization of video
	news programmes},
  journal = {Multimedia Systems},
  year = {2010},
  pages = {1-16},
  abstract = {In this paper, we propose a content selection framework that improves
	the users’ experience when they are enriching or authoring pieces
	of news. This framework combines a variety of techniques to retrieve
	semantically related videos, based on a set of criteria which are
	specified automatically depending on the media’s constraints. The
	combination of different content selection mechanisms can improve
	the quality of the retrieved scenes, because each technique’s limitations
	are minimized by other techniques’ strengths. We present an evaluation
	based on a number of experiments, which show that the retrieved results
	are better when all criteria are used at time.},
  affiliation = {Mathematics and Computing Institute, University of São Paulo, Av.
	Trabalhador Sancarlense, 400, PO Box 668, São Carlos, SP 13560-970,
	Brazil},
  issn = {0942-4962},
  keyword = {Computer Science},
  publisher = {Springer Berlin / Heidelberg}
}

@ARTICLE{Manzato2008,
  author = {Manzato, M. and Goularte, R.},
  title = {Video News Classification for Automatic Content Personalization:
	A Genetic Algorithm Based Approach},
  journal = {In Proceedings of the XIV Webmedia},
  year = {2008},
  owner = {danilo},
  timestamp = {2009.02.19}
}

@CONFERENCE{Manzato2009,
  author = {Manzato, Marcelo Garcia and Coimbra, D. B. and Goularte, R.},
  title = {Multimedia content personalization based on peer-level annotation},
  booktitle = {European Interactive TV Conference},
  year = {2009},
  volume = {1},
  pages = {1--8},
  organization = {7th European Interactive TV Conference},
  owner = {danilo},
  timestamp = {2009.02.26}
}

@ARTICLE{Marchionini2006,
  author = {Marchionini, Gary and Wildemuth, Barbara M. and Geisler, Gary},
  title = {The Open Video Digital Library: A Möbius strip of research and practice},
  journal = {J. Am. Soc. Inf. Sci. Technol.},
  year = {2006},
  volume = {57},
  pages = {1629--1643},
  month = {October},
  address = {New York, NY, USA},
  issue = {12},
  numpages = {15},
  publisher = {John Wiley \& Sons, Inc.}
}

@INCOLLECTION{Meng2008,
  author = {Meng, Xiangzeng and Liu, Lei},
  title = {On Retrieval of Flash Animations Based on Visual Features},
  year = {2008},
  pages = {270--277},
  abstract = {Flash is undergoing an explosive spread as a new prevailing media
	format on the Web. Unfortunately, few research efforts have been
	devoted to content-based Flash retrieval (CBFR) in IR community,
	which goes against the utilization of the enormous Flash resources.
	In this paper, Flash animation in 3-layer architecture is presented
	after it is segmented based on its visual features to a series of
	scenes on time-line of the production process. A promising approach
	of Flash animation retrieval is proposed based on the visual features
	of scenes and some meta-parameters. An experimental prototype system
	of Flash animation retrieval with roughly 100,000 Flash animations
	in total is built. The primary experiment demonstrates the flexibility
	and the effectiveness of our approach of CBFR.},
  citeulike-article-id = {3253034},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\On Retrieval of Flash Animations Based on Visual Features.pdf:PDF},
  journal = {Technologies for E-Learning and Digital Entertainment},
  owner = {danilo},
  posted-at = {2008-09-13 21:13:16},
  priority = {2},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.09.13}
}

@ARTICLE{Mezaris2004,
  author = {Mezaris, V. and Kompatsiaris, I. and Strintzis , M. G.},
  title = {Video Object Segmentation Using Bayes-Based Temporal Tracking and
	Trajectory-Based Region Merging},
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  year = {2004},
  owner = {danilo},
  timestamp = {2008.12.09}
}

@BOOK{Misiti1996,
  title = {Matlab: wavelet toolbox user's guide},
  publisher = {Natick: Math Works},
  year = {1996},
  author = {M. Misiti and Y. Misiti and G. Oppenheim and J. Poggi},
  owner = {danilocoimbra},
  timestamp = {2010.12.22}
}

@INCOLLECTION{Misra2010,
  author = {Misra, Hemant and Hopfgartner, Frank and Goyal, Anuj and Punitha,
	P. and Jose, Joemon},
  title = {TV News Story Segmentation Based on Semantic Coherence and Content
	Similarity},
  booktitle = {Advances in Multimedia Modeling},
  publisher = {Springer Berlin / Heidelberg},
  year = {2010},
  editor = {Boll, Susanne and Tian, Qi and Zhang, Lei and Zhang, Zili and Chen,
	Yi-Ping},
  volume = {5916},
  series = {Lecture Notes in Computer Science},
  pages = {347-357},
  affiliation = {University of Glasgow Dept. of Computing Science Glasgow G12 8QQ UK}
}

@INCOLLECTION{Morisawa2005,
  author = {Morisawa, Keisuke and Nitta, Naoko and Babaguchi, Noboru},
  title = {Video Scene Retrieval with Sign Sequence Matching Based on Audio
	Features},
  year = {2005},
  pages = {121--129},
  abstract = {In this paper, we propose a method of quickly retrieving semantically
	similar scenes to a query video segment from large-scale videos with
	audio features. This method first classifies the sound of the target
	and query videos into voices and background sounds and extracts feature
	vectors by focusing on the sound sources. The feature vectors are
	then clustered by K-means algorithm and the cluster ID, which we
	call sign, is assigned to the feature vectors in the corresponding
	cluster, consequently representing a video segment as a sign sequence.
	Finally, the video scenes are retrieved by sign sequences matching
	using Dynamic Programming. The experimental results show this method
	is potentially useful for scene retrieval.},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Video Scene Retrieval with Sign Sequence Matching Based on Audio Features.pdf:PDF},
  journal = {Advances in Multimedia Information Processing - PCM 2004},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.09.13}
}

@ARTICLE{Nesvadba2004,
  author = {J Nesvadba},
  title = {Low-level cross-media statistical approach for semantic partitioning
	of audio-visual content in a home multimedia environment},
  year = {2004},
  abstract = {In this paper, a new automatic method is proposed for the segmentation
	of audio-visual (A/V) broadcast content. Semantic scene boundaries
	are identified by means of a statistical decision model based on
	the detection of audio silences and visual transitions. The type
	of content segmentation presented here is performed in real-time
	and requires low computational resources thus allowing for implementation
	on home multimedia devices.},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Low-level cross-media statistical approach for semantic partitioning of audio-visual content in a home multimedia environment.pdf:PDF},
  keywords = {Acoustic signal processing ; Implementation ; Segmentation ; Statistical
	decision ; Statistical model ; Multimedia communication ; Partition
	method ; Audio signal processing ; Semantic analysis},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.08.31}
}

@ARTICLE{ng2002,
  author = {A. Y. Ng and M. I. Jordan and Y. Weiss},
  title = {On spectral clustering: analysis and an algorithm},
  journal = {Advances in Neural Information Processing Systems},
  year = {2002},
  volume = {14},
  location = {Cambridge},
  publisher = {MIT Press}
}

@INPROCEEDINGS{Ngo1998,
  author = {Ngo, C. W. and Pong, T. C. and Chin, R. T.},
  title = {A survey of video parsing and image indexing techniques in compressed
	domain},
  booktitle = { Symposium on Image and Speech and Signal Processing and Robotics},
  year = {1998},
  pages = {231--236}
}

@ARTICLE{Ngo2001,
  author = {Ngo, Tchong-wah and Zhang, Hong-jiang and Pong, Tting-chuen},
  title = {Recent advances in content based video analysis},
  journal = {International Journal of Image and Graphics},
  year = {2001},
  volume = {1},
  pages = {1--3}
}

@BOOK{Nievergelt1999,
  title = {Wavelets made easy},
  publisher = {Boston: Bikhäuser},
  year = {1999},
  author = {Y. Nievergelt},
  owner = {danilocoimbra},
  timestamp = {2010.12.22}
}

@INCOLLECTION{Ogawa2008,
  author = {Ogawa, Akira and Takahashi, Tomokazu and Ide, Ichiro and Murase,
	Hiroshi},
  title = {Cross-Lingual Retrieval of Identical News Events by Near-Duplicate
	Video Segment Detection},
  booktitle = {Advances in Multimedia Modeling},
  publisher = {Springer Berlin / Heidelberg},
  year = {2008},
  editor = {Satoh, Shin’ichi and Nack, Frank and Etoh, Minoru},
  volume = {4903},
  series = {Lecture Notes in Computer Science},
  pages = {287-296},
  affiliation = {Graduate School of Information Science, Nagoya University, Furo-cho,
	Chikusa-ku, Nagoya, 464–8601 Japan}
}

@INCOLLECTION{Oh2005,
  author = {Oh, Jung Hwan and Wen, Quan and Hwang, Sae and Lee, Jeongkyu},
  title = {Video Abstraction},
  booktitle = {Video Data Management and Information Retrieval},
  publisher = {Idea Group Publishing},
  year = {2005},
  editor = {Sagarmay Deb},
  owner = {danilo},
  timestamp = {2009.02.01}
}

@ARTICLE{Pai2004,
  author = {Pai, M. and McCulloch, M and Gorman, J. D. and Pai, N. and Enanoria,
	W. and Kennedy, G. and Tharyan, P. and Colford Jr., J. M.},
  title = {Clinical Research Methods - Systematic reviews and meta-analyses:
	An illustrated, step-by-step guide},
  journal = {The National Medical Journal of India},
  year = {2004},
  volume = {17},
  pages = {86-94},
  owner = {danilo},
  timestamp = {2008.12.07}
}

@INCOLLECTION{Pao2008,
  author = {Pao,, H. T. and Chen,, Y. H. and Lai,, P. S. and Xu,, Y. Y. and Fu,,
	Hsin-Chia},
  title = {Constructing and application of multimedia TV-news archives},
  publisher = {Pergamon Press, Inc.},
  year = {2008},
  volume = {35},
  number = {3},
  pages = {1444--1450},
  address = {Tarrytown, NY, USA},
  doi = {http://dx.doi.org/10.1016/j.eswa.2007.08.056},
  journal = {Expert Syst. Appl.}
}

@INCOLLECTION{Parshin2006,
  author = {Parshin, Vyacheslav and Paradzinets, Aliaksandr and Chen, Liming},
  title = {Multimodal Data Fusion for Video Scene Segmentation},
  year = {2006},
  pages = {279--289},
  abstract = {Automatic video segmentation into semantic units is important to organize
	an effective content based access to long video. The basic building
	blocks of professional video are shots. However the semantic meaning
	they provide is of a too low level. In this paper we focus on the
	problem of video segmentation into more meaningful high-level narrative
	units called scenes - aggregates of shots that are temporally continuous,
	share the same physical settings or represent continuous ongoing
	action. A statistical video scene segmentation framework is proposed
	which is capable to combine multiple mid-level features in a symmetrical
	and scalable manner. Two kinds of such features extracted in visual
	and audio domain are suggested. The results of experimental evaluations
	carried out on ground truth video are reported. They show that our
	algorithm effectively fuses multiple modalities with higher performance
	as compared with an alternative conventional fusion technique.},
  citeulike-article-id = {3252990},
  doi = {http://dx.doi.org/10.1007/11590064\_25},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Multimodal Data Fusion for Video Scene Segmentation.pdf:PDF},
  journal = {Visual Information and Information Systems},
  owner = {danilo},
  posted-at = {2008-09-13 20:52:09},
  priority = {2},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.09.13},
  url = {http://dx.doi.org/10.1007/11590064\_25}
}

@BOOK{Pereira2002,
  title = {The MPEG-4 Book},
  publisher = {Prentice Hall PTR},
  year = {2002},
  author = {Pereira,, Fernando C. and Ebrahimi,, Touradj},
  address = {Upper Saddle River, NJ, USA},
  isbn = {0130616214}
}

@ARTICLE{Pfeiffer2001,
  author = {Pfeiffer, Silvia and Lienhart, Rainer and Efflsberg, Wolfgang },
  title = {Scene Determination Based on Video and Audio Features},
  journal = {Multimedia Tools and Applications},
  year = {2001},
  volume = {15},
  pages = {59--81},
  number = {1},
  abstract = {Determining automatically what constitutes a scene in a video is a
	challenging task, particularly since there is no precise definition
	of the term â€œsceneï¿½?. It is left to the individual to set attributes
	shared by consecutive shots which group them into scenes. Certain
	basic attributes such as dialogs, settings and continuing sounds
	are consistent indicators. We have therefore developed a scheme for
	identifying scenes which clusters shots according to detected dialogs,
	settings and similar audio. Results from experiments show automatic
	identification of these types of scenes to be reliable.},
  citeulike-article-id = {3252967},
  doi = {http://dx.doi.org/10.1023/A:1011315803415},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Scene Determination Based on Video and Audio Features.pdf:PDF},
  owner = {danilo},
  posted-at = {2008-09-13 20:36:27},
  priority = {2},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.09.13},
  url = {http://dx.doi.org/10.1023/A:1011315803415}
}

@ARTICLE{Porter2003,
  author = {Porter, S. and Mirmehdi, M. and Thomas, B.},
  title = {Temporal video segmentation and classification of edit effects},
  journal = {Image and Vision Computing},
  year = {2003},
  volume = {21},
  pages = {1097-1106},
  abstract = {<P>The process of shot break detection is a fundamental component
	in automatic video indexing, editing and archiving. This paper introduces
	a novel approach to the detection and classification of shot transitions
	in video sequences including cuts, fades and dissolves. It uses the
	average inter-frame correlation coefficient and block-based motion
	estimation to track image blocks through the video sequence and to
	distinguish changes caused by shot transitions from those caused
	by camera and object motion. We present a number of experiments in
	which we achieve better results compared with two established techniques.</P>},
  doi = {doi:10.1016/j.imavis.2003.08.014}
}

@INPROCEEDINGS{Qian1999,
  author = {Qian, R. and Haering, N. and Sezan, I.},
  title = {A computational approach to semantic event detection},
  booktitle = {Proc. IEEE Computer Society Conference on. Computer Vision and Pattern
	Recognition},
  year = {1999},
  volume = {1},
  pages = {--206 Vol. 1},
  abstract = {We propose a three-level video event detection algorithm and apply
	it to animal hunt detection in wildlife documentaries. The first
	level extracts texture, color and motion features, and detects motion
	blobs. The mid-level employs a neural network to verify whether the
	motion blobs belong to objects of interest. This level also generates
	shot summaries in terms of intermediate-level descriptors which combine
	low-level features from the first level and contain results of mid-level,
	domain specific inferences made on the basis of shot features. The
	shot summaries are then used by a domain-specific inference process
	at the third level to detect the video segments that contain events
	of interest, e.g., hunts. Event based video indexing, summarization
	and browsing are among the applications of the proposed approach},
  doi = {10.1109/CVPR.1999.786939},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\A computational approach to semantic event detection.pdf:PDF},
  keywords = {content-based retrieval, database indexing, inference mechanisms,
	neural nets, animal hunt detection, browsing, computational approach,
	domain specific inferences, motion blobs, motion features, neural
	network, semantic event detection, summarization, three-level video
	event detection algorithm, video indexing, video segments, wildlife
	documentaries},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@ARTICLE{Qian2006,
  author = {Xueming Qian and Guizhong Liu and Rui Su},
  title = {Effective Fades and Flashlight Detection Based on Accumulating Histogram
	Difference},
  journal = {Circuits and Systems for Video Technology, IEEE Transactions on},
  year = {2006},
  volume = {16},
  pages = {1245-1258},
  number = {10},
  month = {Oct. },
  abstract = {Scene change detection is a fundamental step in automatic video indexing,
	browsing and retrieval. Fade in and fade out are two kinds of gradually
	changing scenes which are difficult to be detected in comparison
	with the abruptly changing scenes. The salient character of flashlight
	effect is the luminance change, which is caused by abrupt appearance
	or disappearance of the illumination source. Performance of shot
	boundary detection is not satisfactory for the video sequences containing
	flashlights, if no flashlight discrimination strategy is adopted.
	In this paper, an effective fades and flashlight detection method
	is proposed for both the compressed and uncompressed videos, based
	on the accumulating histogram difference (AHD). This fades detection
	method is proposed in terms of their mathematical models. AHDs of
	all the two consecutive frames during fades transitions can be classified
	into six cases. The flashlight detection method is proposed based
	on the AHD and the energy variation characters. AHD and energy variation
	characters for the starting and ending frames of a flashlight have
	certain regularities, which can also be expressed by cases. Thus
	the fades and flashlight detection problems are converted into cases
	matching ones. Experimental results on several test video sequences
	with different bit rates show the effectiveness of the proposed AHD
	based fades and flashlight detection method},
  doi = {10.1109/TCSVT.2006.881858},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Effective Fades and Flashlight Detection Based on Accumulating Histogram Difference.pdf:PDF},
  issn = {1558-2205},
  keywords = {data compression, image sequences, video coding, video retrievalaccumulating
	histogram difference, automatic video indexing, energy variation
	characters, fades detection method, flashlight detection method,
	flashlight discrimination strategy, illumination source, luminance
	change, mathematical models, scene change detection, shot boundary
	detection, video browsing, video compression, video retrieval, video
	sequences},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@ARTICLE{Quelhas2007,
  author = {Quelhas, P. and Monay, F. and Odobez, J.-M. and Gatica-Perez, D.
	and Tuytelaars, T.},
  title = {A Thousand Words in a Scene},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2007},
  volume = {29},
  pages = {1575-1589},
  number = {9},
  month = {Sept. },
  abstract = {This paper presents a novel approach for visual scene modeling and
	classification, investigating the combined use of text modeling methods
	and local invariant features. Our work attempts to elucidate (1)
	whether a textlike bag-of-visterms (BOV) representation (histogram
	of quantized local visual features) is suitable for scene (rather
	than object) classification, (2) whether some analogies between discrete
	scene representations and text documents exist, and 3) whether unsupervised,
	latent space models can be used both as feature extractors for the
	classification task and to discover patterns of visual co-occurrence.
	Using several data sets, we validate our approach, presenting and
	discussing experiments on each of these issues. We first show, with
	extensive experiments on binary and multiclass scene classification
	tasks using a 9,500-image data set, that the BOV representation consistently
	outperforms classical scene classification approaches. In other data
	sets, we show that our approach competes with or outperforms other
	recent more complex methods. We also show that probabilistic latent
	semantic analysis (PLSA) generates a compact scene representation,
	is discriminative for accurate classification, and is more robust
	than the BOV representation when less labeled training data is available.
	Finally, through aspect-based image ranking experiments, we show
	the ability of PLSA to automatically extract visually meaningful
	scene patterns, making such representation useful for browsing image
	collections.},
  doi = {10.1109/TPAMI.2007.1155},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\A Thousand Words in a Scene.pdf:PDF},
  issn = {0162-8828},
  keywords = {feature extraction, image classification, image representation, image
	segmentation, probability, text analysisfeature extractor, image
	data set, latent space model, local invariant feature, probabilistic
	latent semantic analysis, scene classification, text modeling method,
	textlike bag-of-visterm representation, visual scene model},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@ARTICLE{Quelhas2005,
  author = {Quelhas, P. and Monay, F. and Odobez, J.-M. and Gatica-Perez, D.
	and Tuytelaars, T. and Van Gool, L.},
  title = {Modeling scenes with local descriptors and latent aspects},
  journal = {Computer Vision, 2005. ICCV 2005. Tenth IEEE International Conference
	on},
  year = {2005},
  volume = {1},
  pages = { 883-890 Vol. 1},
  month = {Oct.},
  abstract = {We present a new approach to model visual scenes in image collections,
	based on local invariant features and probabilistic latent space
	models. Our formulation provides answers to three open questions:(l)
	whether the invariant local features are suitable for scene (rather
	than object) classification; (2) whether unsupennsed latent space
	models can be used for feature extraction in the classification task;
	and (3) whether the latent space formulation can discover visual
	co-occurrence patterns, motivating novel approaches for image organization
	and segmentation. Using a 9500-image dataset, our approach is validated
	on each of these issues. First, we show with extensive experiments
	on binary and multi-class scene classification tasks, that a bag-of-visterm
	representation, derived from local invariant descriptors, consistently
	outperforms state-of-the-art approaches. Second, we show that probabilistic
	latent semantic analysis (PLSA) generates a compact scene representation,
	discriminative for accurate classification, and significantly more
	robust when less training data are available. Third, we have exploited
	the ability of PLSA to automatically extract visually meaningful
	aspects, to propose new algorithms for aspect-based image ranking
	and context-sensitive image segmentation.},
  doi = {10.1109/ICCV.2005.152},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Modeling scenes with local descriptors and latent aspects.pdf:PDF},
  issn = {1550-5499 },
  keywords = { feature extraction, image classification, image representation, image
	segmentation, natural scenes, probability aspect-based image ranking,
	bag-of-visterm representation, context-sensitive image segmentation,
	feature extraction, image collection, image organization, local descriptors,
	local invariant feature, multiclass scene classification, probabilistic
	latent semantic analysis, probabilistic latent space model, visual
	cooccurrence pattern, visual scene modelling},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@INPROCEEDINGS{Ren2010,
  author = {Keyan Ren and Qingxuan Jia and Hanxu Sun and Weiyu Zhang},
  title = {Urban scene segmentation by graphical model},
  booktitle = {Computer Engineering and Technology (ICCET), 2010 2nd International
	Conference on},
  year = {2010},
  volume = {4},
  pages = {456--460},
  abstract = {This paper propose a simple and flexible frame work, using graphical
	model to understand diversity of urban scenes with varying viewpoints.
	Our algorithm constructs a CRF network using over segmented superpixel
	regions and learn the appearance model from different set of features
	for specific class of our interest. Also, we introduce a training
	algorithm to learn a model for edge potential among these superpixel
	areas based on their feature difference. The algorithm gives competitive
	and visually pleasing results for urban scene segmentation. We show
	the inference from our trained network improve the class labeling
	performance compared to the result when using the appearance model
	solely.}
}

@INPROCEEDINGS{Ren2007,
  author = {Reede Ren and Punitha Puttu Swamy and Joemon M. Jose and Jana Urban},
  title = {Attention-based video summarisation in rushes collection},
  booktitle = {TVS '07: Proceedings of the international workshop on TRECVID video
	summarization},
  year = {2007},
  pages = {89--93},
  address = {New York, NY, USA},
  publisher = {ACM},
  abstract = {This paper presents the framework of a general video summarisation
	system on the rushes collection, which formalises the summarisation
	process as an 0-1 Knapsack optimisation problem. Three stages are
	included, namely content analysis, content selection and summary
	composition. Content analysis is the pre-processing step, consisting
	of shot segmentation, feature extraction, raw video discrimination
	and shot clustering. Content selection weights the importance of
	video segments by an attention model. A greedy approximation approach
	is employed in the composition of summary video with the cost function,
	which balances the video importance gain and the duration cost. The
	average content coverage achieved on the rushes test collection is
	about 29%, while the average qualification score on readability is
	3.13 with the redundancy credit at 4.08.},
  doi = {http://doi.acm.org/10.1145/1290031.1290048},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Attention-based video summarisation in rushes collection.pdf:PDF},
  isbn = {978-1-59593-780-3},
  location = {Augsburg, Bavaria, Germany},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@BOOK{richard2002,
  title = {H.264 and mpeg-4 video compression},
  year = {2003},
  author = {Richardson, I. E. G.},
  note = {315 pags},
  owner = {danilo},
  timestamp = {2009.02.28}
}

@BOOK{Riffe1998,
  title = {Analyzing media messages: using quantitative content analysis in
	research},
  publisher = {Lawrence Elbau Assoiates, Inc.},
  year = {1998},
  editor = {Mahwah, N. J.},
  author = {Riffe, Daniel and Lacy, Stephen and Fico, Frederick G.},
  pages = {208},
  owner = {danilocoimbra},
  timestamp = {2010.12.05}
}

@INPROCEEDINGS{Rui1998,
  author = {Rui, Yong and Huang, T.S. and Mehrotra, S.},
  title = {Exploring video structure beyond the shots},
  booktitle = {Proc. IEEE International Conference on Multimedia Computing and Systems},
  year = {1998},
  pages = {237--240},
  abstract = {While existing shot-based video analysis approaches provide users
	with better access to the video than the raw data stream does, they
	are still not sufficient for meaningful video browsing and retrieval,
	since: (1) the shots in a long video are still too many to be presented
	to the user; and (2) shots do not capture the underlying semantic
	structure of the video, based on which the user may wish to browse/retrieve
	the video. To explore video structure at the semantic level this
	paper presents an effective approach for video scene structure construction,
	in which shots are grouped into semantic-related scenes. The output
	of the proposed algorithm provides a structured video that greatly
	facilitates user's access. Experiments based on real-world movie
	videos validate the effectiveness of the proposed approach},
  doi = {10.1109/MMCS.1998.693648},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Exploring Video Structure Beyond The Shots.pdf:PDF},
  keywords = {image recognition, image sequences, multimedia computing, video signal
	processing, visual databases, movie videos, semantic level, semantic
	structure, semantic-related scenes, shot-based video analysis, user
	access, video browsing, video retrieval, video scene structure construction,
	video structure},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.15}
}

@INPROCEEDINGS{Salway2007,
  author = {Andrew Salway and Bart Lehane and Noel E. O'Connor},
  title = {Associating characters with events in films},
  booktitle = {CIVR '07: Proceedings of the 6th ACM international conference on
	Image and video retrieval},
  year = {2007},
  pages = {510--517},
  address = {New York, NY, USA},
  publisher = {ACM},
  abstract = {The work presented here combines the analysis of a film's audiovisual
	features with the analysis of an accompanying audio description.
	Specifically, we describe a technique for semantic-based indexing
	of feature films that associates character names with meaningful
	events. The technique fuses the results of event detection based
	on audiovisual features with the inferred on-screen presence of characters,
	based on an analysis of an audio description script. In an evaluation
	with 215 events from 11 films, the technique performed the character
	detection task with Precision = 93% and Recall = 71%. We then go
	on to show how novel access modes to film content are enabled by
	our analysis. The specific examples illustrated include video retrieval
	via a combination of event-type and character name and our first
	steps towards visualization of narrative and character interplay
	based on characters occurrence and co-occurrence in events.},
  location = {Amsterdam, The Netherlands}
}

@INPROCEEDINGS{Santos2010,
  author = {Santos,Jr., Jo\~{a}o Benedito dos and de \'{A}vila, Paulo Muniz and
	Baldini, Rog\'{e}rio and Ishitani, Lucila and Nascimento, Rinaldi
	and dos Santos, Mateus},
  title = {Trends on building interactive applications in the Brazilian digital
	televison system},
  booktitle = {Proceedings of the 7th IEEE conference on Consumer communications
	and networking conference},
  year = {2010},
  series = {CCNC'10},
  pages = {973--977},
  address = {Piscataway, NJ, USA},
  publisher = {IEEE Press},
  acmid = {1834436},
  isbn = {978-1-4244-5175-3},
  keywords = {ISDTV-T middleware, digital television, interactive multimedia, tools},
  location = {Las Vegas, Nevada, USA},
  numpages = {5},
  url = {http://portal.acm.org/citation.cfm?id=1834217.1834436}
}

@INPROCEEDINGS{sethi2001,
  author = {I. K. Sethi and I. L. Coman},
  title = {Mining association rules between low-level image features and high-level
	concepts},
  booktitle = {Proceedings of the SPIE Data Mining and Knowledge Discovery},
  year = {2001},
  volume = {3},
  pages = {279--290}
}

@ARTICLE{Shao2006,
  author = {Xi Shao and Changsheng Xu and Namunu C. Maddage and Qi Tian and Mohan
	S. Kankanhalli and Jesse S. Jin},
  title = {Automatic summarization of music videos},
  journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
  year = {2006},
  volume = {2},
  pages = {127--148},
  number = {2},
  abstract = {In this article, we propose a novel approach for automatic music video
	summarization. The proposed summarization scheme is different from
	the current methods used for video summarization. The music video
	is separated into the music track and video track. For the music
	track, a music summary is created by analyzing the music content
	using music features, an adaptive clustering algorithm, and music
	domain knowledge. Then, shots in the video track are detected and
	clustered. Finally, the music video summary is created by aligning
	the music summary and clustered video shots. Subjective studies by
	experienced users have been conducted to evaluate the quality of
	music summaries and effectiveness of the proposed summarization approach.
	Experiments are performed on different genres of music videos and
	comparisons are made with the summaries generated based on music
	track, video track, and manually. The evaluation results indicate
	that summaries generated using the proposed method are effective
	in helping realize users' expectations.},
  doi = {http://doi.acm.org/10.1145/1142020.1142023},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Automatic summarization of music videos.pdf:PDF},
  owner = {danilo},
  publisher = {ACM},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@ARTICLE{Sikora1997,
  author = {Sikora, T.},
  title = {MPEG digital video-coding standards},
  journal = {Signal Processing Magazine, IEEE},
  year = {1997},
  volume = {14},
  pages = {82-100},
  number = {5},
  month = {Sep},
  abstract = {The efficient digital representation of image and video signals has
	been the subject of considerable research over the past 20 years.
	Digital video-coding technology has developed into a mature field
	and products have been developed that are targeted for a wide range
	of emerging applications, such as video on demand, digital TV/HDTV
	broadcasting, and multimedia image/video database services. With
	the increased commercial interest in video communications, the need
	for international image- and video-compression standards arose. To
	meet this need, the Moving Picture Experts Group (MPEG) was formed
	to develop coding standards. MPEG-1 and MPEG-2 video-coding standards
	have attracted much attention worldwide, with an increasing number
	of very large scale integration (VLSI) and software implementations
	of these standards becoming commercially available. MPEG-4, the most
	recent MPEG standard that is still under development, is targeted
	for future content-based multimedia applications. We provide an overview
	of the MPEG video-coding algorithms and standards and their role
	in video communications. We review the basic concepts and techniques
	that are relevant in the context of the MPEG video-compression standards
	and outline MPEG-1 and MPEG-2 video-coding algorithms. The specific
	properties of the standards related to their applications are presented,
	and the basic elements of the forthcoming MPEG-4 standard are also
	described. We also discuss the performance of the standards and their
	success in the market place},
  doi = {10.1109/79.618010},
  issn = {1053-5888},
  keywords = {code standards, data compression, discrete cosine transforms, image
	representation, telecommunication standards, transform coding, video
	coding, visual communicationDCT, HDTV broadcasting, MPEG digital
	video coding standards, MPEG video coding algorithms, MPEG-1, MPEG-2,
	MPEG-4, Moving Picture Experts Group, VLSI, digital TV broadcasting,
	digital representation, image compression standards, image representation,
	multimedia applications, multimedia image/video database services,
	software implementation, transform domain coding, video communications,
	video compression standards, video on demand, video signal representation}
}

@ARTICLE{Smeaton2007,
  author = {Alan F. Smeaton},
  title = {Techniques used and open challenges to the analysis, indexing and
	retrieval of digital video},
  journal = {Information Systems},
  year = {2007},
  volume = {32},
  pages = {545--559},
  number = {4},
  abstract = {Video in digital format is now commonplace and widespread in both
	professional use, and in domestic consumer products from camcorders
	to mobile phones. Video content is growing in volume and while we
	can capture, compress, store, transmit and display video with great
	facility, editing videos and manipulating them based on their content
	is still a non-trivial activity. In this paper, we give a brief review
	of the state of the art of video analysis, indexing and retrieval
	and we point to research directions which we think are promising
	and could make searching and browsing of video archives based on
	video content, as easy as searching and browsing (text) web pages.
	We conclude the paper with a list of grand challenges for researchers
	working in the area.},
  address = {Oxford, UK, UK},
  doi = {http://dx.doi.org/10.1016/j.is.2006.09.001},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Techniques used and open challenges to the analysis, indexing and retrieval of digital video.pdf:PDF},
  owner = {danilo},
  publisher = {Elsevier Science Ltd.},
  timestamp = {2008.10.09}
}

@ARTICLE{Smeulders2000,
  author = {Smeulders, A.W.M. and Worring, M. and Santini, S. and Gupta, A. and
	Jain, R.},
  title = {Content-based image retrieval at the end of the early years},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2000},
  volume = {22},
  pages = {1349-1380},
  number = {12},
  doi = {10.1109/34.895972},
  keywords = {computer vision, content-based retrieval, image colour analysis, image
	retrieval, image segmentation, image texture, reviews, visual databasesaccumulative
	features, content-based image retrieval, databases, global features,
	local geometry, pictures, salient points, semantic gap, semantics,
	sensory gap, similarity, system architecture, system engineering,
	use patterns}
}

@INPROCEEDINGS{Smith2001,
  author = {J. R. Smith},
  title = {Universal Multimedia Access},
  booktitle = {Proceedings of SPIE},
  year = {2001},
  volume = {4209},
  publisher = {SPIE}
}

@ARTICLE{Snoek2005a,
  author = {Snoek, Cees G.M. and Worring, Marcel},
  title = {Multimodal Video Indexing: A Review of the State-of-the-art},
  journal = {Multimedia Tools and Applications},
  year = {2005},
  volume = {25},
  pages = {5-35},
  abstract = {Efficient and effective handling of video documents depends on the
	availability of indexes. Manual indexing is unfeasible for large
	video collections. In this paper we survey several methods aiming
	at automating this time and resource consuming process. Good reviews
	on single modality based video indexing have appeared in literature.
	Effective indexing, however, requires a multimodal approach in which
	either the most appropriate modality is selected or the different
	modalities are used in collaborative fashion. Therefore, instead
	of separately treating the different information sources involved,
	and their specific algorithms, we focus on the similarities and differences
	between the modalities. To that end we put forward a unifying and
	multimodal framework, which views a video document from the perspective
	of its author. This framework forms the guiding principle for identifying
	index types, for which automatic methods are found in literature.
	It furthermore forms the basis for categorizing these different methods.},
  issn = {1380-7501},
  issue = {1},
  keyword = {Computer Science},
  publisher = {Springer Netherlands}
}

@INPROCEEDINGS{Snoek2005,
  author = {Snoek, Cees G. M. and Worring, Marcel and Smeulders, Arnold W. M.},
  title = {Early versus late fusion in semantic video analysis},
  booktitle = {MULTIMEDIA '05: Proceedings of the 13th annual ACM international
	conference on Multimedia},
  year = {2005},
  pages = {399--402},
  address = {New York, NY, USA},
  publisher = {ACM},
  location = {Hilton, Singapore}
}

@ARTICLE{Filho2007,
  author = {Souza Filho, Guido Lemos de AND Leite, Luiz Eduardo Cunha AND Batista,
	Carlos Eduardo Coelho Freire},
  title = {{Ginga-J: the procedural middleware for the Brazilian digital TV
	system}},
  journal = {{Journal of the Brazilian Computer Society}},
  year = {2007},
  volume = {12},
  pages = {47 - 56},
  month = {03},
  issn = {0104-6500},
  language = {en},
  publisher = {scielo}
}

@BOOK{Srinivasan2005,
  title = {Managing Multimedia Semantics},
  publisher = {IRM Press},
  year = {2005},
  editor = {Srinivasan, Uma and Nepal, Surya},
  author = {Srinivasan, Uma and Nepal, Surya},
  note = {409 pag.},
  owner = {danilo},
  timestamp = {2009.02.28}
}

@MISC{MPEG72004,
  author = {International Organisation for Standardisation},
  title = {{MPEG-7 Overview}},
  year = {2004},
  url = {http://www.chiariglione.org/mpeg/standards/mpeg-7/mpeg-7.htm},
  urlaccessdate = {fevereiro de 2009}
}

@MISC{MPEG42002,
  author = {International Organisation for Standardisation},
  title = {{MPEG-4 Description}},
  year = {2002},
  url = {http://www.chiariglione.org/mpeg/standards/mpeg-4/mpeg-4.htm},
  urlaccessdate = {fevereiro de 2009}
}

@MISC{MPEG22000,
  author = {International Organisation for Standardisation},
  title = {{Short MPEG-2 Description}},
  year = {2000},
  url = {http://www.chiariglione.org/mpeg/standards/mpeg-2/mpeg-2.htm},
  urlaccessdate = {fevereiro de 2009}
}

@MISC{MPEG11996,
  author = {International Organisation for Standardisation},
  title = {{Short MPEG-1 Description}},
  year = {1996},
  url = {http://www.chiariglione.org/mpeg/standards/mpeg-1/mpeg-1.htm},
  urlaccessdate = {fevereiro de 2009}
}

@ARTICLE{Sun2003,
  author = {Sun, S. and Haynor, D. R. and Kim, Y},
  title = {Semiautomatic Video Object Segmentation Using VSnakes},
  journal = {IEEE Transactionson Circuits and Systems for Video Technology},
  year = {2003},
  owner = {danilo},
  timestamp = {2008.12.09}
}

@ARTICLE{Sundaram2002,
  author = {Sundaram, H. and Chang, Shih-Fu},
  title = {Computable scenes and structures in films},
  journal = IEEE_J_MM,
  year = {2002},
  volume = {4},
  pages = {482--491},
  number = {4},
  abstract = {We present a computational scene model and also derive novel algorithms
	for computing audio and visual scenes and within-scene structures
	in films. We use constraints derived from film-making rules and from
	experimental results in the psychology of audition, in our computational
	scene model. Central to the computational model is the notion of
	a causal, finite-memory viewer model. We segment the audio and video
	data separately. In each case, we determine the degree of correlation
	of the most recent data in the memory with the past. The audio and
	video scene boundaries are determined using local maxima and minima,
	respectively. We derive four types of computable scenes that arise
	due to different kinds of audio and video scene boundary synchronizations.
	We show how to exploit the local topology of an image sequence in
	conjunction with statistical tests, to determine dialogs. We also
	derive a simple algorithm to detect silences in audio. An important
	feature of our work is to introduce semantic constraints based on
	structure and silence in our computational model. This results in
	computable scenes that are more consistent with human observations.
	The algorithms were tested on a difficult data set: three commercial
	films. We take the first hour of data from each of the three films.
	The best results: computational scene detection: 94%; dialogue detection:
	91%; and recall 100% precision.},
  doi = {10.1109/TMM.2002.802017},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Computable scenes and structures in films .pdf:PDF},
  issn = {1520-9210},
  keywords = {audio signal processing, edge detection, image segmentation, video
	signal processing, audio scenes, audition, causal finite-memory viewer
	model, commercial films, computable scenes, computational scene model,
	data set, dialogue detection, film-making production rules, film-making
	rules, films, human observations, image sequence, joint audio-visual
	segmentation, local maxima, local minima, local topology, psychology,
	semantic constraints, silence detection, statistical tests, structure
	discovery, visual scenes, within-scene structures},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@INPROCEEDINGS{Sundaram2000,
  author = {Sundaram, H. and Chang, Shih-Fu},
  title = {Video scene segmentation using video and audio features},
  booktitle = {Proc. IEEE International Conference on Multimedia and Expo ICME 2000},
  year = {2000},
  volume = {2},
  pages = {1145--1148 vol.2},
  abstract = {We present a novel algorithm for video scene segmentation. We model
	a scene as a semantically consistent chunk of audio-visual data.
	Central to the segmentation framework is the idea of a finite-memory
	model. We separately segment the audio and video data into scenes,
	using data in the memory. The audio segmentation algorithm determines
	the correlations amongst the envelopes of audio features. The video
	segmentation algorithm determines the correlations amongst shot key-frames.
	The scene boundaries in both cases are determined using local correlation
	minima. Then, we fuse the resulting segments using a nearest neighbor
	algorithm that is further refined using a time-alignment distribution
	derived from the ground truth. The algorithm was tested on a difficult
	data set; the first hour of a commercial film with good results.
	It achieves a scene segmentation accuracy of},
  doi = {10.1109/ICME.2000.871563},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\VIDEO SCENE SEGMENTATION USING VIDEO AND AUDIO FEATURES.pdf:PDF},
  keywords = {audio signal processing, image segmentation, multimedia systems, video
	signal processing, audio data segmentation, audio-visual data, commercial
	film, finite-memory model, local correlation minima, nearest neighbor
	algorithm, shot key-frames, time-alignment distribution, video scene
	segmentation},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@INPROCEEDINGS{Sundaram2000a,
  author = {Hari Sundaram and Shih-Fu Chang},
  title = {Determining computable scenes in films and their structures using
	audio-visual memory models},
  booktitle = {MULTIMEDIA '00: Proceedings of the eighth ACM international conference
	on Multimedia},
  year = {2000},
  pages = {95--104},
  address = {New York, NY, USA},
  publisher = {ACM},
  abstract = {In this paper we present novel algorithms for computing scenes and
	within-scene structures in films. We begin by mapping insights from
	film-making rules and experimental results from the psychology of
	audition into a computational scene model. We define a computable
	scene to be a chunk of audio-visual data that exhibits long-term
	consistency with regard to three properties: (a) chromaticity (b)
	lighting (c) ambient sound. Central to the computational model is
	the notion of a causal, finite-memory viewer model. We segment the
	audio and video data separately. In each case we determine the degree
	of correlation of the most recent data in the memory with the past.
	The respective scene boundaries are determined using local minima
	and aligned using a nearest neighbor algorithm. We introduce a periodic
	analysis transform to automatically determine the structure within
	a scene. We then use statistical tests on the transform to determine
	the presence of a dialogue. The algorithms were tested on a difficult
	data set: five commercial films. We take the first hour of data from
	each of the five films. The best results: scene detection: 88% recall
	and 72% precision, dialogue detection: 91% recall and 100% precision.},
  doi = {http://doi.acm.org/10.1145/354384.354440},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Determining computable scenes in films and their structures using audio-visual memory models.pdf:PDF},
  isbn = {1-58113-198-4},
  location = {Marina del Rey, California, United States},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@INCOLLECTION{Sural2005,
  author = {Sural, S. and M. Mohan and A. K. Majumdar},
  title = {A Soft Decision Histogram from the HSV Color Space for Video Shot
	Detection},
  booktitle = {Video Data Management and Information Retrieval},
  publisher = {Idea Group Publishing},
  year = {2005},
  editor = {Sagarmay Deb},
  owner = {danilo},
  timestamp = {2009.02.01}
}

@ARTICLE{Swain1991,
  author = {Swain, Michael J. and Ballard, Dana H.},
  title = {Color indexing},
  journal = {International Journal of Computer Vision},
  year = {1991},
  volume = {7},
  pages = {11-32},
  note = {10.1007/BF00130487},
  abstract = {Computer vision is embracing a new research focus in which the aim
	is to develop visual skills for robots that allow them to interact
	with a dynamic, realistic environment. To achieve this aim, new kinds
	of vision algorithms need to be developed which run in real time
	and subserve the robot's goals. Two fundamental goals are determining
	the location of a known object. Color can be successfully used for
	both tasks.},
  issn = {0920-5691},
  issue = {1},
  keyword = {Computer Science},
  publisher = {Springer Netherlands},
  url = {http://dx.doi.org/10.1007/BF00130487}
}

@INPROCEEDINGS{Tan2002,
  author = {Yap-Peng Tan and Hong Lu},
  title = {Model-based clustering and analysis of video scenes},
  booktitle = {Image Processing. 2002. Proceedings. 2002 International Conference
	on},
  year = {2002},
  volume = {1},
  pages = {617--620},
  abstract = { We make two contributions. First, we develop an unsupervised method
	to discover clusters of video scenes and summarize them with a concise
	Gaussian mixture model. To search for the best possible model, an
	effective procedure is devised to compare among models with different
	dimensions (i.e., numbers of mixture components) and, for a given
	dimension, among models with different parameters. Second, we propose
	a scene interference measure to characterize the interaction among
	different scenes of a video sequence. When applied to the clustered
	video scenes, the measure can reveal the dominant video segments
	of a class of videos without requiring much domain-specific knowledge.
	The proposed methods have been tested with a large number of sports
	videos and promising results are reported.},
  doi = {10.1109/ICIP.2002.1038099},
  issn = {1522-4880},
  keywords = { concise Gaussian mixture model; scene interaction; scene interference
	measure; scene-level semantics; sports videos; unsupervised method;
	video scenes; video sequence; image classification; image segmentation;
	image sequences; pattern clustering; video signal processing;}
}

@BOOK{tanenbaum2003,
  title = {Computer Networks},
  publisher = {Prentice Hall},
  year = {2003},
  author = {A. S. Tanenbaum},
  address = {Upper Saddle River, NJ},
  edition = {4a}
}

@INPROCEEDINGS{Tang2009,
  author = {Tang, Lin-Xie and Mei, Tao and Hua, Xian-Sheng},
  title = {Near-lossless video summarization},
  booktitle = {Proceedings of the seventeen ACM international conference on Multimedia},
  year = {2009},
  series = {MM '09},
  pages = {351--360},
  address = {New York, NY, USA},
  publisher = {ACM},
  acmid = {1631321},
  isbn = {978-1-60558-608-3},
  keywords = {online video service, video storage, video summarization},
  location = {Beijing, China},
  numpages = {10}
}

@ARTICLE{Tavanapong2004,
  author = {Tavanapong, W. and Junyu Zhou},
  title = {Shot clustering techniques for story browsing},
  journal = {Multimedia, IEEE Transactions on},
  year = {2004},
  volume = {6},
  pages = { 517-527},
  number = {4},
  month = {Aug.},
  abstract = {Automatic video segmentation is the first and necessary step for organizing
	a long video file into several smaller units. The smallest basic
	unit is a shot. Relevant shots are typically grouped into a high-level
	unit called a scene. Each scene is part of a story. Browsing these
	scenes unfolds the entire story of a film, enabling users to locate
	their desired video segments quickly and efficiently. Existing scene
	definitions are rather broad, making it difficult to compare the
	performance of existing techniques and to develop a better one. This
	paper introduces a stricter scene definition for narrative films
	and presents ShotWeave, a novel technique for clustering relevant
	shots into a scene using the stricter definition. The crux of ShotWeave
	is its feature extraction and comparison. Visual features are extracted
	from selected regions of representative frames of shots. These regions
	capture essential information needed to maintain viewers' thought
	in the presence of shot breaks. The new feature comparison is developed
	based on common continuity-editing techniques used in film making.
	Experiments were performed on full-length films with a wide range
	of camera motions and a complex composition of shots. The experimental
	results show that ShotWeave outperforms two recent techniques utilizing
	global visual features in terms of segmentation accuracy and time.},
  doi = {10.1109/TMM.2004.830810},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Shot clustering techniques for story browsing.pdf:PDF},
  issn = {1520-9210},
  keywords = { content-based retrieval, feature extraction, image retrieval, image
	segmentation, pattern clustering, video signal processing ShotWeave,
	automatic video segmentation, camera motions, content-based indexing,
	content-based retrieval, continuity-editing techniques, feature extraction,
	full-length films, scene definition, scene segmentation, shot clustering,
	story browsing, video browsing, video file, video segments},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@INPROCEEDINGS{Tong2005,
  author = {Xiaofeng Tong and Qingshan Liu and Lingyu Duan and Hanqing Lu and
	Changsheng Xu and Qi Tian},
  title = {A unified framework for semantic shot representation of sports video},
  booktitle = {MIR '05: Proceedings of the 7th ACM SIGMM international workshop
	on Multimedia information retrieval},
  year = {2005},
  pages = {127--134},
  address = {New York, NY, USA},
  publisher = {ACM},
  abstract = {The development of mid-level shot description helps to bridge the
	gap between low-level feature and high-level semantics in video indexing
	and analysis. In this paper, we present a unified framework for semantic
	shot representation in field-ball sports genres, in which a video
	shot is characterized via three essential properties, namely, camera
	shot size, subject in a scene and video production technology. The
	three properties clearly represent the primary factors of a shot,
	and provide a unified viewpoint of semantic shot definition. Based
	on this framework, we design an effective architecture for semantic
	shot management comprising three main components as: 1) flexible
	shot clustering and retrieval by adjusting the weights of three properties
	according to different requirements; 2) semantics based video temporal
	segmentation for further event recognition; and 3) comprehensive
	sports video semantics analysis. Extensive experiments on soccer,
	basketball and tennis demonstrate the effectiveness and validity
	of this framework.},
  doi = {http://doi.acm.org/10.1145/1101826.1101848},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\A unified framework for semantic shot representation of sports video.pdf:PDF},
  isbn = {1-59593-244-5},
  location = {Hilton, Singapore},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@ARTICLE{town2001,
  author = {C. P. Town and D. Sinclair},
  title = {Content-based image retrieval using semantic visual categories},
  journal = {Society for Manufacturing Engineers. Technical Report},
  year = {2001}
}

@ARTICLE{Truong2005,
  author = {Ba Tu Truong and Svetha Venkatesh and Chitra Dorai},
  title = {Extraction of Film Takes for Cinematic Analysis},
  journal = {Multimedia Tools Appl.},
  year = {2005},
  volume = {26},
  pages = {277--298},
  number = {3},
  abstract = {In this paper, we focus on the `reverse editing' problem in movie
	analysis, i.e., the extraction of film takes, original camera shots
	that a film editor extracts and arranges to produce a finished scene.
	The ability to disassemble final scenes and shots into takes is essential
	for nonlinear browsing, content annotation and the extraction of
	higher order cinematic constructs from film. A two-part framework
	for take extraction is proposed. The first part focuses on the filtering
	out action-driven scenes for which take extraction is not useful.
	The second part focuses on extracting film takes using agglomerative
	hierarchical clustering methods along with different similarity metrics
	and group distances and demonstrates our findings with 10 movies.},
  address = {Hingham, MA, USA},
  doi = {http://dx.doi.org/10.1007/s11042-005-0892-z},
  issn = {1380-7501},
  owner = {danilo},
  publisher = {Kluwer Academic Publishers},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.11.06}
}

@ARTICLE{vasconcelos2004,
  author = {N. Vasconselos},
  title = {On the efficient evaluation of probabilistic similarity functions
	for image retrieval},
  journal = {IEEE Transactions on Inf. Theory},
  year = {2004},
  volume = {50},
  pages = {1482--1496},
  number = {7}
}

@INCOLLECTION{Velivelli2003,
  author = {Velivelli, Atulya and Ngo, Chong-Wah and Huang, Thomas},
  title = {Detection of Documentary Scene Changes by Audio-Visual Fusion},
  year = {2003},
  pages = {227--238},
  abstract = {The concept of a documentary scene was inferred from the audio-visual
	characteristics of certain documentary videos. It was observed that
	the amount of information from the visual component alone was not
	enough to convey a semantic context to most portions of these videos,
	but a joint observation of the visual component and the audio component
	conveyed a better semantic context. From the observations that we
	made on the video data, we generated an audio score and a visual
	score. We later generated a weighted audio-visual score within an
	interval and adaptively expanded or shrunk this interval until we
	found a local maximum score value. The video ultimately will be divided
	into a set of intervals that correspond to the documentary scenes
	in the video. After we obtained a set of documentary scenes, we made
	a check for any redundant detections.},
  citeulike-article-id = {3253052},
  doi = {http://dx.doi.org/10.1007/3-540-45113-7\_23},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Detection of Documentary Scene Changes by Audio-Visual Fusion.pdf:PDF},
  journal = {Image and Video Retrieval},
  owner = {danilo},
  posted-at = {2008-09-13 21:25:43},
  priority = {2},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.09.13},
  url = {http://dx.doi.org/10.1007/3-540-45113-7\_23}
}

@ARTICLE{Vendrig2003,
  author = {Jeroen Vendrig and Marcel Worring},
  title = {Interactive Adaptive Movie Annotation},
  journal = {IEEE MultiMedia},
  year = {2003},
  volume = {10},
  pages = {30-37},
  number = {3},
  abstract = {Effectively labeling the visual content of movies is essential for
	annotation. We present the interactive and adaptive i-Notation system,
	which describes actors' names, automatically processes multimodal
	information sources, and deals with available sources' varying quality.
	It provides the basis for intelligent interaction and demonstrates
	significant improvements in annotation efficiency.},
  address = {Los Alamitos, CA, USA},
  doi = {http://doi.ieeecomputersociety.org/10.1109/MMUL.2003.1218254},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Interactive Adaptive Movie Annotation.pdf:PDF},
  issn = {1070-986X},
  owner = {danilo},
  publisher = {IEEE Computer Society},
  timestamp = {2008.12.05}
}

@ARTICLE{vetro2005,
  author = {A. Vetro and C. Timmerer},
  title = {{Digital Item Adaptation: Overview of Standardization and Research
	Activities}},
  journal = {IEEE Transactions on Multimedia},
  year = {2005},
  volume = {7},
  pages = {418--426},
  number = {3}
}

@ARTICLE{Wang2003,
  author = {Wang, Jihua and Chua, Tat-Seng },
  title = {A cinematic-based framework for scene boundary detection in video},
  journal = {The Visual Computer},
  year = {2003},
  volume = {19},
  pages = {329--341},
  number = {5},
  abstract = {Most current video retrieval systems use shots as the basis for information
	organization and access. In cinematography, scene is the basic story
	unit that the directors use to compose and convey their ideas. This
	paper proposes a framework based on the concept of continuity to
	analyze video contents and extract scene boundaries. Starting from
	a set of shots, the framework successively applies the concept of
	visual, position, camera focal distance, motion, audio and semantic
	continuity to group shots that exhibit some form of continuity into
	scenes. The framework helps to explain the principles and the heuristics
	behind most cinematic rules. The idea is tested using the first three
	levels of continuity to extract the scenes defined using the most
	common cinematic rules. The method has been found to be effective.},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\A cinematic-based framework for scene boundary detection in video .pdf:PDF},
  owner = {danilo},
  timestamp = {2008.10.09}
}

@INPROCEEDINGS{Wang2002,
  author = {Jihua Wang and Tat-Seng Chua},
  title = {A framework for video scene boundary detection},
  booktitle = {MULTIMEDIA '02: Proceedings of the tenth ACM international conference
	on Multimedia},
  year = {2002},
  pages = {243--246},
  address = {New York, NY, USA},
  publisher = {ACM},
  abstract = {Most current video retrieval systems use shot as the basis for information
	organization and access. In cinematography, scene is the basic story
	unit that the directors use to convey their ideas. This paper proposes
	a framework based on the concept of continuity to analyze video contents
	and extract scene boundaries. Starting from a set of shots, the framework
	successively applies the concept of visual, position, cameral focal
	distance, motion, audio and semantic continuity to group shots that
	exhibit some form of continuity into scenes. The idea is tested using
	the first three levels of continuity to extract the scenes defined
	using most common cinematic rules. The method has been found to be
	effective.},
  doi = {http://doi.acm.org/10.1145/641007.641055},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\A framework for video scene boundary detection.pdf:PDF},
  isbn = {1-58113-620-X},
  location = {Juan-les-Pins, France},
  owner = {danilo},
  timestamp = {2008.10.09}
}

@ARTICLE{Wang2008a,
  author = {Jinqiao Wang and Lingyu Duan and Qingshan Liu and Hanqing Lu and
	Jin, J.S.},
  title = {A Multimodal Scheme for Program Segmentation and Representation in
	Broadcast Video Streams},
  journal = {Multimedia, IEEE Transactions on},
  year = {2008},
  volume = {10},
  pages = {393-408},
  number = {3},
  abstract = {With the advance of digital video recording and playback systems,
	the request for efficiently managing recorded TV video programs is
	evident so that users can readily locate and browse their favorite
	programs. In this paper, we propose a multimodal scheme to segment
	and represent TV video streams. The scheme aims to recover the temporal
	and structural characteristics of TV programs with visual, auditory,
	and textual information. In terms of visual cues, we develop a novel
	concept named program-oriented informative images (POIM) to identify
	the candidate points correlated with the boundaries of individual
	programs. For audio cues, a multiscale Kullback-Leibler (K-L) distance
	is proposed to locate audio scene changes (ASC), and accordingly
	ASC is aligned with video scene changes to represent candidate boundaries
	of programs. In addition, latent semantic analysis (LSA) is adopted
	to calculate the textual content similarity (TCS) between shots to
	model the inter-program similarity and intra-program dissimilarity
	in terms of speech content. Finally, we fuse the multimodal features
	of POIM, ASC, and TCS to detect the boundaries of programs including
	individual commercials (spots). Towards effective program guide and
	attracting content browsing, we propose a multimodal representation
	of individual programs by using POIM images, key frames, and textual
	keywords in a summarization manner. Extensive experiments are carried
	out over an open benchmarking dataset TRECVID 2005 corpus and promising
	results have been achieved. Compared with the electronic program
	guide (EPG), our solution provides a more generic approach to determine
	the exact boundaries of diverse TV programs even including dramatic
	spots.},
  doi = {10.1109/TMM.2008.917362},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\A Multimodal Scheme for Program Segmentation and Representation in Broadcast Video Streams.pdf:PDF},
  keywords = {digital video broadcasting, video streamingTV programs, audio scene
	changes, broadcast video streams, digital video recording, electronic
	program guide, latent semantic analysis, multimodal scheme, multiscale
	Kullback-Leibler distance, playback systems, program segmentation,
	program-oriented informative images, textual content similarity},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@ARTICLE{Wang2007c,
  author = {Wang, Peng and Liu, Zhi-Qiang and Yang, Shi-Qiang },
  title = {Investigation on unsupervised clustering algorithms for video shot
	categorization},
  journal = {Soft Computing - A Fusion of Foundations, Methodologies and Applications},
  year = {2007},
  volume = {11},
  pages = {355--360},
  number = {4},
  month = {February},
  abstract = {Automatic categorization of video shots is the first and necessary
	step for organizing a long video stream into high-level scenes. However,
	existing techniques on video shot categorization still suffer from
	the problem of semantic gap between low-level audio-visual features
	and high-level semantic concepts. To bridge the gap, current researchers
	have been making efforts on the characterizations of: (1) spatio-temporal
	coherence among shots, and (2) bipartite correlation between descriptive
	features and shot categories. In the most recent works, spectral
	clustering methods and information-theoretic co-clustering (ITCC)
	have been actively studied and used to solve the above two issues,
	respectively. In this paper, we investigate the effectiveness of
	the two algorithms on video shot categorization. The comparison is
	examined in terms of estimating number of clusters and classification
	accuracies, where the K-means clustering algorithm is used as the
	benchmark. Experiments on 4-h sports videos show that both algorithms
	perform better than K-means. While the ITCC algorithm has advantages
	in estimating the number of clusters, the spectral clustering is
	better concerning the classification accuracy.},
  citeulike-article-id = {3253062},
  doi = {http://dx.doi.org/10.1007/s00500-006-0089-z},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Investigation on unsupervised clustering algorithms for video shot categorization.pdf:PDF},
  owner = {danilo},
  posted-at = {2008-09-13 21:34:38},
  priority = {2},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.09.13},
  url = {http://dx.doi.org/10.1007/s00500-006-0089-z}
}

@INPROCEEDINGS{Wang2007a,
  author = {Wang, Tao and Gao, Yue and Wang, Patricia P. and Li, Eric and Hu,
	Wei and Zhang, Yimin and Yong, Junhai},
  title = {Video summarization by redundancy removing and content ranking},
  booktitle = {MULTIMEDIA '07: Proceedings of the 15th international conference
	on Multimedia},
  year = {2007},
  pages = {577--580},
  address = {New York, NY, USA},
  publisher = {ACM},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Video summarization by redundancy removing and content ranking.pdf:PDF},
  isbn = {978-1-59593-702-5},
  location = {Augsburg, Germany}
}

@ARTICLE{Wang2007,
  author = {Yong Wang and Jae-Gon Kim and Shih-Fu Chang and Hyung-Myung Kim},
  title = {Utility-Based Video Adaptation for Universal Multimedia Access (UMA)
	and Content-Based Utility Function Prediction for Real-Time Video
	Transcoding},
  journal = {Multimedia, IEEE Transactions on},
  year = {2007},
  volume = {9},
  pages = {213-220},
  number = {2},
  doi = {10.1109/TMM.2006.886253},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Utility-Based Video Adaptation for Universal Multimedia Access (UMA) and Content-Based Utility Function Prediction for Real-Time Video Transcoding.pdf:PDF},
  keywords = {feature extraction, multimedia computing, pattern classification,
	pattern clustering, prediction theory, regression analysis, transcoding,
	video codingMPEG-4 transcoding, content-based statistical paradigm,
	content-based utility function prediction, heterogeneous video resource
	condition, rate-distortion framework, real-time video transcoding,
	regression problem, universal multimedia access, user preference,
	utility-based video adaptation}
}

@ARTICLE{Wang2000,
  author = {Wang, Yao and Liu, Zhu and Huang, Jin-Cheng},
  title = {Multimedia content analysis-using both audio and visual clues},
  journal = IEEE_M_SP,
  year = {2000},
  volume = {17},
  pages = {12--36},
  number = {6},
  abstract = {Multimedia content analysis refers to the computerized understanding
	of the semantic meanings of a multimedia document, such as a video
	sequence with an accompanying audio track. With a multimedia document,
	its semantics are embedded in multiple forms that are usually complimentary
	of each other, Therefore, it is necessary to analyze all types of
	data: image frames, sound tracks, texts that can be extracted from
	image frames, and spoken words that can be deciphered from the audio
	track. This usually involves segmenting the document into semantically
	meaningful units, classifying each unit into a predefined scene type,
	and indexing and summarizing the document for efficient retrieval
	and browsing. We review advances in using audio and visual information
	jointly for accomplishing the above tasks. We describe audio and
	visual features that can effectively characterize scene content,
	present selected algorithms for segmentation and classification,
	and review some testbed systems for video archiving and retrieval.
	We also describe audio and visual descriptors and description schemes
	that are being considered by the MPEG-7 standard for multimedia content
	description},
  doi = {10.1109/79.888862},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Multimedia content analysis-using both audio and visual clues.pdf:PDF},
  keywords = {audio signal processing, content-based retrieval, image classification,
	image retrieval, image segmentation, image sequences, multimedia
	systems, telecommunication standards, video signal processing, MPEG-7
	standard, algorithms, audio clues, audio descriptors, audio track,
	browsing, classification, computerized understanding, document retrieval,
	image frames, multimedia content analysis, multimedia content description,
	multimedia document, scene content, segmentation, semantic meanings,
	sound tracks, spoken words, testbed systems, texts, video archiving,
	video retrieval, video sequence, visual clues, visual descriptors},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@ARTICLE{WEISER1993,
  author = {Weiser,, Mark},
  title = {Some computer science issues in ubiquitous computing},
  journal = {Commun. ACM},
  year = {1993},
  volume = {36},
  pages = {75--84},
  number = {7},
  address = {New York, NY, USA},
  doi = {http://doi.acm.org/10.1145/159544.159617},
  publisher = {ACM}
}

@ARTICLE{WEISER1991,
  author = {M. WEISER},
  title = {{The Computer of the 21st Century}},
  journal = {Scientific American},
  year = {1991},
  volume = {265},
  pages = {94--104},
  number = {3}
}

@ARTICLE{Wen1999,
  author = {Wen, Xiaodong and Huffmire, Theodore D. and Hu, Helen H. and Finkelstein,
	Adam},
  title = {Wavelet-based video indexing and querying},
  journal = {Multimedia Systems},
  year = {1999},
  volume = {7},
  pages = {350-358},
  note = {10.1007/s005300050137},
  abstract = {We present several algorithms suitable for analysis of broadcast video.
	First, we show how wavelet analysis of frames of video can be used
	to detect transitions between shots in a video stream, thereby dividing
	the stream into segments. Next we describe how each segment can be
	inserted into a video database using an indexing scheme that involves
	a wavelet-based signature. Finally, we show that during a subsequent
	broadcast of a similar or identical video clip, the segment can be
	found in the database by quickly searching for the relevant signature.
	The method is robust against noise and typical variations in the
	video stream, even global changes in brightness that can fool histogram-based
	techniques. In the paper, we compare experimentally our shot transition
	mechanism to a color histogram implementation, and also evaluate
	the effectiveness of our database-searching scheme. Our algorithms
	are very efficient and run in realtime on a desktop computer. We
	describe how this technology could be employed to construct a smart
	VCR that was capable of alerting the viewer to the beginning of a
	specific program or identifying},
  affiliation = {Princeton University, Department of Computer Science, Princeton, NJ
	08544, USA US},
  issn = {0942-4962},
  issue = {5},
  keyword = {Computer Science},
  publisher = {Springer Berlin / Heidelberg},
  url = {http://dx.doi.org/10.1007/s005300050137}
}

@ARTICLE{Wu2001,
  author = {Chwan-Hwa Wu and Irwin, J.D. and Dai, F.F.},
  title = {Enabling multimedia applications for factory automation},
  journal = {Industrial Electronics, IEEE Transactions on},
  year = {2001},
  volume = {48},
  pages = {913-919},
  number = {5},
  abstract = {Emerging multimedia communication and processing technologies enable
	many factory applications. However, these technologies are advancing
	at a prodigious pace. Therefore, designing, managing, and upgrading
	these networks within this fast-paced environment represents a formidable
	challenge. The various enabling technologies and the standards that
	control their use are the subject of this paper. Special emphasis
	is given to the use of these technologies to support developments
	on the factory floor. This strategy involves the use of video, audio,
	and data communications, as well as the inherent processing involved.
	The role that local area networks and the Internet play in the development
	of future factory applications is also discussed},
  doi = {10.1109/41.954555},
  keywords = {Internet, data communication, factory automation, image coding, manufacturing
	industries, multimedia communication, video coding, visual communicationInternet,
	JPEG, MPEG, audio communication, data communication, enabling technologies,
	factory automation, fast-paced environment, image coding, inherent
	processing, local area networks, manufacturing automation, multimedia
	applications, multimedia communication, processing technologies,
	video coding, video communication}
}

@INPROCEEDINGS{Xie2007,
  author = {Lexing Xie and Natsev, A. and Tesic, J.},
  title = {Dynamic Multimodal Fusion in Video Search},
  booktitle = {Multimedia and Expo, 2007 IEEE International Conference on},
  year = {2007},
  pages = {1499 -1502},
  month = july,
  keywords = {dynamic multimodal fusion;information-retrieval problem;multimodal
	search;query-dependent fusion strategy;semantic analysis;video search;query
	processing;video retrieval;video signal processing;}
}

@INCOLLECTION{Yacsarouglu2003,
  author = {Yasaroglu, Y. and Alatan, A.},
  title = {Summarizing Video: Content, Features, and HMM Topologies},
  year = {2003},
  pages = {101--110},
  abstract = {An algorithm is proposed for automatic summarization of multimedia
	content by segmenting digital video into semantic scenes using HMMs.
	Various multi-modal low-level features are extracted to determine
	state transitions in HMMs for summarization. Advantage of using different
	model topologies and observation sets in order to segment different
	content types is emphasized and verified by simulations. Performance
	of the proposed algorithm is also compared with a deterministic scene
	segmentation method. A better performance is observed due to the
	flexibility of HMMs in modeling different content types.},
  journal = {Visual Content Processing and Representation}
}

@INPROCEEDINGS{Yi2004,
  author = {Haoran Yi and Deepu Rajan and Liang-Tien Chia},
  title = {A motion based scene tree for browsing and retrieval of compressed
	videos},
  booktitle = {MMDB '04: Proceedings of the 2nd ACM international workshop on Multimedia
	databases},
  year = {2004},
  pages = {10--18},
  address = {New York, NY, USA},
  publisher = {ACM},
  abstract = {This paper describes a fully automatic content-based approach for
	browsing and retrieval of MPEG-2 compressed video. The first step
	of the approach is the detection of shot boundaries based on motion
	vectors available from the compressed video stream. The next step
	involves the construction of a scene tree from the shots obtained
	earlier. The scene tree is shown to capture some semantic information
	as well as to provide a construct for hierarchical browsing of compressed
	videos. Finally, we build a new model for video similarity based
	on global as well as local motion associated with each node in the
	scene tree. To this end, we propose new approaches to camera motion
	and object motion estimation. The experimental results demonstrate
	that the integration of the above techniques results in an efficient
	framework for browsing and searching large video databases.},
  doi = {http://doi.acm.org/10.1145/1032604.1032608},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\A motion based scene tree for browsing and retrieval of compressed videos.pdf:PDF},
  isbn = {1-58113-975-6},
  location = {Washington, DC, USA},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.11.06}
}

@INPROCEEDINGS{Yinzi2010,
  author = {Chen Yinzi and Deng Yang and Guo Yonglei and Wang Wendong and Zou
	Yanming and Wang Kongqiao},
  title = {A Temporal Video Segmentation and Summary Generation Method Based
	on Shots' Abrupt and Gradual Transition Boundary Detecting},
  booktitle = {Communication Software and Networks, 2010. ICCSN '10. Second International
	Conference on},
  year = {2010},
  pages = {271 -275},
  abstract = {A temporal video segmentation method is proposed in this paper. This
	method is based on the detection of shot abrupt transition and gradual
	transition, and then takes into account the conditions of user terminals,
	which could generate different video summarization for each user.
	The experimental results show that the video summarization is able
	to meet the user's browsing requirements, and allows user to enjoy
	better browsing experience.},
  doi = {10.1109/ICCSN.2010.58},
  keywords = {abrupt transition boundary detection;gradual transition boundary detection;summary
	generation method;temporal video segmentation;user terminal condition;video
	summarization;image segmentation;video signal processing;}
}

@INPROCEEDINGS{Yong2003,
  author = {Yong, Cheng and De, Xu},
  title = {A method for scene structure construction with time constraint},
  booktitle = {Proc. International Conference on Neural Networks and Signal Processing},
  year = {2003},
  volume = {2},
  pages = {1213--1216 Vol.2},
  abstract = {For more efficiently organizing, browsing and retrieving video, it
	is important to segment video to extract description structure. Usually
	video is segmented into shots, but since the shots in a long video
	are too many to be presented to the user, and they do not capture
	the underlying semantic structure of the video, high-level structure,
	such as act or scene, needs to be constructed. This paper proposes
	a time-constraint shot clustering method for video scene structure
	construction, in which shots are grouped into clusters based on time-constraint
	visual similarity, and clusters are merged into semantic-related
	scenes. The experimental results show that our method provides a
	better way for video browsing and retrieval.},
  doi = {10.1109/ICNNSP.2003.1281088},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\A method for scene structure construction with time constraint.pdf:PDF},
  keywords = {Petri nets, image classification, image retrieval, video signal processing,
	image classification, semantic structure, time constraint shot clustering,
	time constraint visual similarity, video browsing, video retrieval,
	video scene structure construction, video segmentation},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.15}
}

@ARTICLE{Yoo2008,
  author = {Hun-Woo Yoo},
  title = {Retrieval of movie scenes by semantic matrix and automatic feature
	weight update},
  journal = {Expert Syst. Appl.},
  year = {2008},
  volume = {34},
  pages = {2382--2395},
  number = {4},
  abstract = {A new semantic-based video scene retrieval method is proposed in this
	paper. Twelve low-level features extracted from a video clip are
	represented in a genetic chromosome and target videos that user has
	in mind are retrieved by the interactive genetic algorithm through
	the feedback iteration. In this procedure, high-level semantic relevance
	between retrieved videos is accumulated with so-called semantic relevance
	matrix and semantic frequency matrix for each iteration, and they
	are combined with an automatic feature weight update scheme to retrieve
	more target videos at the next iteration. Experiments over 300 movie
	scene clips extracted from latest well-known movies, showed an user
	satisfaction of 0.71 at the fourth iteration for eight queries such
	as ''gloominess'', ''happiness'', ''quietness'', ''action'', ''conversation'',
	''explosion'', ''war'', and ''car chase''.},
  address = {Tarrytown, NY, USA},
  doi = {http://dx.doi.org/10.1016/j.eswa.2007.04.012},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Retrieval of movie scenes by semantic matrix and automatic feature weight update.pdf:PDF},
  owner = {danilo},
  publisher = {Pergamon Press, Inc.},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.11.06}
}

@INPROCEEDINGS{Yoshitaka1997,
  author = {Yoshitaka, A. and Ishii, T. and Hirakawa, M. and Ichikawa, T.},
  title = {Content-based retrieval of video data by the grammar of film},
  booktitle = {Proc. IEEE Symposium on Visual Languages},
  year = {1997},
  pages = {310--317},
  abstract = {Image/video data contains numerous information such as objects, image
	attributes (e.g., color, height, width) of objects, and various semantic
	contents. Provision of a scheme for the retrieval of video data content
	is still one of the unfulfilled issues. The authors propose a method
	of retrieving video data from films by way of specifying the semantic
	content of scenes. Related to the creation of films, there is a so-called
	&ldquo;the grammar of film&rdquo;, which is an accumulation of knowledge
	and rules for expressing certain semantics of a scene more effectively.
	They concentrate on the features of video that can be observed as
	a consequence of the effects of the grammar of film. Since the features
	relate to shot length, the degree of image dynamics of shots, or
	the combination of similar shots, this method is less time-consuming
	than other ways of evaluating sequences of images in detail},
  doi = {10.1109/VL.1997.626599},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Content-based retrieval of video data by the grammar of film.pdf:PDF},
  keywords = {feature extraction, grammars, image sequences, multimedia computing,
	query processing, visual databases, content-based video data retrieval,
	grammar of film, image attributes, image data, image dynamics, image
	sequences, knowledge, objects, rules, scene, semantic contents, shot
	length},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.15}
}

@INPROCEEDINGS{Yu2007,
  author = {Yu, Hui and Su, Bolan and Lu, Hong and Xue, Xiangyang},
  title = {News video retrieval by learning multimodal semantic information},
  booktitle = {Proceedings of the 9th international conference on Advances in visual
	information systems},
  year = {2007},
  series = {VISUAL'07},
  pages = {403--414},
  address = {Berlin, Heidelberg},
  publisher = {Springer-Verlag},
  acmid = {1783340},
  isbn = {3-540-76413-5, 978-3-540-76413-7},
  keywords = {TRECVID, manual search task, rich semantic information, video retrieval},
  location = {Shanghai, China},
  numpages = {12},
  url = {http://portal.acm.org/citation.cfm?id=1783294.1783340}
}

@INPROCEEDINGS{Yu2009,
  author = {Yu, Xiaoqing and Li, Changlian and Xu, Xueqong and Yang, Shengqi
	and Wan, Wanggen},
  title = {Automatic scene change detection for composed speech and music sound
	under low snr in compressed domain},
  booktitle = {Wireless Mobile and Computing (CCWMC 2009), IET International Communication
	Conference on},
  year = {2009},
  pages = {578--581},
  abstract = {With the amount of MP3 compressed data increasing, automatic scene
	change detection is becoming more and more important. Several studies
	have proposed some interesting approaches. However, none of these
	techniques analyze the audio signals in a low SNR noisy environment
	in compressed domain. In this paper, a new scene-change detection
	algorithm is proposed to detect the composed speech and music scene
	change automatically in a low SNR noisy environment in compressed
	domain. From the Modified Discrete Cosine Transform (MDCT) matrix,
	three robust features are extracted which include the Compressed
	MDCT Average Envelope (CMAE) parameters, the Compressed Possibilities
	Density Ratio (CPDR), and the Compressed Possibilities density Ratio
	local Average (CPRA). A statistical activity detection model in compressed
	domain employs the CMA to segment the audio signal into pure noise
	and noisy audio segments for compressed data sets. The automatic
	scene detection for composed speech and music sound with noise under
	low SNR in compressed domain is implemented. The experimental results
	show that the proposed methods can detect the pure noise, noisy speech
	and noisy music effectively even when SNR is as low as 0dB.}
}

@ARTICLE{Zachary2001,
  author = {John Zachary and S. S. Iyengar and Jacob Barhen},
  title = {Content based image retrieval and information theory: A general approach},
  journal = {Journal of the American Society for Information Science and Technology},
  year = {2001},
  volume = {52},
  pages = {840--852},
  owner = {danilocoimbra},
  timestamp = {2010.12.15}
}

@INCOLLECTION{Zeng2010,
  author = {Zeng, Xianglin and Zhang, Xiaoqin and Hu, Weiming and Li, Wanqing},
  title = {Video Scene Segmentation Using Time Constraint Dominant-Set Clustering},
  booktitle = {Advances in Multimedia Modeling},
  publisher = {Springer Berlin / Heidelberg},
  year = {2010},
  editor = {Boll, Susanne and Tian, Qi and Zhang, Lei and Zhang, Zili and Chen,
	Yi-Ping},
  volume = {5916},
  series = {Lecture Notes in Computer Science},
  pages = {637--643},
  abstract = {Video scene segmentation plays an important role in video structure
	analysis. In this paper, we propose a time constraint dominant-set
	clustering algorithm for shot grouping and scene segmentation, in
	which the similarity between shots is based on autocorrelogram feature,
	motion feature and time constraint. Therefore, the visual evidence
	and time constraint contained in the video content are effectively
	incorporated into a unified clustering framework. Moreover, the number
	of clusters in our algorithm does not need to be predefined and thus
	it provides an automatic framework for scene segmentation. Compared
	with normalized cut clustering based scene segmentation, our algorithm
	can achieve more accurate results and requires less computing resources.},
  affiliation = {Institute of Automation National Laboratory of Pattern Recognition
	Beijing China}
}

@ARTICLE{Zhai2006,
  author = {Yun Zhai and Shah, M.},
  title = {Video scene segmentation using Markov chain Monte Carlo},
  journal = {Multimedia, IEEE Transactions on},
  year = {2006},
  volume = {8},
  pages = { 686-697},
  number = {4},
  abstract = {Videos are composed of many shots that are caused by different camera
	operations, e.g., on/off operations and switching between cameras.
	One important goal in video analysis is to group the shots into temporal
	scenes, such that all the shots in a single scene are related to
	the same subject, which could be a particular physical setting, an
	ongoing action or a theme. In this paper, we present a general framework
	for temporal scene segmentation in various video domains. The proposed
	method is formulated in a statistical fashion and uses the Markov
	chain Monte Carlo (MCMC) technique to determine the boundaries between
	video scenes. In this approach, a set of arbitrary scene boundaries
	are initialized at random locations and are automatically updated
	using two types of updates: diffusion and jumps. Diffusion is the
	process of updating the boundaries between adjacent scenes. Jumps
	consist of two reversible operations: the merging of two scenes and
	the splitting of an existing scene. The posterior probability of
	the target distribution of the number of scenes and their corresponding
	boundary locations is computed based on the model priors and the
	data likelihood. The updates of the model parameters are controlled
	by the hypothesis ratio test in the MCMC process, and the samples
	are collected to generate the final scene boundaries. The major advantage
	of the proposed framework is two-fold: 1) it is able to find the
	weak boundaries as well as the strong boundaries, i.e., it does not
	rely on the fixed threshold; 2) it can be applied to different video
	domains. We have tested the proposed method on two video domains:
	home videos and feature films, and accurate results have been obtained.},
  doi = {10.1109/TMM.2006.876299},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Video scene segmentation using Markov chain Monte Carlo.pdf:PDF},
  keywords = { Markov processes, Monte Carlo methods, image segmentation, video
	signal processing Markov chain Monte Carlo, diffusion process, jumps
	process, posterior probability, target distribution, video analysis,
	video scene segmentation},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@INPROCEEDINGS{Zhai2005,
  author = {Yun Zhai and Shah, M.},
  title = {A general framework for temporal video scene segmentation},
  booktitle = {Computer Vision, 2005. ICCV 2005. Tenth IEEE International Conference
	on},
  year = {2005},
  volume = {2},
  pages = {1111--1116},
  abstract = {Videos are composed of many shots caused by different camera operations,
	e.g., on/off operations and switching between cameras. One important
	goal in video analysis is to group the shots into temporal scenes,
	such that all the shots in a single scene are related to a particular
	physical setting, an on-going action or a theme. In this paper, we
	present a general framework for temporal scene segmentation for various
	video types. The proposed method is formulated in a statistical fashion
	and uses the Markov chain Monte Carlo (MCMC) technique to determine
	the boundaries between video scenes. In this approach, an arbitrary
	number of scene boundaries are randomly initialized and automatically
	updated using two types of updates: diffuse and jumps. The posterior
	probability on the number of scenes and their boundary locations
	is computed based on the model priors and the data likelihood. The
	updates of the model parameters are controlled by the hypothesis
	ratio test in the MCMC process. The proposed framework has been experimented
	on two types of videos, home videos and feature films, and accurate
	results have been obtained},
  doi = {10.1109/ICCV.2005.6},
  issn = {1550-5499},
  keywords = {Markov chain Monte Carlo technique;camera operation;home video;hypothesis
	ratio test;scene boundary;temporal video scene segmentation;video
	analysis;Markov processes;Monte Carlo methods;image segmentation;}
}

@INPROCEEDINGS{Zhang2003,
  author = {Dengsheng Zhang and Guojun Lu},
  title = {Evaluation of similarity measurement for image retrieval},
  booktitle = {Neural Networks and Signal Processing, 2003. Proceedings of the 2003
	International Conference on},
  year = {2003},
  volume = {2},
  pages = { 928 -- 931},
  abstract = {Similarity measurement is one of the key issues in content based image
	retrieval (CBIR). In CBIR, images are represented as features in
	the database. Once the features are extracted from the indexed images,
	the retrieval becomes the measurement of similarity between the features.
	Many similarity measurements exist. A number of commonly used similarity
	measurements are described and evaluated in this paper. They are
	evaluated in a standard shape image database. Results show that city
	block distance and chi;2 Statistics measure outperform other distance
	measure in terms of both retrieval accuracy and retrieval efficiency.},
  doi = {10.1109/ICNNSP.2003.1280752},
  keywords = { city block distance; content based image retrieval; feature extraction;
	image representation; indexed images; similarity measurement; standard
	shape image database; content-based retrieval; feature extraction;
	image representation; image retrieval; statistics; visual databases;}
}

@ARTICLE{Zhang2006b,
  author = {Dongsong Zhang and Lina Zhou and Robert O. Briggs and Jay F. Nunamaker
	and Jr.},
  title = {Instructional video in e-learning: Assessing the impact of interactive
	video on learning effectiveness},
  journal = {Information \& Management},
  year = {2006},
  volume = {43},
  pages = {15 - 27},
  number = {1},
  abstract = {Interactive video in an e-learning system allows proactive and random
	access to video content. Our empirical study examined the influence
	of interactive video on learning outcome and learner satisfaction
	in e-learning environments. Four different settings were studied:
	three were e-learning environments--with interactive video, with
	non-interactive video, and without video. The fourth was the traditional
	classroom environment. Results of the experiment showed that the
	value of video for learning effectiveness was contingent upon the
	provision of interactivity. Students in the e-learning environment
	that provided interactive video achieved significantly better learning
	performance and a higher level of learner satisfaction than those
	in other settings. However, students who used the e-learning environment
	that provided non-interactive video did not improve either. The findings
	suggest that it may be important to integrate interactive instructional
	video into e-learning systems.}
}

@INPROCEEDINGS{Zhang1994,
  author = {HongJiang Zhang and Yihong Gong and Smoliar, S.W. and Shuang Yeo
	Tan},
  title = {Automatic parsing of news video},
  booktitle = {Multimedia Computing and Systems, 1994., Proceedings of the International
	Conference on},
  year = {1994},
  pages = {45 -54},
  month = may,
  doi = {10.1109/MMCS.1994.292432},
  keywords = {automatic parsing;domain knowledge;frame structure models;news video;news
	video data;spatial structure;temporal structure;video content parsing;knowledge
	based systems;multimedia systems;visual databases;}
}

@TECHREPORT{Zhang2002,
  author = {Zhang, H. J.},
  title = {Content-Based Video Analysis, Retrieval and Browsing},
  institution = {Media Computing Group, Microsoft Research Asia},
  year = {2002},
  address = {Beijing},
  owner = {danilocoimbra},
  timestamp = {2010.12.02}
}

@INPROCEEDINGS{Zhang2010,
  author = {Shilin Zhang and Hui Wang},
  title = {Video segmentation based on acoustic analysis},
  booktitle = {Artificial Intelligence and Education (ICAIE), 2010 International
	Conference on},
  year = {2010},
  pages = {1-4},
  abstract = {Video segmentation is a key step for the long videos recorded from
	Television channels to be represented in the hierarchical structure.
	In this paper, a novel approach based on acoustic cues for automatic
	segmenting television stream into individual programs is proposed.
	This presented method is composed of the following steps: Several
	sets of repetitions in the audio track is detected by using silence
	detection and robust audio hashing; The found repetitions are treated
	as advertisements if the range of their length is from 5 seconds
	to 120 seconds; Programs are segmented from the recorded TV streams
	using the detected advertisements. Experiments on real-world TV recordings
	show the effectiveness of the proposed approach.},
  doi = {10.1109/ICAIE.2010.5641407}
}

@INPROCEEDINGS{Zhang2008,
  author = {Zhang, Yu-Jin and Jiang, Fan},
  title = {Home video structuring with a two-layer shot clustering approach},
  booktitle = {Proc. 3rd International Symposium on Communications, Control and
	Signal Processing ISCCSP 2008},
  year = {2008},
  pages = {500--504},
  abstract = {Content-based video structuring above shot level faces technical challenges
	in semantic feature extraction and flexible shot cluster organization.
	Aiming at solving these problems, a two-layer shot clustering approach
	for home video structuring, which operates directly in MPEG domain,
	is presented in this paper. Such an approach goes one-step further
	than conversional one-layer structure to constructs a hierarchical
	content structure to represent more details of video contents as
	well as their interior correlations. With two independent aspects
	of human perception taken into consideration, this structure provides
	fine-grained organization of video shots. Promising results are achieved
	in the experiments made on MPEG-7 test videos.},
  doi = {10.1109/ISCCSP.2008.4537277},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Home video structuring with a two-layer shot clustering approach.pdf:PDF},
  keywords = {pattern clustering, video signal processing, MPEG, content-based video
	structuring, home video structuring, semantic feature extraction,
	two-layer shot clustering, Content-based analysis, Hierarchical content
	structure, Shot cluster, Video organization},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.15}
}

@INPROCEEDINGS{Zhao2001,
  author = {Zhao, Li and Yang, Shi-Qiang and Feng, Bo},
  title = {Video scene detection using slide windows method based on temporal
	constrain shot similarity},
  booktitle = {Proc. IEEE International Conference on Multimedia and Expo ICME 2001},
  year = {2001},
  pages = {1171--1174},
  abstract = {For more efficiently organizing, browsing, and retrieving digital
	video, it is important to extract video structure information at
	both scene and shot levels. This paper presents an effective approach
	to video scene segmentation (shot grouping). In our proposed method,
	we define a new similarity measurement by considering the visual
	similarity among shots together with the temporal relation condition
	and use a slide window model to detect scene boundary. The experimental
	results show that our method produces reasonable detection results.},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Video scene detection using slide windows method based on temporal constrain shot similarity.pdf:PDF},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@INPROCEEDINGS{Zhao2006,
  author = {Ming Zhao and Shi-Yong Neo and Hai-Kiat Goh and Tat-Seng Chua},
  title = {Multi-faceted contextual model for person identification in news
	video},
  booktitle = {Multi-Media Modelling Conference Proceedings, 2006 12th International},
  year = {2006},
  pages = {8 pp.},
  keywords = {TRECVID;audio-visual analysis;multifaceted contextual model;multimedia
	new;multimodal feature;news video;person identification;person temporal
	profile;rank boosting;feature extraction;object detection;video signal
	processing;}
}

@INPROCEEDINGS{Zhao2007,
  author = {Zhao, Yanjun and Wang, Tao and Wang, Peng and Hu, Wei and Du, Yangzhou
	and Zhang, Yimin and Xu, Guangyou},
  title = {Scene Segmentation and Categorization Using NCuts},
  booktitle = {Proc. IEEE Conference on Computer Vision and Pattern Recognition
	CVPR '07},
  year = {2007},
  pages = {1--7},
  abstract = {For video summarization and retrieval, one of the important modules
	is to group temporal-spatial coherent shots into high-level semantic
	video clips namely scene segmentation. In this paper, we propose
	a novel scene segmentation and categorization approach using normalized
	graph cuts(NCuts). Starting from a set of shots, we first calculate
	shot similarity from shot key frames. Then by modeling scene segmentation
	as a graph partition problem where each node is a shot and the weight
	of edge represents the similarity between two shots, we employ NCuts
	to find the optimal scene segmentation and automatically decide the
	optimum scene number by Q function. To discover more useful information
	from scenes, we analyze the temporal layout patterns of shots, and
	automatically categorize scenes into two different types, i.e. parallel
	event scenes and serial event scenes. Extensive experiments are tested
	on movie, and TV series. The promising results demonstrate that the
	proposed NCuts based scene segmentation and categorization methods
	are effective in practice.},
  doi = {10.1109/CVPR.2007.383489},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Scene Segmentation and Categorization Using NCuts.pdf:PDF},
  keywords = {graph theory, image segmentation, spatiotemporal phenomena, video
	retrieval, video signal processing, Ncuts, Q function, normalized
	graph cuts, scene categorization, scene segmentation, temporal layout
	pattern, temporal-spatial coherent shots, video retrieval, video
	summarization},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@INCOLLECTION{Zheng2009,
  author = {Zheng, Fuguang and Li, Shijin and Li, Hao and Feng, Jun},
  title = {Weighted Block Matching-Based Anchor Shot Detection with Dynamic
	Background},
  booktitle = {Image Analysis and Recognition},
  publisher = {Springer Berlin / Heidelberg},
  year = {2009},
  editor = {Kamel, Mohamed and Campilho, Aurélio},
  volume = {5627},
  series = {Lecture Notes in Computer Science},
  pages = {220-228},
  affiliation = {Hohai University School of Computer &amp; Information Engineering
	Nanjing China}
}

@INCOLLECTION{Zhou2002,
  author = {Zhou, Junyu and Tavanapong, Wallapak},
  title = {ShotWeave: A Shot Clustering Technique for Story Browsing for Large
	Video Databases},
  booktitle = {EDBT '02: Proceedings of the Worshops XMLDM, MDDE, and YRWS on XML-Based
	Data Management and Multimedia Engineering-Revised Papers},
  publisher = {Springer-Verlag},
  year = {2002},
  pages = {299--317},
  address = {London, UK},
  isbn = {3-540-00130-1}
}

@INPROCEEDINGS{Zhou2006,
  author = {Zhou, Xianzhong and Zhao, Yaqin and Wang, Jianyu},
  title = {An Efficient Scene Detection Using Rough Set-Based Fuzzy Clustering
	for Film Video},
  booktitle = {Proc. Sixth World Congress on Intelligent Control and Automation
	WCICA 2006},
  year = {2006},
  volume = {2},
  pages = {10435--10439},
  abstract = {It is important to organize the unstructured video data properly for
	content-based video analysis and retrieval. In this paper, an efficient
	film video scene detection method is presented by using a rough set-based
	shot fuzzy clustering algorithm for film. Equivalence relation theory
	is applied to group video shots into clusters. Initial shot clustering
	is performed directly by judging whether equivalence relations are
	equal, not computing the intersection of equivalence classes as usual.
	And excessive generation of some small classes is suppressed by secondary
	clustering on the basis of defining fuzzy similarity between two
	initial clusters. Afterwards, according to the characterization of
	film video scene, three types of temporal relationships between two
	shot clusters are introduced. We introduce a temporally and spatially
	integrated strategy for parsing shot clusters into semantic scenes.
	The scheme offers an efficient mean for browsing and effectively
	retrieving film video.},
  doi = {10.1109/WCICA.2006.1714048},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\An Efficient Scene Detection Using Rough Set-Based Fuzzy Clustering for Film Video.pdf:PDF},
  keywords = {equivalence relation, film video, scene detection, temporal relationships
	between two shot clusters, equivalence relation, film video, scene
	detection, temporal relationships between two shot clusters},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.15}
}

@INCOLLECTION{Zhu2009,
  author = {Zhu, Chengjun and Ouyang, Yuanxin and Gao, Lei and Chen, Zhenyong
	and Xiong, Zhang},
  title = {An Automatic Video Text Detection, Localization and Extraction Approach},
  booktitle = {Advanced Internet Based Systems and Applications},
  publisher = {Springer Berlin / Heidelberg},
  year = {2009},
  editor = {Damiani, Ernesto and Yetongnon, Kokou and Chbeir, Richard and Dipanda,
	Albert},
  volume = {4879},
  series = {Lecture Notes in Computer Science},
  pages = {1-9},
  abstract = {Text in video is a very compact and accurate clue for video indexing
	and summarization. This paper presents an algorithm regarding word
	group as a special symbol to detect, localize and extract video text
	using support vector machine (SVM) automatically. First, four sobel
	operators are applied to get the EM(edge map) of the video frame
	and the EM is segmented into N×2N size blocks. Then character features
	and characters group structure features are extracted to construct
	a 19-dimension feature vector. We use a pre-trained SVM to partition
	each block into two classes: text and non-text blocks. Secondly a
	dilatation-shrink process is employed to adjust the text position.
	Finally text regions are enhanced by multiple frame information.
	After binarization of enhanced text region, the text region with
	clean background is recognized by OCR software. Experimental results
	show that the proposed method can detect, localize, and extract video
	texts with high accuracy.},
  affiliation = {Beihang University School of Computer Science and Technology No. 37
	Xue Yuan Road, Haidian District Beijing P.R.China}
}

@ARTICLE{Zhu2009a,
  author = {Zhu, Songhao and Liu, Yuncai},
  title = {Video scene segmentation and semantic representation using a novel
	scheme},
  journal = {Multimedia Tools and Applications},
  year = {2009},
  volume = {42},
  pages = {183-205},
  note = {10.1007/s11042-008-0233-0},
  abstract = {Grouping video content into semantic segments and classifying semantic
	scenes into different types are the crucial processes to content-based
	video organization, management and retrieval. In this paper, a novel
	approach to automatically segment scenes and semantically represent
	scenes is proposed. Firstly, video shots are detected using a rough-to-fine
	algorithm. Secondly, key-frames within each shot are selected adaptively
	with hybrid features, and redundant key-frames are removed by template
	matching. Thirdly, spatio-temporal coherent shots are clustered into
	the same scene based on the temporal constraint of video content
	and visual similarity between shot activities. Finally, under the
	full analysis of typical characters on continuously recorded videos,
	scene content is semantically represented to satisfy human demand
	on video retrieval. The proposed algorithm has been performed on
	various genres of films and TV program. Promising experimental results
	show that the proposed method makes sense to efficient retrieval
	of interesting video content.},
  affiliation = {Shanghai Jiao tong University Institute of Image Processing and Pattern
	Recognition, School of Electronics and Electric Engineering 800,
	Don chuan Road Shanghai 200240 China},
  issn = {1380-7501},
  issue = {2},
  keyword = {Computer Science},
  publisher = {Springer Netherlands},
  url = {http://dx.doi.org/10.1007/s11042-008-0233-0}
}

@INPROCEEDINGS{Zhu2008,
  author = {Zhu, Songhao and Liu, Yuncai},
  title = {A novel scheme for video scenes segmentation and semantic representation},
  booktitle = {Proc. IEEE International Conference on Multimedia and Expo},
  year = {2008},
  pages = {1289--1292},
  abstract = {Grouping video contents into semantic segments is the crucial pass
	to content-based video summarization and retrieval. In this paper,
	we present a novel scene segmentation and semantic representation
	scheme for various video types. We first detect video shot using
	a coarse-to-fine algorithm. The key frames without useful information
	are detected and removed using template matching. Spatio-temporal
	coherent shots are then grouped into the same scene based on the
	temporal constraint of video content and visual similarity of shot
	activity. With general editing technique used in the continuously
	recorded video, semantic representation of scene content is specified
	to satisfy human demand on video retrieval. The proposed algorithm
	has been performed on various types of videos containing movie and
	TV program. Promising experimental results shows that the proposed
	method makes sense to efficient retrieval of video contents of interest.},
  doi = {10.1109/ICME.2008.4607678},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\A novel scheme for video scenes segmentation and semantic representation.pdf:PDF},
  keywords = {content-based retrieval, image segmentation, video retrieval, video
	signal processing, TV program, coarse-to-fine algorithm, content-based
	video retrieval, content-based video summarization, general editing
	technique, movie, semantic representation, semantic segments, shot
	activity, spatiotemporal coherent shots, template matching, video
	content, video content grouping, video scenes segmentation, visual
	similarity, Video content analysis, semantic representation, video
	segmentation},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@ARTICLE{Zhu2008a,
  author = {Zhu, Songhao and Liu, Yuncai},
  title = {Automatic scene detection for advanced story retrieval},
  journal = {Expert Systems with Applications},
  year = {2008},
  volume = {In Press, Uncorrected Proof},
  abstract = {Browsing video scenes is just the process to unfold the story scenarios
	of a long video archive, which can help users to locate their desired
	video segments quickly and efficiently. Automatic scene detection
	of a long video stream file is hence the first and crucial step toward
	a concise and comprehensive content-based representation for indexing,
	browsing and retrieval purposes. In this paper, we present a novel
	scene detection scheme for various video types. We first detect video
	shot using a coarse-to-fine algorithm. The key frames without useful
	information are detected and removed using template matching. Spatio-temporal
	coherent shots are then grouped into the same scene based on the
	temporal constraint of video content and visual similarity of shot
	activity. The proposed algorithm has been performed on various types
	of videos containing movie and TV program. Promising experimental
	results shows that the proposed method makes sense to efficient retrieval
	of video contents of interest.},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Automatic scene detection for advanced story retrieval.pdf:PDF},
  keywords = {Content-based retrieval;Video scene detection; Feature selection;
	Time-constraint},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.08.31}
}

@INCOLLECTION{Zhu2001,
  author = {Zhu, Xingquan and Wu, Lide and Xue, Xiangyang and Lu, Xiaoye and
	Fan, Jianping},
  title = {Automatic Scene Detection in News Program by Integrating Visual Feature
	and Rules},
  year = {2001},
  pages = {843--848},
  abstract = {Organizing video sequences from shot level to scene level is a challenging
	task, however, without the scene information, it will be very hard
	to extract the content of the video. In this paper, a visual and
	rule information integrated strategy is proposed to detect the scene
	information of the News programs. First, we extract out the video
	caption and anchor person (AP) shots in the video. Second, the rules
	among the scene, video caption, AP shots will be used to detect the
	scenes. Third, the visual features will also be used to analysis
	the scene information in the region which can not be covered by the
	rules. And experimental results based on this strategy are provided
	and analyzed on broadcast News videos.},
  citeulike-article-id = {3388255},
  doi = {http://dx.doi.org/10.1007/3-540-45453-5\_109},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Automatic scene detection for advanced story retrieval.pdf:PDF},
  journal = {Advances in Multimedia Information Processing ï¿½ PCM 2001},
  owner = {danilo},
  posted-at = {2008-10-08 20:21:29},
  priority = {2},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09},
  url = {http://dx.doi.org/10.1007/3-540-45453-5\_109}
}

@ARTICLE{Zhu2003,
  author = {Yingying Zhu and Dongru Zhou},
  title = {Video browsing and retrieval based on multimodal integration},
  journal = {Web Intelligence, 2003. WI 2003. Proceedings. IEEE/WIC International
	Conference on},
  year = {2003},
  pages = { 650-653},
  month = {Oct.},
  abstract = {The rapid growth of multimedia data requires more effective content-based
	video browsing and retrieval. We present a system developed for video
	browsing and retrieval based on multimedia integration. First, a
	basic structure of the system is defined. Second, a robust scene
	segmentation method is presented, which analyzes audio and visual
	information and accounts for their interrelations and coincidence
	to semantically identify video scenes. We then extract text from
	key frames with video OCR technique and extract text transcriptions
	by speech recognition to classify video scenes and form the full-text
	indices. Finally, natural language understanding technique is used
	to automatically classify video scenes on the basis of the texts
	obtained from close caption, video OCR process and speech recognition.
	In this way, we have developed the content-based video database system
	which integrates multimodality to browse and retrieve video data.
	The experimental results show that multimodal integration is effective
	for video scene segmentation. Our system built on the idea of multimodal
	integration makes content-based browsing and retrieval of video data,
	key-frame-based video abstract and search by keywords practical.},
  file = {:C\:\\Users\\danilo\\Desktop\\artigos google\\Video browsing and retrieval based on multimodal integration.pdf:PDF},
  issn = { },
  keywords = { content-based retrieval, image segmentation, multimedia databases,
	natural languages, optical character recognition, speech recognition,
	text analysis, video databases audio information, content-based video
	browsing, content-based video database system, full-text indices,
	key-frame-based video abstract, multimedia data, multimedia integration,
	multimodal integration, natural language understanding technique,
	speech recognition, text transcriptions, video OCR technique, video
	retrieval, video scene segmentation, video scenes classification,
	visual information},
  owner = {danilo},
  review = {Danilo01 Tï¿½tulo Condizente com o tema de pesquisa
	
	Danilo02 Fase de leitura de resumos
	
	Danilo03 O resumo condiz com o tema
	
	Danilo04},
  timestamp = {2008.10.09}
}

@INBOOK{Zutschi2005,
  chapter = {The role of relevance feedback in managing multimedia semantics:
	A survey},
  pages = {288--304},
  title = {Managing Multimedia Semantics},
  publisher = {Idea Group Inc.},
  year = {2005},
  author = {Zutschi, S. and Wilson, C. and Krishnaswamy, S. and Srinivasan, B.},
  owner = {danilo},
  timestamp = {2009.02.15}
}

@BOOK{Zhang2006,
  title = {Advances in Image and Video Segmentation},
  publisher = {IRM Press},
  year = {2006},
  editor = {Yu-Jin Zhang},
  address = {Hershey, Pa, USA},
  owner = {danilo},
  timestamp = {2009.02.02}
}

@comment{jabref-meta: selector_publisher:}

@comment{jabref-meta: selector_author:}

@comment{jabref-meta: selector_journal:}

@comment{jabref-meta: selector_keywords:}

